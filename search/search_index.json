{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MAS Devops Ansible Collection \uf0c1 Requirements \uf0c1 Python & Ansible \uf0c1 Python 3.8 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine: The following python modules are required in order to use this collection. openshift ansible Important As of version 6 of this collection the dependencies have changed. The upgrade from community.kubernetes to kubernetes.core necessitates an upgrade in the version of the kubernetes and openshift modules from v11 to v12. Useful commands \uf0c1 Confirm availability and version: python3 --version Installed Python modules: python3 -m pip install ansible junit_xml pymongo xmljson kubernetes==12.0.1 openshift==0.12.1 Confirm that ansible has been correctly installed: ansible-playbook --version IBM Cloud CLI \uf0c1 If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must install the IBM Cloud CLI: Useful commands \uf0c1 Install: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash Confirm availability and version: ibmcloud version OpenShift CLI \uf0c1 If you do not already have the oc command line tool, you can download it as below: wget -q https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar -xvzf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz mv openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc /usr/local/bin/ rm -rf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz Useful commands \uf0c1 Confirm availability and version: oc version Installation \uf0c1 Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops Change Log \uf0c1 Note that links to pull requests prior to public release of the code (4.0) direct to IBM GitHub Enterprise, and will only be accessible to IBM employees. 6.4 Add support for MVI deployment ( #196 ) 6.3 Allow ocp_server and ocp_token to be used for ocp_login ( #211 ) 6.2 Multiple Updates: Support manual upgrade approvals ( #205 ) Add support for Db2u operator ( #203 ) Add Workspace config generator ( #189 ) 6.1 Create WSL project and enable HPU deploy ( #201 ) 6.0 Multiple Updates: Upgrade to kubernetes.core Ansible module ( #194 ) Remove BAS support (replaced by UDS) ( #194 ) 5.3 Multiple Updates: Add support for db2wh backup & restore ( #133 ) Add support for appConnect ( #170 ) Switch BAS from FullDeployment to AnalyticsProxy ( #178 ) 5.2 Multiple Updates: Support MongoDb CPU and memory configuration ( #158 ) Separate CIS_APIKEY support for MAS Installation ( #156 ) Support configurable prometheus storage & retention policy ( #151 ) Support configurable application spec ( #160 ) 5.1 Multiple Updates: Add support for Cloud Object Storage setup ( #122 ) Conditional application deployment in Tekton pipelines ( #118 ) Add support for CP4D v4 alongside existing support for v3.5 ( #93 ) 5.0 Multiple Updates: Add support for AI Applications' must-gather tooling ( #91 ) Migrate airgap support into ibm.mas_airgap collection ( #38 ) Support for Assist application ( #76 ) Significant refactoring for CP4D support ( #68 ) Migrate build system to GitHub Actions ( #68 ) 4.5 Add support for Manage ( #61 ) 4.4 Add CP4D and DB2W playbooks ( #51 ) 4.3 Add support for playbook junit result generation ( #39 ) 4.2 Add support for Tekton pipelines ( #34 ) 4.1 Add ocp_verify role and associated playbook ( #20 ) 4.0 Initial Public Release on ibm.mas_devops ( #5 ) 3.3 Support configurable SLS settings ( #53 ) 3.2 Add support for BAS ( #44 ) 3.1 Add support for SLS ( #35 ) 3.0 Switch to config dir instead of config file list ( #36 ) 2.7 Support AirGap install of MAS ( #28 ) 2.6 Add support for Gen2 application mgmt (install and configure) ( #24 ) 2.5 Add support for Watson Studio ( #16 ) 2.4 Add support for MongoDb Community Edition ( #25 ) 2.3 Add support for IBM Cloud resource groups ( #20 ) 2.2 Support DNS and certificate mgmt with CIS & LetsEncrypt ( #10 ) 2.1 Add support for AMQ Streams (Kafka) ( #19 ) 2.0 Major refactor of the roles and playbooks ( #17 ) 1.2 Add initial Spark support (incomplete) ( #15 ) 1.1 Enable db2wh SSL and generate jdbccfg for MAS ( #9 ) 1.0 Initial release","title":"Home"},{"location":"#mas-devops-ansible-collection","text":"","title":"MAS Devops Ansible Collection"},{"location":"#requirements","text":"","title":"Requirements"},{"location":"#python-ansible","text":"Python 3.8 is recommended as it is the most widely used version of Python within our development team, but any in-support 3.x version of Python should work fine: The following python modules are required in order to use this collection. openshift ansible Important As of version 6 of this collection the dependencies have changed. The upgrade from community.kubernetes to kubernetes.core necessitates an upgrade in the version of the kubernetes and openshift modules from v11 to v12.","title":"Python &amp; Ansible"},{"location":"#useful-commands","text":"Confirm availability and version: python3 --version Installed Python modules: python3 -m pip install ansible junit_xml pymongo xmljson kubernetes==12.0.1 openshift==0.12.1 Confirm that ansible has been correctly installed: ansible-playbook --version","title":"Useful commands"},{"location":"#ibm-cloud-cli","text":"If you are using this collection to manage an OpenShift cluster in IBM Cloud RedHat OpenShift Kubernetes Service (ROKS), then you must install the IBM Cloud CLI:","title":"IBM Cloud CLI"},{"location":"#useful-commands_1","text":"Install: curl -sL https://raw.githubusercontent.com/IBM-Cloud/ibm-cloud-developer-tools/master/linux-installer/idt-installer | bash Confirm availability and version: ibmcloud version","title":"Useful commands"},{"location":"#openshift-cli","text":"If you do not already have the oc command line tool, you can download it as below: wget -q https://github.com/openshift/origin/releases/download/v3.11.0/openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz tar -xvzf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz mv openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit/oc /usr/local/bin/ rm -rf openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit openshift-origin-client-tools-v3.11.0-0cbc58b-linux-64bit.tar.gz","title":"OpenShift CLI"},{"location":"#useful-commands_2","text":"Confirm availability and version: oc version","title":"Useful commands"},{"location":"#installation","text":"Install the collection direct from Ansible Galaxy ansible-galaxy collection install ibm.mas_devops","title":"Installation"},{"location":"#change-log","text":"Note that links to pull requests prior to public release of the code (4.0) direct to IBM GitHub Enterprise, and will only be accessible to IBM employees. 6.4 Add support for MVI deployment ( #196 ) 6.3 Allow ocp_server and ocp_token to be used for ocp_login ( #211 ) 6.2 Multiple Updates: Support manual upgrade approvals ( #205 ) Add support for Db2u operator ( #203 ) Add Workspace config generator ( #189 ) 6.1 Create WSL project and enable HPU deploy ( #201 ) 6.0 Multiple Updates: Upgrade to kubernetes.core Ansible module ( #194 ) Remove BAS support (replaced by UDS) ( #194 ) 5.3 Multiple Updates: Add support for db2wh backup & restore ( #133 ) Add support for appConnect ( #170 ) Switch BAS from FullDeployment to AnalyticsProxy ( #178 ) 5.2 Multiple Updates: Support MongoDb CPU and memory configuration ( #158 ) Separate CIS_APIKEY support for MAS Installation ( #156 ) Support configurable prometheus storage & retention policy ( #151 ) Support configurable application spec ( #160 ) 5.1 Multiple Updates: Add support for Cloud Object Storage setup ( #122 ) Conditional application deployment in Tekton pipelines ( #118 ) Add support for CP4D v4 alongside existing support for v3.5 ( #93 ) 5.0 Multiple Updates: Add support for AI Applications' must-gather tooling ( #91 ) Migrate airgap support into ibm.mas_airgap collection ( #38 ) Support for Assist application ( #76 ) Significant refactoring for CP4D support ( #68 ) Migrate build system to GitHub Actions ( #68 ) 4.5 Add support for Manage ( #61 ) 4.4 Add CP4D and DB2W playbooks ( #51 ) 4.3 Add support for playbook junit result generation ( #39 ) 4.2 Add support for Tekton pipelines ( #34 ) 4.1 Add ocp_verify role and associated playbook ( #20 ) 4.0 Initial Public Release on ibm.mas_devops ( #5 ) 3.3 Support configurable SLS settings ( #53 ) 3.2 Add support for BAS ( #44 ) 3.1 Add support for SLS ( #35 ) 3.0 Switch to config dir instead of config file list ( #36 ) 2.7 Support AirGap install of MAS ( #28 ) 2.6 Add support for Gen2 application mgmt (install and configure) ( #24 ) 2.5 Add support for Watson Studio ( #16 ) 2.4 Add support for MongoDb Community Edition ( #25 ) 2.3 Add support for IBM Cloud resource groups ( #20 ) 2.2 Support DNS and certificate mgmt with CIS & LetsEncrypt ( #10 ) 2.1 Add support for AMQ Streams (Kafka) ( #19 ) 2.0 Major refactor of the roles and playbooks ( #17 ) 1.2 Add initial Spark support (incomplete) ( #15 ) 1.1 Enable db2wh SSL and generate jdbccfg for MAS ( #9 ) 1.0 Initial release","title":"Change Log"},{"location":"playbooks/cp4d/","text":"CP4D Playbooks \uf0c1 CloudPak for Data support comes in two flavours: CP4D v3.5 and CP4D v4. For users of MAS v8.7 or later only CP4D v4 is supported. MAS 8.6 with the January 2022 maintenance updates supports both, and earlier version of MAS only support CP4D v3.5. CloudPak for Data v3.5 is installed from the IBM Operator Catalog using the v1.0 channel to the cpd-meta-ops namespace. CloudPak for Data v4 is installed from the IBM Operator Catalog using the v2.0 channel to the ibm-common-services namespace. Warning The credentials to sign-in when deploying CP4D v3.5 are the defaults, which are admin/password . Yes, really! Create DB2 Instance \uf0c1 This playbook will auto-detect whether CP4D v3.5 or v4 is active in the cluster. Refer to the cp4d_db2wh role documentation for more information. export DB2WH_INSTANCE_NAME=db2w-shared export DB2WH_META_STORAGE_CLASS=ibmc-file-silver-gid export DB2WH_USER_STORAGE_CLASS=ibmc-file-gold-gid export DB2WH_BACKUP_STORAGE_CLASS=ibmc-file-gold-gid export DB2WH_LOGS_STORAGE_CLASS=ibmc-file-silver-gid export DB2WH_TEMP_STORAGE_CLASS=ibmc-file-silver-gid export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/cp4d/create-db2-instance.yml Backup & Restore DB2 Instance \uf0c1 This playbook will execute procedures to take a backup from a source DB2 instance and restore into a target DB2 instance. export DB2WH_BACKUP_DIR='/Users/Documents/db_backup' export DB2WH_INSTANCE_ID_SOURCE='db2wh-1637258370283030' export DB2WH_INSTANCE_ID_TARGET='db2wh-1641225392064040' ansible-playbook playbooks/cp4d/db2wh-backup.yml ansible-playbook playbooks/cp4d/db2wh-restore.yml Hack Worker Nodes \uf0c1 This playbook will auto-detect whether CP4D v3.5 or v4 is active in the cluster. Refer to the cp4d_hack_worker_nodes role documentation for more information. export CLUSTER_NAME=masinst1 export CLUSTER_TYPE=roks export IBMCLOUD_APIKEY=xxx export CPD_ENTITLEMENT_KEY=xxx ansible-playbook playbooks/cp4d/hack-worker-nodes.yml Install Services: Db2 \uf0c1 This playbook will install CP4D and enable the CP4D Db2 Warehouse service. Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-db2.yml Install Services: Fullstack \uf0c1 This playbook will install CP4D and enable the CP4D Db2 Warehouse service and Watson Studio with Watson Machine Learning, Apache Spark, & Watson AI OpenScale capabilities enabled. Watson Machine Learning As part of Watson Studio, Watson Machine Learning helps data scientists and developers accelerate AI and machine learning deployment. Apache Spark Apache Spark is a runtime environment configured inside of Watson Studio similar to a Python Runtime environment. When Spark is enabled from CP4D, you can opt to create a notebook and choose Spark as runtime to expand data modeling capabilities. Watson AI OpenScale Watson OpenScale enables tracking AI models in production, validation and test models to mitigate operational risks. Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Predict requires Watson Studio, Machine Learning and Spark; Openscale is an optional dependency Health & Predict Utilities requires Watson Studio base capability only Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-fullstack.yml Install Services: Watson Studio \uf0c1 This playbook will install CP4D and install the Watson Studio with Watson Machine Learning, Apache Spark, & Watson AI OpenScale capabilities enabled. Watson Machine Learning As part of Watson Studio, Watson Machine Learning helps data scientists and developers accelerate AI and machine learning deployment. Apache Spark Apache Spark is a runtime environment configured inside of Watson Studio similar to a Python Runtime environment. When Spark is enabled from CP4D, you can opt to create a notebook and choose Spark as runtime to expand data modeling capabilities. Watson AI OpenScale Watson OpenScale enables tracking AI models in production, validation and test models to mitigate operational risks. Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Predict requires Watson Studio, Machine Learning and Spark; Openscale is an optional dependency Health & Predict Utilities requires Watson Studio base capability only Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-watsonstudio.yml","title":"CP4D Playbooks"},{"location":"playbooks/cp4d/#cp4d-playbooks","text":"CloudPak for Data support comes in two flavours: CP4D v3.5 and CP4D v4. For users of MAS v8.7 or later only CP4D v4 is supported. MAS 8.6 with the January 2022 maintenance updates supports both, and earlier version of MAS only support CP4D v3.5. CloudPak for Data v3.5 is installed from the IBM Operator Catalog using the v1.0 channel to the cpd-meta-ops namespace. CloudPak for Data v4 is installed from the IBM Operator Catalog using the v2.0 channel to the ibm-common-services namespace. Warning The credentials to sign-in when deploying CP4D v3.5 are the defaults, which are admin/password . Yes, really!","title":"CP4D Playbooks"},{"location":"playbooks/cp4d/#create-db2-instance","text":"This playbook will auto-detect whether CP4D v3.5 or v4 is active in the cluster. Refer to the cp4d_db2wh role documentation for more information. export DB2WH_INSTANCE_NAME=db2w-shared export DB2WH_META_STORAGE_CLASS=ibmc-file-silver-gid export DB2WH_USER_STORAGE_CLASS=ibmc-file-gold-gid export DB2WH_BACKUP_STORAGE_CLASS=ibmc-file-gold-gid export DB2WH_LOGS_STORAGE_CLASS=ibmc-file-silver-gid export DB2WH_TEMP_STORAGE_CLASS=ibmc-file-silver-gid export MAS_INSTANCE_ID=inst1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/cp4d/create-db2-instance.yml","title":"Create DB2 Instance"},{"location":"playbooks/cp4d/#backup-restore-db2-instance","text":"This playbook will execute procedures to take a backup from a source DB2 instance and restore into a target DB2 instance. export DB2WH_BACKUP_DIR='/Users/Documents/db_backup' export DB2WH_INSTANCE_ID_SOURCE='db2wh-1637258370283030' export DB2WH_INSTANCE_ID_TARGET='db2wh-1641225392064040' ansible-playbook playbooks/cp4d/db2wh-backup.yml ansible-playbook playbooks/cp4d/db2wh-restore.yml","title":"Backup &amp; Restore DB2 Instance"},{"location":"playbooks/cp4d/#hack-worker-nodes","text":"This playbook will auto-detect whether CP4D v3.5 or v4 is active in the cluster. Refer to the cp4d_hack_worker_nodes role documentation for more information. export CLUSTER_NAME=masinst1 export CLUSTER_TYPE=roks export IBMCLOUD_APIKEY=xxx export CPD_ENTITLEMENT_KEY=xxx ansible-playbook playbooks/cp4d/hack-worker-nodes.yml","title":"Hack Worker Nodes"},{"location":"playbooks/cp4d/#install-services-db2","text":"This playbook will install CP4D and enable the CP4D Db2 Warehouse service. Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-db2.yml","title":"Install Services: Db2"},{"location":"playbooks/cp4d/#install-services-fullstack","text":"This playbook will install CP4D and enable the CP4D Db2 Warehouse service and Watson Studio with Watson Machine Learning, Apache Spark, & Watson AI OpenScale capabilities enabled. Watson Machine Learning As part of Watson Studio, Watson Machine Learning helps data scientists and developers accelerate AI and machine learning deployment. Apache Spark Apache Spark is a runtime environment configured inside of Watson Studio similar to a Python Runtime environment. When Spark is enabled from CP4D, you can opt to create a notebook and choose Spark as runtime to expand data modeling capabilities. Watson AI OpenScale Watson OpenScale enables tracking AI models in production, validation and test models to mitigate operational risks. Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Predict requires Watson Studio, Machine Learning and Spark; Openscale is an optional dependency Health & Predict Utilities requires Watson Studio base capability only Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-fullstack.yml","title":"Install Services: Fullstack"},{"location":"playbooks/cp4d/#install-services-watson-studio","text":"This playbook will install CP4D and install the Watson Studio with Watson Machine Learning, Apache Spark, & Watson AI OpenScale capabilities enabled. Watson Machine Learning As part of Watson Studio, Watson Machine Learning helps data scientists and developers accelerate AI and machine learning deployment. Apache Spark Apache Spark is a runtime environment configured inside of Watson Studio similar to a Python Runtime environment. When Spark is enabled from CP4D, you can opt to create a notebook and choose Spark as runtime to expand data modeling capabilities. Watson AI OpenScale Watson OpenScale enables tracking AI models in production, validation and test models to mitigate operational risks. Application Support For more information on how Predict and HP Utilities make use of Watson Studio, refer to Predict/HP Utilities documentation Predict requires Watson Studio, Machine Learning and Spark; Openscale is an optional dependency Health & Predict Utilities requires Watson Studio base capability only Refer to the cp4d_install and cp4d_install_services role documentation for more information. export CPD_VERSION=cpd40 export CPD_STORAGE_CLASS=ibmc-file-gold-gid export CPD_BLOCK_STORAGE_CLASS=ibmc-block-gold ansible-playbook playbooks/cp4d/install-services-watsonstudio.yml","title":"Install Services: Watson Studio"},{"location":"playbooks/dependencies/","text":"Dependencies Playbooks \uf0c1 Install AMQ Streams \uf0c1 AMQ Streams operator will be installed into the amq-streams namespace, a cluster named maskafka will be created using the small configuration and ibmc-block-gold as the storage class. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the amqstreams role documentation for more information. export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-amqstreams.yml Install MongoDb (CE) \uf0c1 MongoDb CE operator will be installed into the mongoce namespace, a 3 node cluster cluster will be created. The cluster will bind six 20GB ibmc-block-gold PVCs, these provide persistence for the data and system logs across the three nodes. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the mongodb role documentation for more information. export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-mongodb-ce.yml Install UDS \uf0c1 Installs IBM User Data Services . Refer to the uds_install role documentation for more information. export UDS_STORAGE_CLASS=ibmc-block-bronze export UDS_CONTACT_EMAIL=john@email.com export UDS_CONTACT_FIRSTNAME=john export UDS_CONTACT_LASTNAME=winter export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-uds.yml Install SLS \uf0c1 Before you use this playbook you will likely want to edit the mas_config_dir variable to supply your own configuration, instead of the sample data provided. Required environment variables \uf0c1 SLS_ENTITLEMENT_KEY Provide your IBM entitlement key Optional environment variables \uf0c1 SLS_CATALOG_SOURCE Set to ibm-sls-operators if you want to deploy pre-release development builds SLS_CHANNEL Override the default release channel (3.x) SLS_ICR_CP Override the registry source for all container images deployed by the SLS operator SLS_ICR_CPOPEN Override the registry source for all container images deployed by the SLS operator SLS_ENTITLEMENT_USERNAME Override the default entitlement username (cp) SLS_NAMESPACE Override the default entitlement username (ibm-sls) SLS_STORAGE_CLASS Defines Storage Class to be used by SLS Persistent Volumes SLS_LICENSE_ID Must be set to the license id specified in the license file when one is provided SLS_REGISTRATION_KEY optional var when you want to install sls using a registration key you have. Example usage: release build \uf0c1 export SLS_INSTANCE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx export SLS_STORAGE_CLASS=xxx ansible-playbook playbooks/dependencies/install-sls.yml Note Lookup your entitlement key from the IBM Container Library Example usage: pre-release build \uf0c1 export SLS_CATALOG_SOURCE=ibm-sls-operators export SLS_CHANNEL=3.1.0-pre.stable export SLS_INSTANCE_ID=xxx export SLS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export SLS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export SLS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export SLS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export SLS_STORAGE_CLASS=xxx ansible-playbook playbooks/dependencies/install-sls.yml Important You must have already installed the development (pre-release) catalogs, pre-release builds are not available directly from the IBM Operator Catalog. Install AppConnect \uf0c1 AppConnect will be installed into the ibm-app-connect namespace, using ibmc-file-gold-gid as the storage class. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the appconnect_install role documentation for more information. export APPCONNECT_STORAGE_CLASS=ibmc-file-gold-gid export APPCONNECT_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-appconnect.yml Install GPU \uf0c1 Installs NVIDIA Graphical Processing Unit (GPU) and its prerequisite Node Feature Discovery (NFD) . The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source. Refer to the gpu_install role documentation for more information. Example usage: \uf0c1 export NFD_NAMESPACE=nfd-operator export NFD_CHANNEL=stable export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v1.9 ansible-playbook playbooks/dependencies/install-gpu.yml Install DB2 \uf0c1 Installs IBM DB2 using the db2u operator. Refer to the [db2u] role documentation for more information. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. export DB2U_META_STORAGE_CLASS=ibmc-file-gold export DB2U_DATA_STORAGE_CLASS=ibmc-block-gold export DB2U_INSTANCE_NAME=db2u-db01 export ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-db2.yml","title":"Dependencies Playbooks"},{"location":"playbooks/dependencies/#dependencies-playbooks","text":"","title":"Dependencies Playbooks"},{"location":"playbooks/dependencies/#install-amq-streams","text":"AMQ Streams operator will be installed into the amq-streams namespace, a cluster named maskafka will be created using the small configuration and ibmc-block-gold as the storage class. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the amqstreams role documentation for more information. export KAFKA_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-amqstreams.yml","title":"Install AMQ Streams"},{"location":"playbooks/dependencies/#install-mongodb-ce","text":"MongoDb CE operator will be installed into the mongoce namespace, a 3 node cluster cluster will be created. The cluster will bind six 20GB ibmc-block-gold PVCs, these provide persistence for the data and system logs across the three nodes. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the mongodb role documentation for more information. export MONGODB_STORAGE_CLASS=ibmc-block-gold export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-mongodb-ce.yml","title":"Install MongoDb (CE)"},{"location":"playbooks/dependencies/#install-uds","text":"Installs IBM User Data Services . Refer to the uds_install role documentation for more information. export UDS_STORAGE_CLASS=ibmc-block-bronze export UDS_CONTACT_EMAIL=john@email.com export UDS_CONTACT_FIRSTNAME=john export UDS_CONTACT_LASTNAME=winter export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-uds.yml","title":"Install UDS"},{"location":"playbooks/dependencies/#install-sls","text":"Before you use this playbook you will likely want to edit the mas_config_dir variable to supply your own configuration, instead of the sample data provided.","title":"Install SLS"},{"location":"playbooks/dependencies/#required-environment-variables","text":"SLS_ENTITLEMENT_KEY Provide your IBM entitlement key","title":"Required environment variables"},{"location":"playbooks/dependencies/#optional-environment-variables","text":"SLS_CATALOG_SOURCE Set to ibm-sls-operators if you want to deploy pre-release development builds SLS_CHANNEL Override the default release channel (3.x) SLS_ICR_CP Override the registry source for all container images deployed by the SLS operator SLS_ICR_CPOPEN Override the registry source for all container images deployed by the SLS operator SLS_ENTITLEMENT_USERNAME Override the default entitlement username (cp) SLS_NAMESPACE Override the default entitlement username (ibm-sls) SLS_STORAGE_CLASS Defines Storage Class to be used by SLS Persistent Volumes SLS_LICENSE_ID Must be set to the license id specified in the license file when one is provided SLS_REGISTRATION_KEY optional var when you want to install sls using a registration key you have.","title":"Optional environment variables"},{"location":"playbooks/dependencies/#example-usage-release-build","text":"export SLS_INSTANCE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx export SLS_STORAGE_CLASS=xxx ansible-playbook playbooks/dependencies/install-sls.yml Note Lookup your entitlement key from the IBM Container Library","title":"Example usage: release build"},{"location":"playbooks/dependencies/#example-usage-pre-release-build","text":"export SLS_CATALOG_SOURCE=ibm-sls-operators export SLS_CHANNEL=3.1.0-pre.stable export SLS_INSTANCE_ID=xxx export SLS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export SLS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export SLS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export SLS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export SLS_STORAGE_CLASS=xxx ansible-playbook playbooks/dependencies/install-sls.yml Important You must have already installed the development (pre-release) catalogs, pre-release builds are not available directly from the IBM Operator Catalog.","title":"Example usage: pre-release build"},{"location":"playbooks/dependencies/#install-appconnect","text":"AppConnect will be installed into the ibm-app-connect namespace, using ibmc-file-gold-gid as the storage class. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. Refer to the appconnect_install role documentation for more information. export APPCONNECT_STORAGE_CLASS=ibmc-file-gold-gid export APPCONNECT_ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-appconnect.yml","title":"Install AppConnect"},{"location":"playbooks/dependencies/#install-gpu","text":"Installs NVIDIA Graphical Processing Unit (GPU) and its prerequisite Node Feature Discovery (NFD) . The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source. Refer to the gpu_install role documentation for more information.","title":"Install GPU"},{"location":"playbooks/dependencies/#example-usage","text":"export NFD_NAMESPACE=nfd-operator export NFD_CHANNEL=stable export GPU_NAMESPACE=nvidia-gpu-operator export GPU_CHANNEL=v1.9 ansible-playbook playbooks/dependencies/install-gpu.yml","title":"Example usage:"},{"location":"playbooks/dependencies/#install-db2","text":"Installs IBM DB2 using the db2u operator. Refer to the [db2u] role documentation for more information. The generated configuration for MAS will be available in the ~/masconfig directory on the local system. export DB2U_META_STORAGE_CLASS=ibmc-file-gold export DB2U_DATA_STORAGE_CLASS=ibmc-block-gold export DB2U_INSTANCE_NAME=db2u-db01 export ENTITLEMENT_KEY=xxx export MAS_INSTANCE_ID=masdev1 export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/dependencies/install-db2.yml","title":"Install DB2"},{"location":"playbooks/fullstack-roks/","text":"Full Stack on IBM Cloud \uf0c1 This master playbook will drive the following playbooks in sequence: Provision & setup OCP on IBM Cloud (20-30 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install Kafka (AMQ Streams) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install Cloud Pak for Data Operator (2 minutes) Install AppConnect (?) Install Cloud Pak for Data Services: Db2 Warehouse (1 hour) Watson Studio with Apache Spark , Watson Machine Learning , & Watson AI OpenScale (4-5 hours) Create Db2 Warehouse Cluster (45 minutes) Additional Db2 configuration for Manage (5 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) Install applications: Install & configure Manage (10 minute install + 2 hours configure) Install & configure IoT (25 minute install + 5 minutes configure) Install & configure Monitor (10 minute install + ? configure) Install & configure Predict (10 minute install + 5 minutes configure) Install & configure Safety (? minute install + ? configure) Install & configure Maximo Scheduler Optmization (10 minute install + ? configure) Install & configure Health & Predict Utilities (10 minute install + ? configure) All timings are estimates, see the individual pages for each of these playbooks for more information. Warning The install time for Cloud Pak for Data with all the services supported by MAS enabled is considerable. Unfortunately this is out of our control, plan accordingly! Also note that Cloud Pak for Data requires approximately 40 PVCs. You may need to contact IBM to increase the quota assigned to your IBM Cloud account if you see PVCs stuck in pending state and this error message: \"Your order will exceed the maximum number of storage volumes allowed. Please contact Sales\" Preparation \uf0c1 Before you run the playbook you need to configure a few things in your MAS_CONFIG_DIR : Copy your entitlement license key file \uf0c1 Copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Important Make sure you set SLS_LICENSE_ID to the correct value. For full details on what configuration options are available with the SLS install refer to the Install SLS topic. Required environment variables \uf0c1 IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for BAS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for BAS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for BAS CPD_ENTITLEMENT_KEY Lookup your entitlement key from the [IBM Container Library](https://myibm.ibm.com/ Optional environment variables \uf0c1 IBMCLOUD_RESOURCEGROUP creates an IBM Cloud resource group to be used, if none are passed, Default resource group will be used. OCP_VERSION to override the default version of OCP to use (latest 4.6 release) W3_USERNAME to enable access to pre-release development builds of MAS ARTIFACTORY_APIKEY to enable access to pre-release development builds of MAS KAFKA_CLUSTER_SIZE to override the default configuration used (small) MONGODB_NAMESPACE overrides the Kubernetes namespace where the MongoDb CE operator will be installed, this will default to mongoce MAS_CATALOG_SOURCE to override the use of the IBM Operator Catalog as the catalog source MAS_CHANNEL to override the use of the 8.x channel MAS_DOMAIN to set a custom domain for the MAS installation MAS_UPGRADE_STRATEGY to override the use of Manual upgrade strategy. MAS_ICR_CP to override the value MAS uses for the IBM Entitled Registry ( cp.icr.io/cp ) MAS_ICR_CPOPEN to override the value MAS uses for the IBM Open Registry ( icr.io/cpopen ) MAS_ENTITLEMENT_USERNAME to override the username MAS uses to access content in the IBM Entitled Registry CIS_CRN to enable integration with IBM Cloud Internet Services (CIS) for DNS & certificate management CIS_SUBDOMAIN if you want to use a subdomain within your CIS instance CPD_WSL_PROJECT_ID to set a Watson Studio project id to be passed to HP Utilities application during its deployment and configuration. If not set, a new project will be created in Watson Studio automatically to configure HP Utilities application in the MAS instance created by this playbook CPD_WSL_PROJECT_NAME to set a Watson Studio project name, if not set a default project name will be used CPD_WSL_PROJECT_DESCRIPTION - to set a Watson Studio project description, if not set a default project description will be used Tip MAS_ICR_CP , MAS_ICR_CPOPEN , & MAS_ENTITLEMENT_USERNAME are primarily used when working with pre-release builds in conjunction with W3_USERNAME , ARTIFACTORY_APIKEY and the MAS_CATALOG_SOURCE environment variables. Release build \uf0c1 # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # MAS configuration export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/fullstack-roks.yml Note Lookup your entitlement keys from the IBM Container Library Pre-release build \uf0c1 # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m1dev87 export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/fullstack-roks.yml","title":"IBM Cloud"},{"location":"playbooks/fullstack-roks/#full-stack-on-ibm-cloud","text":"This master playbook will drive the following playbooks in sequence: Provision & setup OCP on IBM Cloud (20-30 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install Kafka (AMQ Streams) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install Cloud Pak for Data Operator (2 minutes) Install AppConnect (?) Install Cloud Pak for Data Services: Db2 Warehouse (1 hour) Watson Studio with Apache Spark , Watson Machine Learning , & Watson AI OpenScale (4-5 hours) Create Db2 Warehouse Cluster (45 minutes) Additional Db2 configuration for Manage (5 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) Install applications: Install & configure Manage (10 minute install + 2 hours configure) Install & configure IoT (25 minute install + 5 minutes configure) Install & configure Monitor (10 minute install + ? configure) Install & configure Predict (10 minute install + 5 minutes configure) Install & configure Safety (? minute install + ? configure) Install & configure Maximo Scheduler Optmization (10 minute install + ? configure) Install & configure Health & Predict Utilities (10 minute install + ? configure) All timings are estimates, see the individual pages for each of these playbooks for more information. Warning The install time for Cloud Pak for Data with all the services supported by MAS enabled is considerable. Unfortunately this is out of our control, plan accordingly! Also note that Cloud Pak for Data requires approximately 40 PVCs. You may need to contact IBM to increase the quota assigned to your IBM Cloud account if you see PVCs stuck in pending state and this error message: \"Your order will exceed the maximum number of storage volumes allowed. Please contact Sales\"","title":"Full Stack on IBM Cloud"},{"location":"playbooks/fullstack-roks/#preparation","text":"Before you run the playbook you need to configure a few things in your MAS_CONFIG_DIR :","title":"Preparation"},{"location":"playbooks/fullstack-roks/#copy-your-entitlement-license-key-file","text":"Copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Important Make sure you set SLS_LICENSE_ID to the correct value. For full details on what configuration options are available with the SLS install refer to the Install SLS topic.","title":"Copy your entitlement license key file"},{"location":"playbooks/fullstack-roks/#required-environment-variables","text":"IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for BAS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for BAS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for BAS CPD_ENTITLEMENT_KEY Lookup your entitlement key from the [IBM Container Library](https://myibm.ibm.com/","title":"Required environment variables"},{"location":"playbooks/fullstack-roks/#optional-environment-variables","text":"IBMCLOUD_RESOURCEGROUP creates an IBM Cloud resource group to be used, if none are passed, Default resource group will be used. OCP_VERSION to override the default version of OCP to use (latest 4.6 release) W3_USERNAME to enable access to pre-release development builds of MAS ARTIFACTORY_APIKEY to enable access to pre-release development builds of MAS KAFKA_CLUSTER_SIZE to override the default configuration used (small) MONGODB_NAMESPACE overrides the Kubernetes namespace where the MongoDb CE operator will be installed, this will default to mongoce MAS_CATALOG_SOURCE to override the use of the IBM Operator Catalog as the catalog source MAS_CHANNEL to override the use of the 8.x channel MAS_DOMAIN to set a custom domain for the MAS installation MAS_UPGRADE_STRATEGY to override the use of Manual upgrade strategy. MAS_ICR_CP to override the value MAS uses for the IBM Entitled Registry ( cp.icr.io/cp ) MAS_ICR_CPOPEN to override the value MAS uses for the IBM Open Registry ( icr.io/cpopen ) MAS_ENTITLEMENT_USERNAME to override the username MAS uses to access content in the IBM Entitled Registry CIS_CRN to enable integration with IBM Cloud Internet Services (CIS) for DNS & certificate management CIS_SUBDOMAIN if you want to use a subdomain within your CIS instance CPD_WSL_PROJECT_ID to set a Watson Studio project id to be passed to HP Utilities application during its deployment and configuration. If not set, a new project will be created in Watson Studio automatically to configure HP Utilities application in the MAS instance created by this playbook CPD_WSL_PROJECT_NAME to set a Watson Studio project name, if not set a default project name will be used CPD_WSL_PROJECT_DESCRIPTION - to set a Watson Studio project description, if not set a default project description will be used Tip MAS_ICR_CP , MAS_ICR_CPOPEN , & MAS_ENTITLEMENT_USERNAME are primarily used when working with pre-release builds in conjunction with W3_USERNAME , ARTIFACTORY_APIKEY and the MAS_CATALOG_SOURCE environment variables.","title":"Optional environment variables"},{"location":"playbooks/fullstack-roks/#release-build","text":"# IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # MAS configuration export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/fullstack-roks.yml Note Lookup your entitlement keys from the IBM Container Library","title":"Release build"},{"location":"playbooks/fullstack-roks/#pre-release-build","text":"# IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m1dev87 export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/fullstack-roks.yml","title":"Pre-release build"},{"location":"playbooks/lite-core-quickburn/","text":"MAS Core Service on DevIT Quickburn \uf0c1 This master playbook will drive the following playbooks in sequence: Provision & setup Quickburn (25 minutes) Install dependencies: Install MongoDb (10 minutes) Install SLS (10 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Due to the size limtations of QuickBurn clusters a full MAS stack is not possible. Required environment variables \uf0c1 FYRE_USERNAME FYRE_APIKEY FYRE_PRODUCT_ID CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library Optional environment variables \uf0c1 Refer to the role documentation for full details of all configuration options available in this playbook: ocp_provision ocp_setup_mas_deps mongodb sls_install gencfg_sls gencfg_workspace suite_dns suite_install suite_config suite_verify Release build \uf0c1 # Fyre credentials export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=225 # Cluster configuration export CLUSTER_NAME=xxx export OCP_VERSION=4.6.16 # MAS configuration export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/lite-core-quickburn.yml Note Lookup your entitlement keys from the IBM Container Library Pre-release build \uf0c1 # Fyre credentials export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=225 # Cluster configuration export CLUSTER_NAME=xxx export OCP_VERSION=4.6.16 # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=8.5.0-pre.m2dev85 export MAS_INSTANCE_ID=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/lite-core-quickburn.yml","title":"Core only on QuickBurn"},{"location":"playbooks/lite-core-quickburn/#mas-core-service-on-devit-quickburn","text":"This master playbook will drive the following playbooks in sequence: Provision & setup Quickburn (25 minutes) Install dependencies: Install MongoDb (10 minutes) Install SLS (10 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Due to the size limtations of QuickBurn clusters a full MAS stack is not possible.","title":"MAS Core Service on DevIT Quickburn"},{"location":"playbooks/lite-core-quickburn/#required-environment-variables","text":"FYRE_USERNAME FYRE_APIKEY FYRE_PRODUCT_ID CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library","title":"Required environment variables"},{"location":"playbooks/lite-core-quickburn/#optional-environment-variables","text":"Refer to the role documentation for full details of all configuration options available in this playbook: ocp_provision ocp_setup_mas_deps mongodb sls_install gencfg_sls gencfg_workspace suite_dns suite_install suite_config suite_verify","title":"Optional environment variables"},{"location":"playbooks/lite-core-quickburn/#release-build","text":"# Fyre credentials export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=225 # Cluster configuration export CLUSTER_NAME=xxx export OCP_VERSION=4.6.16 # MAS configuration export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/lite-core-quickburn.yml Note Lookup your entitlement keys from the IBM Container Library","title":"Release build"},{"location":"playbooks/lite-core-quickburn/#pre-release-build","text":"# Fyre credentials export FYRE_USERNAME=xxx export FYRE_APIKEY=xxx export FYRE_PRODUCT_ID=225 # Cluster configuration export CLUSTER_NAME=xxx export OCP_VERSION=4.6.16 # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=8.5.0-pre.m2dev85 export MAS_INSTANCE_ID=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/lite-core-quickburn.yml","title":"Pre-release build"},{"location":"playbooks/lite-core-roks/","text":"MAS Core Service on IBM Cloud ROKS \uf0c1 This master playbook will drive the following actions: Provision & setup OCP on IBM Cloud (20-30 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install & configure MAS: Generate MAS Workspace Configuration (1 minute) Configure Cloud Internet Services integration (Optional, 1 minute) Install & configure MAS (15 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information. Preparation \uf0c1 Before you run the playbook you must prepare the entitlement license key file that will be used during the playbook run. Copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Tip If you do not already have an entitlement file, create a random 12 character hex string and use this as the license ID when requesting your entitlement file from Rational License Key Server. Required environment variables \uf0c1 IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for UDS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for UDS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for UDS Optional environment variables \uf0c1 Refer to the role documentation for full details of all configuration options available in this playbook: ocp_provision ocp_setup_mas_deps mongodb sls_install gencfg_sls uds_install gencfg_workspace suite_dns suite_install suite_config suite_verify Release build \uf0c1 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_LICENSE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx # UDS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-core-roks.yml Pre-release build \uf0c1 The simplest configuration to deploy a pre-release build (only available to IBM employees) of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m2dev88 export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_LICENSE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx # UDS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-core-roks.yml Locating the playbook \uf0c1 After you have installed the ibm.mas_devops collection you will be able to find the playbook on your system as part of that installation. For example, if you installed the collection to /home/david/.ansible/collections/ansible_collections the path to this playbook will be /home/david/.ansible/collections/ansible_collections/ibm/mas_devops/playbooks/lite-core-roks.yml Alternatively: You can download the playbook from GitHub, but make sure to download the version of the playbook that corresponds to the version of the ibm.mas_devops Ansible collection that you have installed. You can close the repository from GitHub, but make sure to use the branch/tag corresponding to the version of the ibm.mas_devops Ansible colleciton that you have installed.","title":"Core only on IBM Cloud"},{"location":"playbooks/lite-core-roks/#mas-core-service-on-ibm-cloud-roks","text":"This master playbook will drive the following actions: Provision & setup OCP on IBM Cloud (20-30 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install & configure MAS: Generate MAS Workspace Configuration (1 minute) Configure Cloud Internet Services integration (Optional, 1 minute) Install & configure MAS (15 minutes) All timings are estimates, see the individual pages for each of these playbooks for more information.","title":"MAS Core Service on IBM Cloud ROKS"},{"location":"playbooks/lite-core-roks/#preparation","text":"Before you run the playbook you must prepare the entitlement license key file that will be used during the playbook run. Copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Tip If you do not already have an entitlement file, create a random 12 character hex string and use this as the license ID when requesting your entitlement file from Rational License Key Server.","title":"Preparation"},{"location":"playbooks/lite-core-roks/#required-environment-variables","text":"IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for UDS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for UDS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for UDS","title":"Required environment variables"},{"location":"playbooks/lite-core-roks/#optional-environment-variables","text":"Refer to the role documentation for full details of all configuration options available in this playbook: ocp_provision ocp_setup_mas_deps mongodb sls_install gencfg_sls uds_install gencfg_workspace suite_dns suite_install suite_config suite_verify","title":"Optional environment variables"},{"location":"playbooks/lite-core-roks/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_LICENSE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx # UDS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-core-roks.yml","title":"Release build"},{"location":"playbooks/lite-core-roks/#pre-release-build","text":"The simplest configuration to deploy a pre-release build (only available to IBM employees) of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m2dev88 export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # SLS configuration export SLS_LICENSE_ID=xxx export SLS_ENTITLEMENT_KEY=xxx # UDS configuration export UDS_CONTACT_EMAIL=xxx@xxx.com export UDS_CONTACT_FIRSTNAME=xxx export UDS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-core-roks.yml","title":"Pre-release build"},{"location":"playbooks/lite-core-roks/#locating-the-playbook","text":"After you have installed the ibm.mas_devops collection you will be able to find the playbook on your system as part of that installation. For example, if you installed the collection to /home/david/.ansible/collections/ansible_collections the path to this playbook will be /home/david/.ansible/collections/ansible_collections/ibm/mas_devops/playbooks/lite-core-roks.yml Alternatively: You can download the playbook from GitHub, but make sure to download the version of the playbook that corresponds to the version of the ibm.mas_devops Ansible collection that you have installed. You can close the repository from GitHub, but make sure to use the branch/tag corresponding to the version of the ibm.mas_devops Ansible colleciton that you have installed.","title":"Locating the playbook"},{"location":"playbooks/lite-manage-roks/","text":"MAS Core with Manage on IBM Cloud \uf0c1 This master playbook will drive the following playbooks in sequence: Provision & setup OCP on IBM Cloud (20-30 minutes) Update Default Cluster Pull Secret and Reboot Worker Nodes (10 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install Cloud Pak for Data Operator (2 minutes) Install Cloud Pak for Data Services: Db2 Warehouse (1 hour) Create Db2 Warehouse Cluster (45 minutes) Additional Db2 configuration for Manage (5 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) Install Manage application: Install application (10 minutes) Configure workspace (2 hours) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook. Preparation \uf0c1 Before you run the playbook you need to configure a few things in your MAS_CONFIG_DIR : Prepare your entitlement license key file \uf0c1 First, set SLS_LICENSE_ID to the correct ID (a 12 character hex string) from your entitlement file, then copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Tip If you do not already have an entitlement file, create a random 12 character hex string and use this as the license ID when requesting your entitlement file from Rational License Key Server. Required environment variables \uf0c1 IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for BAS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for BAS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for BAS CPD_ENTITLEMENT_KEY Lookup your entitlement key from the [IBM Container Library](https://myibm.ibm.com/ Optional environment variables \uf0c1 IBMCLOUD_RESOURCEGROUP creates an IBM Cloud resource group to be used, if none are passed, Default resource group will be used. OCP_VERSION to override the default version of OCP to use (latest 4.6 release) W3_USERNAME to enable access to pre-release development builds of MAS ARTIFACTORY_APIKEY to enable access to pre-release development builds of MAS MONGODB_NAMESPACE overrides the Kubernetes namespace where the MongoDb CE operator will be installed, this will default to mongoce MAS_CATALOG_SOURCE to override the use of the IBM Operator Catalog as the catalog source MAS_CHANNEL to override the use of the 8.x channel MAS_DOMAIN to set a custom domain for the MAS installation MAS_UPGRADE_STRATEGY to override the use of Manual upgrade strategy. MAS_ICR_CP to override the value MAS uses for the IBM Entitled Registry ( cp.icr.io/cp ) MAS_ICR_CPOPEN to override the value MAS uses for the IBM Open Registry ( icr.io/cpopen ) MAS_ENTITLEMENT_USERNAME to override the username MAS uses to access content in the IBM Entitled Registry MAS_APPWS_COMPONENTS to customize the application components installed in the Manage Workspace CIS_CRN to enable integration with IBM Cloud Internet Services (CIS) for DNS & certificate management CIS_SUBDOMAIN if you want to use a subdomain within your CIS instance Tip MAS_ICR_CP , MAS_ICR_CPOPEN , & MAS_ENTITLEMENT_USERNAME are primarily used when working with pre-release builds in conjunction with W3_USERNAME , ARTIFACTORY_APIKEY and the MAS_CATALOG_SOURCE environment variables. Tip By default only the base Manage component is installed. To customise the components that are enabled use the optional MAS_APPWS_COMPONENTS environment variable, for example to enable Health set it to the following: export MAS_APPWS_COMPONENTS=\"{'base':{'version':'latest'}, 'health':{'version':'latest'}}\" To install Health as a Standalone with a specified version, set MAS_APP_ID to health and set MAS_APPWS_COMPONENTS to \"{'health':{'version':'x.x.x'}}\" . The default version when installing health is set to the latest version. Release build \uf0c1 The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export BAS_CONTACT_MAIL=xxx@xxx.com export BAS_CONTACT_FIRSTNAME=xxx export BAS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-manage-roks.yml Pre-release build \uf0c1 The simplest configuration to deploy a pre-release build (only available to IBM employees) of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m1dev87 export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export BAS_CONTACT_MAIL=xxx@xxx.com export BAS_CONTACT_FIRSTNAME=xxx export BAS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-manage-roks.yml","title":"Manage on IBM Cloud"},{"location":"playbooks/lite-manage-roks/#mas-core-with-manage-on-ibm-cloud","text":"This master playbook will drive the following playbooks in sequence: Provision & setup OCP on IBM Cloud (20-30 minutes) Update Default Cluster Pull Secret and Reboot Worker Nodes (10 minutes) Install dependencies: Install MongoDb (Community Edition) (10 minutes) Install SLS (10 minutes) Install UDS (35 minutes) Install Cloud Pak for Data Operator (2 minutes) Install Cloud Pak for Data Services: Db2 Warehouse (1 hour) Create Db2 Warehouse Cluster (45 minutes) Additional Db2 configuration for Manage (5 minutes) Install & configure MAS: Configure Cloud Internet Services integration (Optional, 1 minute) Generate MAS Workspace Configuration (1 minute) Install & configure MAS (15 minutes) Install Manage application: Install application (10 minutes) Configure workspace (2 hours) All timings are estimates, see the individual pages for each of these playbooks for more information. Use this sample playbook as a starting point for installing any MAS application, just customize the application install and configure stages at the end of the playbook.","title":"MAS Core with Manage on IBM Cloud"},{"location":"playbooks/lite-manage-roks/#preparation","text":"Before you run the playbook you need to configure a few things in your MAS_CONFIG_DIR :","title":"Preparation"},{"location":"playbooks/lite-manage-roks/#prepare-your-entitlement-license-key-file","text":"First, set SLS_LICENSE_ID to the correct ID (a 12 character hex string) from your entitlement file, then copy the MAS license key file that you obtained from Rational License Key Server to $MAS_CONFIG_DIR/entitlement.lic (the file must have this exact name). During the installation of SLS this license file will be automatically bootstrapped into the system. Tip If you do not already have an entitlement file, create a random 12 character hex string and use this as the license ID when requesting your entitlement file from Rational License Key Server.","title":"Prepare your entitlement license key file"},{"location":"playbooks/lite-manage-roks/#required-environment-variables","text":"IBMCLOUD_APIKEY The API key that will be used to create a new ROKS cluster in IBMCloud CLUSTER_NAME The name to assign to the new ROKS cluster MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) SLS_LICENSE_ID The license ID must match the license file available in $MAS_CONFIG_DIR/entitlement.lic SLS_ENTITLEMENT_KEY Lookup your entitlement key from the IBM Container Library UDS_CONTACT_EMAIL Defines the email for person to contact for BAS UDS_CONTACT_FIRSTNAME Defines the first name of the person to contact for BAS UDS_CONTACT_LASTNAME Defines the last name of the person to contact for BAS CPD_ENTITLEMENT_KEY Lookup your entitlement key from the [IBM Container Library](https://myibm.ibm.com/","title":"Required environment variables"},{"location":"playbooks/lite-manage-roks/#optional-environment-variables","text":"IBMCLOUD_RESOURCEGROUP creates an IBM Cloud resource group to be used, if none are passed, Default resource group will be used. OCP_VERSION to override the default version of OCP to use (latest 4.6 release) W3_USERNAME to enable access to pre-release development builds of MAS ARTIFACTORY_APIKEY to enable access to pre-release development builds of MAS MONGODB_NAMESPACE overrides the Kubernetes namespace where the MongoDb CE operator will be installed, this will default to mongoce MAS_CATALOG_SOURCE to override the use of the IBM Operator Catalog as the catalog source MAS_CHANNEL to override the use of the 8.x channel MAS_DOMAIN to set a custom domain for the MAS installation MAS_UPGRADE_STRATEGY to override the use of Manual upgrade strategy. MAS_ICR_CP to override the value MAS uses for the IBM Entitled Registry ( cp.icr.io/cp ) MAS_ICR_CPOPEN to override the value MAS uses for the IBM Open Registry ( icr.io/cpopen ) MAS_ENTITLEMENT_USERNAME to override the username MAS uses to access content in the IBM Entitled Registry MAS_APPWS_COMPONENTS to customize the application components installed in the Manage Workspace CIS_CRN to enable integration with IBM Cloud Internet Services (CIS) for DNS & certificate management CIS_SUBDOMAIN if you want to use a subdomain within your CIS instance Tip MAS_ICR_CP , MAS_ICR_CPOPEN , & MAS_ENTITLEMENT_USERNAME are primarily used when working with pre-release builds in conjunction with W3_USERNAME , ARTIFACTORY_APIKEY and the MAS_CATALOG_SOURCE environment variables. Tip By default only the base Manage component is installed. To customise the components that are enabled use the optional MAS_APPWS_COMPONENTS environment variable, for example to enable Health set it to the following: export MAS_APPWS_COMPONENTS=\"{'base':{'version':'latest'}, 'health':{'version':'latest'}}\" To install Health as a Standalone with a specified version, set MAS_APP_ID to health and set MAS_APPWS_COMPONENTS to \"{'health':{'version':'x.x.x'}}\" . The default version when installing health is set to the latest version.","title":"Optional environment variables"},{"location":"playbooks/lite-manage-roks/#release-build","text":"The simplest configuration to deploy a release build of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # MAS configuration export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ENTITLEMENT_KEY=xxx export MAS_CONFIG_DIR=~/masconfig # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export BAS_CONTACT_MAIL=xxx@xxx.com export BAS_CONTACT_FIRSTNAME=xxx export BAS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-manage-roks.yml","title":"Release build"},{"location":"playbooks/lite-manage-roks/#pre-release-build","text":"The simplest configuration to deploy a pre-release build (only available to IBM employees) of IBM Maximo Application Suite (core only) with dependencies is: # IBM Cloud ROKS configuration export IBMCLOUD_APIKEY=xxx export CLUSTER_NAME=xxx # Allow development catalogs to be installed export W3_USERNAME=xxx export ARTIFACTORY_APIKEY=xxx # MAS configuration export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=m1dev87 export MAS_INSTANCE_ID=$CLUSTER_NAME export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig # CP4D configuration export CPD_ENTITLEMENT_KEY=xxx # SLS configuration export SLS_ENTITLEMENT_KEY=xxx export SLS_LICENSE_ID=xxx # BAS configuration export BAS_CONTACT_MAIL=xxx@xxx.com export BAS_CONTACT_FIRSTNAME=xxx export BAS_CONTACT_LASTNAME=xxx ansible-playbook playbooks/lite-manage-roks.yml","title":"Pre-release build"},{"location":"playbooks/mas/","text":"MAS Playbooks \uf0c1 Install MAS \uf0c1 Before you use this playbook you will likely want to edit the mas_config variable to supply your own configurtation, instead of the sample data provided. Required environment variables \uf0c1 MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Provide your IBM entitlement key MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here) Optional environment variables \uf0c1 MAS_CATALOG_SOURCE Set to ibm-mas-operators if you want to deploy pre-release development builds MAS_CHANNEL Override the default release channel (8.x) MAS_DOMAIN Override the default generated domain for the MAS installation MAS_ICR_CP Override the registry source for all container images deployed by the MAS operator MAS_ICR_CPOPEN Override the registry source for all container images deployed by the MAS operator MAS_ENTITLEMENT_USERNAME Override the default entitlement username (cp) MAS_UPGRADE_STRATEGY Override the use of Manual upgrade strategy. Note Example usage: release build \uf0c1 export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx ansible-playbook playbooks/mas/install-suite.yml Note Lookup your entitlement key from the IBM Container Library Example usage: pre-release build \uf0c1 export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=8.5.0-pre.m2dev85 export MAS_INSTANCE_ID=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/mas/install-suite.yml Important You must have already installed the development (pre-release) catalogs, pre-release builds are not available directly from the IBM Operator Catalog. Cloud Internet Services integration \uf0c1 This optional feature allows you to integrate MAS with an existing instance of IBM Cloud Internet Services (CIS) to provide automatic DNS management and certificates signed by LetsEncrypt. To utilise this feature you must set the optional MAS_DOMAIN detailed previously, and define additional CIS-specific environment variables as follows. Required environment variables \uf0c1 CIS_CRN which can be obtained from your CIS service overview page, it will be in the format: crn:v1:bluemix:public:internet-svcs:global:a/02fd888448c1415baa2bcd65684e4db3:9969652f-6955-482b-b59c-asdasasdede50c:: CIS_APIKEY A Service ID API key with DNS API Editor/Manager access. Note: (This API key will be stored in your cluster for DNS challenge when requesting new certs) Process to create a strict api key that can only access the Internet Services service: - Create a Service ID - Create an \"Access Policy\" with the following entries: - Scope of access set to Resources based on selected attributes - Platform access set to Editor - Service access set to Manager - Create an API KEY for the Service ID, which is the CIS_APIKEY Information on Service IDs Optional environment variables \uf0c1 CIS_SUBDOMAIN Subdomain used by your DNS server. It allow you to reuse CIS for multiple MAS Instances. CIS_SKIP_DNS_ENTRIES Skips DNS entries creation if you are have them CIS_SKIP_CLUSTER_ISSUER Skips Cluster Issuer CR creation and CIS webhook installation if you already have it UPDATE_DNS_ENTRIES Whether to replace DNS entries already created OCP_INGRESS Default to your cluster OCP ingress. This value is used as the target for the DNS entries Example \uf0c1 This example will configure MAS to run under the domain mas.internal.mydomain.com with all DNS entries for MAS managed by a CIS instance controlling mydomain.com . export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_DOMAIN=mas.internal.mydomain.com # Configure CIS integration export IBMCLOUD_APIKEY=xxx export CIS_SUBDOMAIN=mas.internal export CIS_CRN=crn:v1:bluemix:public:internet-svcs:global:a/02fd888448c1415baa2bcd65684e4db3:9969652f-6955-482b-b59c-asdasasdede50c:: ansible-playbook playbooks/mas/install-suite.yml Install MAS Application \uf0c1 Install a MAS (Gen2) application, supported applications: Manage Health Predict MSO IoT Monitor Safety Note Today, this only supports deployment of a MAS application with default settings. Example \uf0c1 export MAS_INSTANCE_ID=xxx export MAS_WORKSPACE_ID=masdev export MAS_APP_ID=manage ansible-playbook playbooks/mas/configure-app.yml Configure MAS Application \uf0c1 Configure a MAS (Gen2) application in a workspace, supported applications: Manage ( base only - see Configure Manage Application ) Health Predict MSO IoT Monitor Safety Note Today, this only supports configuring a workspace with default settings. Example \uf0c1 The following call will install Manage with latest versions of Health and Service Provider components enabled. export MAS_INSTANCE_ID=xxx export MAS_WORKSPACE_ID=masdev export MAS_APP_ID=manage export MAS_APPWS_COMPONENTS=\"{'base':{'version':'latest'},'health':{'version':'latest'},'serviceprovider':{'version':'latest'}}\" ansible-playbook playbooks/mas/configure-app.yml Manage Db2 Hack \uf0c1 This should should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. This will configure the Db2 database instance and create a new schema named maximo (the default schema name used by the Manage application) as well as SQL instructions to prepare database for Manage installation. The parameters are all optional: CPD_NAMESPACE namespace where Cloud Pak for Data is installed. Default is cpd_meta_namespace DB2WH_INSTANCE_NAME name of the DB2 Warehouse instance created in CP4D. Examples \uf0c1 (...) ansible-playbook playbooks/cp4d/install-services-db2.yml ansible-playbook playbooks/mas/hack-manage-db2.yml (...) ansible-playbook playbooks/cp4d/create-db2-instance.yml export CPD_DB2WH_INSTANCE_NAME=db2w-iot ansible-playbook playbooks/mas/hack-manage-db2.yml","title":"MAS Playbooks"},{"location":"playbooks/mas/#mas-playbooks","text":"","title":"MAS Playbooks"},{"location":"playbooks/mas/#install-mas","text":"Before you use this playbook you will likely want to edit the mas_config variable to supply your own configurtation, instead of the sample data provided.","title":"Install MAS"},{"location":"playbooks/mas/#required-environment-variables","text":"MAS_INSTANCE_ID Declare the instance ID for the MAS install MAS_ENTITLEMENT_KEY Provide your IBM entitlement key MAS_CONFIG_DIR Directory where generated config files will be saved (you may also provide pre-generated config files here)","title":"Required environment variables"},{"location":"playbooks/mas/#optional-environment-variables","text":"MAS_CATALOG_SOURCE Set to ibm-mas-operators if you want to deploy pre-release development builds MAS_CHANNEL Override the default release channel (8.x) MAS_DOMAIN Override the default generated domain for the MAS installation MAS_ICR_CP Override the registry source for all container images deployed by the MAS operator MAS_ICR_CPOPEN Override the registry source for all container images deployed by the MAS operator MAS_ENTITLEMENT_USERNAME Override the default entitlement username (cp) MAS_UPGRADE_STRATEGY Override the use of Manual upgrade strategy. Note","title":"Optional environment variables"},{"location":"playbooks/mas/#example-usage-release-build","text":"export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx ansible-playbook playbooks/mas/install-suite.yml Note Lookup your entitlement key from the IBM Container Library","title":"Example usage: release build"},{"location":"playbooks/mas/#example-usage-pre-release-build","text":"export MAS_CATALOG_SOURCE=ibm-mas-operators export MAS_CHANNEL=8.5.0-pre.m2dev85 export MAS_INSTANCE_ID=xxx export MAS_ICR_CP=wiotp-docker-local.artifactory.swg-devops.com export MAS_ICR_CPOPEN=wiotp-docker-local.artifactory.swg-devops.com export MAS_ENTITLEMENT_USERNAME=$W3_USERNAME_LOWERCASE export MAS_ENTITLEMENT_KEY=$ARTIFACTORY_APIKEY export MAS_CONFIG_DIR=~/masconfig ansible-playbook playbooks/mas/install-suite.yml Important You must have already installed the development (pre-release) catalogs, pre-release builds are not available directly from the IBM Operator Catalog.","title":"Example usage: pre-release build"},{"location":"playbooks/mas/#cloud-internet-services-integration","text":"This optional feature allows you to integrate MAS with an existing instance of IBM Cloud Internet Services (CIS) to provide automatic DNS management and certificates signed by LetsEncrypt. To utilise this feature you must set the optional MAS_DOMAIN detailed previously, and define additional CIS-specific environment variables as follows.","title":"Cloud Internet Services integration"},{"location":"playbooks/mas/#required-environment-variables_1","text":"CIS_CRN which can be obtained from your CIS service overview page, it will be in the format: crn:v1:bluemix:public:internet-svcs:global:a/02fd888448c1415baa2bcd65684e4db3:9969652f-6955-482b-b59c-asdasasdede50c:: CIS_APIKEY A Service ID API key with DNS API Editor/Manager access. Note: (This API key will be stored in your cluster for DNS challenge when requesting new certs) Process to create a strict api key that can only access the Internet Services service: - Create a Service ID - Create an \"Access Policy\" with the following entries: - Scope of access set to Resources based on selected attributes - Platform access set to Editor - Service access set to Manager - Create an API KEY for the Service ID, which is the CIS_APIKEY Information on Service IDs","title":"Required environment variables"},{"location":"playbooks/mas/#optional-environment-variables_1","text":"CIS_SUBDOMAIN Subdomain used by your DNS server. It allow you to reuse CIS for multiple MAS Instances. CIS_SKIP_DNS_ENTRIES Skips DNS entries creation if you are have them CIS_SKIP_CLUSTER_ISSUER Skips Cluster Issuer CR creation and CIS webhook installation if you already have it UPDATE_DNS_ENTRIES Whether to replace DNS entries already created OCP_INGRESS Default to your cluster OCP ingress. This value is used as the target for the DNS entries","title":"Optional environment variables"},{"location":"playbooks/mas/#example","text":"This example will configure MAS to run under the domain mas.internal.mydomain.com with all DNS entries for MAS managed by a CIS instance controlling mydomain.com . export MAS_INSTANCE_ID=xxx export MAS_ENTITLEMENT_KEY=xxx export MAS_DOMAIN=mas.internal.mydomain.com # Configure CIS integration export IBMCLOUD_APIKEY=xxx export CIS_SUBDOMAIN=mas.internal export CIS_CRN=crn:v1:bluemix:public:internet-svcs:global:a/02fd888448c1415baa2bcd65684e4db3:9969652f-6955-482b-b59c-asdasasdede50c:: ansible-playbook playbooks/mas/install-suite.yml","title":"Example"},{"location":"playbooks/mas/#install-mas-application","text":"Install a MAS (Gen2) application, supported applications: Manage Health Predict MSO IoT Monitor Safety Note Today, this only supports deployment of a MAS application with default settings.","title":"Install MAS Application"},{"location":"playbooks/mas/#example_1","text":"export MAS_INSTANCE_ID=xxx export MAS_WORKSPACE_ID=masdev export MAS_APP_ID=manage ansible-playbook playbooks/mas/configure-app.yml","title":"Example"},{"location":"playbooks/mas/#configure-mas-application","text":"Configure a MAS (Gen2) application in a workspace, supported applications: Manage ( base only - see Configure Manage Application ) Health Predict MSO IoT Monitor Safety Note Today, this only supports configuring a workspace with default settings.","title":"Configure MAS Application"},{"location":"playbooks/mas/#example_2","text":"The following call will install Manage with latest versions of Health and Service Provider components enabled. export MAS_INSTANCE_ID=xxx export MAS_WORKSPACE_ID=masdev export MAS_APP_ID=manage export MAS_APPWS_COMPONENTS=\"{'base':{'version':'latest'},'health':{'version':'latest'},'serviceprovider':{'version':'latest'}}\" ansible-playbook playbooks/mas/configure-app.yml","title":"Example"},{"location":"playbooks/mas/#manage-db2-hack","text":"This should should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. This will configure the Db2 database instance and create a new schema named maximo (the default schema name used by the Manage application) as well as SQL instructions to prepare database for Manage installation. The parameters are all optional: CPD_NAMESPACE namespace where Cloud Pak for Data is installed. Default is cpd_meta_namespace DB2WH_INSTANCE_NAME name of the DB2 Warehouse instance created in CP4D.","title":"Manage Db2 Hack"},{"location":"playbooks/mas/#examples","text":"(...) ansible-playbook playbooks/cp4d/install-services-db2.yml ansible-playbook playbooks/mas/hack-manage-db2.yml (...) ansible-playbook playbooks/cp4d/create-db2-instance.yml export CPD_DB2WH_INSTANCE_NAME=db2w-iot ansible-playbook playbooks/mas/hack-manage-db2.yml","title":"Examples"},{"location":"playbooks/ocp/","text":"OCP Playbooks \uf0c1 Provision \uf0c1 Refer to the ocp_provision role documentation for more information. export CLUSTER_NAME=masinst1 export OCP_VERSION=\"4.8_openshift\" export IBMCLOUD_APIKEY=xxx ansible-playbook playbooks/ocp/provision-roks.yml export CLUSTER_NAME=masinst1 export OCP_VERSION=4.6.16 export FYRE_USERNAME=xxx export FYRE_PASSWORD=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook playbooks/ocp/provision-quickburn.yml Deprovision \uf0c1 Refer to the ocp_deprovision role documentation for more information. TODO: Update the role doc as it's rubbish atm export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook playbooks/ocp/deprovision-roks.yml export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_PASSWORD=xxx ansible-playbook playbooks/ocp/deprovision-quickburn.yml Configure \uf0c1 Refer to the ocp_setup_mas_deps role documentation for more information. TODO: Provide example (and update the role doc as it's rubbish atm) export VAR1=xxx ansible-playbook playbooks/ocp/configure-ocp.yml Verify \uf0c1 Refer to the ocp_verify role documentation for more information. TODO: Provide example (and update the role doc as it's rubbish atm) export VAR1=xxx ansible-playbook playbooks/ocp/verify-roks.yml","title":"OCP Playbooks"},{"location":"playbooks/ocp/#ocp-playbooks","text":"","title":"OCP Playbooks"},{"location":"playbooks/ocp/#provision","text":"Refer to the ocp_provision role documentation for more information. export CLUSTER_NAME=masinst1 export OCP_VERSION=\"4.8_openshift\" export IBMCLOUD_APIKEY=xxx ansible-playbook playbooks/ocp/provision-roks.yml export CLUSTER_NAME=masinst1 export OCP_VERSION=4.6.16 export FYRE_USERNAME=xxx export FYRE_PASSWORD=xxx export FYRE_PRODUCT_ID=xxx ansible-playbook playbooks/ocp/provision-quickburn.yml","title":"Provision"},{"location":"playbooks/ocp/#deprovision","text":"Refer to the ocp_deprovision role documentation for more information. TODO: Update the role doc as it's rubbish atm export CLUSTER_NAME=masinst1 export IBMCLOUD_APIKEY=xxx ansible-playbook playbooks/ocp/deprovision-roks.yml export CLUSTER_NAME=masinst1 export FYRE_USERNAME=xxx export FYRE_PASSWORD=xxx ansible-playbook playbooks/ocp/deprovision-quickburn.yml","title":"Deprovision"},{"location":"playbooks/ocp/#configure","text":"Refer to the ocp_setup_mas_deps role documentation for more information. TODO: Provide example (and update the role doc as it's rubbish atm) export VAR1=xxx ansible-playbook playbooks/ocp/configure-ocp.yml","title":"Configure"},{"location":"playbooks/ocp/#verify","text":"Refer to the ocp_verify role documentation for more information. TODO: Provide example (and update the role doc as it's rubbish atm) export VAR1=xxx ansible-playbook playbooks/ocp/verify-roks.yml","title":"Verify"},{"location":"roles/amqstreams/","text":"amqstreams \uf0c1 This role provides support to install a Kafka Cluster using Red Hat AMQ Streams and generate configuration that can be directly applied to Maximo Application Suite. The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role. Role Variables \uf0c1 kafka_namespace \uf0c1 The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams kafka_cluster_name \uf0c1 The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka kafka_cluster_size \uf0c1 The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small kafka_storage_class \uf0c1 Required. The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_CLASS Default Value: None kafka_user_name \uf0c1 The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.amqstreams License \uf0c1 EPL-2.0","title":"amqstreams"},{"location":"roles/amqstreams/#amqstreams","text":"This role provides support to install a Kafka Cluster using Red Hat AMQ Streams and generate configuration that can be directly applied to Maximo Application Suite. The Red Hat AMQ streams component is a massively scalable, distributed, and high-performance data streaming platform based on the Apache Kafka project. It offers a distributed backbone that allows microservices and other applications to share data with high throughput and low latency. As more applications move to Kubernetes and Red Hat OpenShift, it is increasingly important to be able to run the communication infrastructure on the same platform. Red Hat OpenShift, as a highly scalable platform, is a natural fit for messaging technologies such as Kafka. The AMQ streams component makes running and managing Apache Kafka OpenShift native through the use of powerful operators that simplify the deployment, configuration, management, and use of Apache Kafka on Red Hat OpenShift. The AMQ streams component is part of the Red Hat AMQ family, which also includes the AMQ broker, a longtime innovation leader in Java\u2122 Message Service (JMS) and polyglot messaging, as well as the AMQ interconnect router, a wide-area, peer-to-peer messaging solution. Tip The role will generate a yaml file containing the definition of a Secret and KafkaCfg resource that can be used to configure the deployed cluster as the MAS system Kafka. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/kafkacfg-amqstreams-system.yaml or used in conjunction with the suite_config role.","title":"amqstreams"},{"location":"roles/amqstreams/#role-variables","text":"","title":"Role Variables"},{"location":"roles/amqstreams/#kafka_namespace","text":"The namespace where the operator and Kafka cluster will be deployed. Environment Variable: KAFKA_NAMESPACE Default Value: amq-streams","title":"kafka_namespace"},{"location":"roles/amqstreams/#kafka_cluster_name","text":"The name of the Kafka cluster that will be created Environment Variable: KAFKA_CLUSTER_NAME Default Value: maskafka","title":"kafka_cluster_name"},{"location":"roles/amqstreams/#kafka_cluster_size","text":"The configuration to apply, there are two configurations available: small and large. Environment Variable: KAFKA_CLUSTER_SIZE Default Value: small","title":"kafka_cluster_size"},{"location":"roles/amqstreams/#kafka_storage_class","text":"Required. The name of the storage class to configure the AMQStreams operator to use for persistent storage in the Kafka cluster. Environment Variable: KAFKA_STORAGE_CLASS Default Value: None","title":"kafka_storage_class"},{"location":"roles/amqstreams/#kafka_user_name","text":"The name of the user to setup in the cluster for MAS. Environment Variable: KAFKA_USER_NAME Default Value: masuser","title":"kafka_user_name"},{"location":"roles/amqstreams/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the KafkaCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/amqstreams/#mas_config_dir","text":"Local directory to save the generated KafkaCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a KafkaCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/amqstreams/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Set storage class suitable for use on IBM Cloud ROKS kafka_storage_class: ibmc-block-gold # Generate a KafkaCfg template mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.amqstreams","title":"Example Playbook"},{"location":"roles/amqstreams/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ansible_version_check/","text":"ansible_version_check \uf0c1 Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used. License \uf0c1 EPL-2.0","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#ansible_version_check","text":"Internal-use role that all other roles in the collection declare a dependency upon to ensure that the minimum supported level of Ansible is used.","title":"ansible_version_check"},{"location":"roles/ansible_version_check/#license","text":"EPL-2.0","title":"License"},{"location":"roles/appconnect_install/","text":"appconnect_install \uf0c1 Installs IBM AppConnect on IBM Cloud Openshift Clusters (ROKS) and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by HP Utilities application. Role Variables \uf0c1 appconnect_namespace \uf0c1 Optional - Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect appconnect_channel \uf0c1 Optional - AppConnect subscription channel to be installed. For MAS 8.7, default v3.0 channel is used. Ensure the supported dashboard version and license ID according to the AppConnect operator/channel version. MAS 8.4 uses AppConnect channel subscription v1.4 and license id = L-APEH-BSVCHU MAS 8.5 uses AppConnect channel subscription v2.0 and license id = L-KSBM-BZWEAT MAS 8.6 uses AppConnect channel subscription v2.0 and license id = L-KSBM-C37J2R MAS 8.7 uses AppConnect channel subscription v3.0 and license id = L-KSBM-C87FU2 For more details: https://www.ibm.com/support/knowledgecenter/SSTTDS_11.0.0/com.ibm.ace.icp.doc/certc_licensingreference.html Environment Variable: APPCONNECT_CHANNEL Default Value: v3.0 appconnect_dashboard_name \uf0c1 Optional - AppConnect Dashboard instance name. Default to dashboard-12020r2 as a reference to AppConnect Dashboard version 12.0.2.0-r2 that is compatible with AppConnect Channel subscription v3.0 and license ID L-KSBM-C87FU2 (MAS 8.7 compatible) For More details: https://www.ibm.com/docs/en/app-connect/containers_cd?topic=resources-licensing-reference-app-connect-operator Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12020r2 appconnect_license_id \uf0c1 Optional - AppConnect license ID. Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-KSBM-C87FU2 (Which is compatible with AppConnect v3.0 operator/channel) appconnect_storage_class \uf0c1 Required - Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid can be used. Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None appconnect_entitlement_username \uf0c1 Optional - Holds your IBM Entitlement username. Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default Value: cp appconnect_entitlement_key \uf0c1 Required - Holds your IBM Entitlement key. Environment Variable: APPCONNECT_ENTITLEMENT_KEY Default Value: None mas_instance_id \uf0c1 Optional - The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Optional - Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.appconnect_install License \uf0c1 EPL-2.0","title":"appconnect_install"},{"location":"roles/appconnect_install/#appconnect_install","text":"Installs IBM AppConnect on IBM Cloud Openshift Clusters (ROKS) and generates configuration that can be directly applied to IBM Maximo Application Suite. This dependency is required by HP Utilities application.","title":"appconnect_install"},{"location":"roles/appconnect_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/appconnect_install/#appconnect_namespace","text":"Optional - Defines the targetted cluster namespace/project where AppConnect will be installed. If not provided, default AppConnect namespace will be ibm-app-connect . Environment Variable: APPCONNECT_NAMESPACE Default Value: ibm-app-connect","title":"appconnect_namespace"},{"location":"roles/appconnect_install/#appconnect_channel","text":"Optional - AppConnect subscription channel to be installed. For MAS 8.7, default v3.0 channel is used. Ensure the supported dashboard version and license ID according to the AppConnect operator/channel version. MAS 8.4 uses AppConnect channel subscription v1.4 and license id = L-APEH-BSVCHU MAS 8.5 uses AppConnect channel subscription v2.0 and license id = L-KSBM-BZWEAT MAS 8.6 uses AppConnect channel subscription v2.0 and license id = L-KSBM-C37J2R MAS 8.7 uses AppConnect channel subscription v3.0 and license id = L-KSBM-C87FU2 For more details: https://www.ibm.com/support/knowledgecenter/SSTTDS_11.0.0/com.ibm.ace.icp.doc/certc_licensingreference.html Environment Variable: APPCONNECT_CHANNEL Default Value: v3.0","title":"appconnect_channel"},{"location":"roles/appconnect_install/#appconnect_dashboard_name","text":"Optional - AppConnect Dashboard instance name. Default to dashboard-12020r2 as a reference to AppConnect Dashboard version 12.0.2.0-r2 that is compatible with AppConnect Channel subscription v3.0 and license ID L-KSBM-C87FU2 (MAS 8.7 compatible) For More details: https://www.ibm.com/docs/en/app-connect/containers_cd?topic=resources-licensing-reference-app-connect-operator Environment Variable: APPCONNECT_DASHBOARD_NAME Default Value: dashboard-12020r2","title":"appconnect_dashboard_name"},{"location":"roles/appconnect_install/#appconnect_license_id","text":"Optional - AppConnect license ID. Environment Variable: APPCONNECT_LICENSE_ID Default Value: L-KSBM-C87FU2 (Which is compatible with AppConnect v3.0 operator/channel)","title":"appconnect_license_id"},{"location":"roles/appconnect_install/#appconnect_storage_class","text":"Required - Storage class where AppConnect will be installed - for IBM Cloud clusters, ibmc-file-gold-gid can be used. Environment Variable: APPCONNECT_STORAGE_CLASS Default Value: None","title":"appconnect_storage_class"},{"location":"roles/appconnect_install/#appconnect_entitlement_username","text":"Optional - Holds your IBM Entitlement username. Environment Variable: APPCONNECT_ENTITLEMENT_USERNAME Default Value: cp","title":"appconnect_entitlement_username"},{"location":"roles/appconnect_install/#appconnect_entitlement_key","text":"Required - Holds your IBM Entitlement key. Environment Variable: APPCONNECT_ENTITLEMENT_KEY Default Value: None","title":"appconnect_entitlement_key"},{"location":"roles/appconnect_install/#mas_instance_id","text":"Optional - The instance ID of Maximo Application Suite that the AppConnect configuration will target. If this or mas_config_dir are not set then the role will not generate an AppConnect template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/appconnect_install/#mas_config_dir","text":"Optional - Local directory to save the generated AppConnect resource definition. This can be used to manually configure a MAS instance to connect to AppConnect instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate an AppConnect template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/appconnect_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.appconnect_install","title":"Example Playbook"},{"location":"roles/appconnect_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cos_setup/","text":"cos_setup \uf0c1 This role provides support for Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes. Role Variables \uf0c1 cos_type \uf0c1 Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None cos_instance_name \uf0c1 Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name. ibmcloud_apikey \uf0c1 Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \uf0c1 Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm # MAS instance and config dir mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup License \uf0c1 EPL-2.0","title":"cos_setup"},{"location":"roles/cos_setup/#cos_setup","text":"This role provides support for Configuring Cloud Object Storage in MAS. It currently supports two providers: In-cluster Ceph Object Storage leveraging OpenShift Container Storage IBM Cloud Object Storage Currently this role only supports generating a system-scoped ObjectStorageCfg resource, but the generated file can be modified if you wish to use other scopes.","title":"cos_setup"},{"location":"roles/cos_setup/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cos_setup/#cos_type","text":"Required. Which COS provider to use; can be set to either ibm for IBM Cloud Object Storage or ocs for OpenShift Container Storage Environment Variable: COS_TYPE Default Value: None","title":"cos_type"},{"location":"roles/cos_setup/#cos_instance_name","text":"Provide an optional name for the Object Storage instance. This is only used when cos_type is set to ibm for IBM Cloud Object Storage. Environment Variable: COS_INSTANCE_NAME Default Value: Object Storage for MAS , if mas_instance_id is set the MAS instance ID will be appended to this name.","title":"cos_instance_name"},{"location":"roles/cos_setup/#ibmcloud_apikey","text":"Required if cos_type is set to ibm . Provide your IBM Cloud API Key. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cos_setup/#ibmcloud_resourcegroup","text":"Only used when cos_type is set to ibm . Provide the name of the resource group which will own the COS instance. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/cos_setup/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the ObjectStorageCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cos_setup/#mas_config_dir","text":"Local directory to save the generated ObjectStorageCfg resource definition. This can be used to manually configure a MAS instance to connect to the Kafka cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a ObjectStorageCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cos_setup/#example-playbook","text":"Create the Ceph Object store on the existing OCS cluster and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ocs mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup Create the IBM Cloud Object storage Instance and prepare the objectstorageCfg yaml to mas_config_dir. - hosts: localhost any_errors_fatal: true vars: cos_type: ibm # MAS instance and config dir mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.cos_setup","title":"Example Playbook"},{"location":"roles/cos_setup/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_db2wh/","text":"cp4d_db2wh \uf0c1 This role creates a Db2 Warehouse instance in Cloud Pak for Data. A Db2 Warehouse cluster will be created and a public TLS encrypted route is configured to allow external access to the cluster. CloudPak for Data 3.5 \uf0c1 The certificates are available from the internal-tls secret in the cpd-meta-ops namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the cpd-meta-ops namespace: oc -n cpd-meta-ops get cpdservice,db2ucluster NAME MESSAGE REASON STATUS LASTACTION PHASE CODE cpdservice.metaoperator.cpd.ibm.com/cpdservice-db2wh Completed Ready CPDInstall Ready 0 cpdservice.metaoperator.cpd.ibm.com/cpdservice-db2wh-dmc CPD binary is running Installing CPDInstall Installing 1 NAME STATE AGE db2ucluster.db2u.databases.ibm.com/db2u-bludb NotReady 8m44s Debugging Db2 install problems \uf0c1 The following command may come in handy: oc -n cpd-meta-ops get formations.db2u.databases.ibm.com db2wh-db01 -o go-template='{{range .status.components}}{{printf \"%s,%s,%s\\n\" .kind .name .status.state}}{{end}}' | column -s, -t Tip The role will generate a yaml file containing the definition of a Secret and JdbcCfg resource that can be used to configure the deployed cluster as the MAS system JDBC datasource. This file can be directly applied using oc apply -f /tmp/jdbccfg-cp4ddb2wh-system.yaml or added to the mas_config list variable used by the ibm.mas_devops.suite_install role to deploy and configure MAS. Installing DB2WH on NFS based volumes \uf0c1 DB2 requires you to set no_root_squash when you use NFS with IBM\u00ae Cloud File Storage (ibmc-file-gold-gid storage class) or the restore morph job fails. This role will install a daemonset which applies the no_root_squash NFS mount option on NFS volumes. For more details see the following references: - https://www.ibm.com/docs/en/db2/11.5?topic=requirements-cloud-file-storage - https://www.ibm.com/docs/en/db2/11.5?topic=cds-nfs-storage-requirements-1 - https://cloud.ibm.com/docs/FileStorage?topic=FileStorage-mountingLinux&interface=ui#norootsquash Role Variables \uf0c1 db2wh_namespace \uf0c1 Only supported when using CloudPak for Data v4.0, otherwise unused. For v3.5 support the value is always set to cpd-meta-ops . Environment Variable: DB2WH_NAMESPACE Default Value: cpd-services db2wh_instance_name \uf0c1 Required. Name of the database instance, note that this is the instance name , which is different from the instance ID . Environment Variable: DB2WH_INSTANCE_NAME Default: None db2wh_version \uf0c1 Version of the DB2 Warehouse instance to be created. Environment Variable: DB2WH_VERSION Default: 11.5.5.1-cn3-x86_64 (CloudPak for Data v3.5), 11.5.6.0-cn3 (CloudPak for Data v4) db2wh_table_org \uf0c1 The way database tables will be organized. It can be set to either ROW or COLUMN . Environment Variable: DB2WH_TABLE_ORG Default: ROW db2wh_meta_storage_class \uf0c1 Required for both CP4D v3.5 and CP4D v4. Storage class used for metadata. Environment Variable: DB2WH_META_STORAGE_CLASS Default: None db2wh_meta_storage_size_gb \uf0c1 Size of the metadata persistent volume, in gigabytes Environment Variable: DB2WH_META_STORAGE_SIZE_GB Default: 20 db2wh_user_storage_class \uf0c1 Required for both CP4D v3.5 and CP4D v4. Storage class used for user data. Environment Variable: DB2WH_USER_STORAGE_CLASS Default: None db2wh_user_storage_size_gb \uf0c1 Size of user persistent volume, in gigabytes. Environment Variable: DB2WH_USER_STORAGE_SIZE_GB Default: 100 db2wh_backup_storage_class \uf0c1 Required for both CP4D v3.5 and CP4D v4. Storage class used for backup. Environment Variable: DB2WH_BACKUP_STORAGE_CLASS Default: None db2wh_backup_storage_size_gb \uf0c1 Size of backup persistent volume, in gigabytes. Environment Variable: DB2WH_BACKUP_STORAGE_SIZE_GB Default: 100 db2wh_logs_storage_class \uf0c1 Required for CP4D v4 only. Storage class used for logs, not used with CP4D v3.5 databases. Environment Variable: DB2WH_LOGS_STORAGE_CLASS Default: None db2wh_logs_storage_size_gb \uf0c1 Size (in gigabytes) of logs persistent volume, not used with CP4D v3.5 databases. Environment Variable: DB2WH_LOGS_STORAGE_SIZE_GB Default: 100 db2wh_temp_storage_class \uf0c1 Required for CP4D v4 only. Storage class used for temporary data, not used with CP4D v3.5 databases. Environment Variable: DB2WH_TEMP_STORAGE_CLASS Default: None db2wh_temp_storage_size_gb \uf0c1 Size (in gigabytes) of temporary persistent volume, not used with CP4D v3.5 databases. Environment Variable: DB2WH_TEMP_STORAGE_SIZE_GB Default: 100 db2wh_cpu_requests \uf0c1 Define the Kubernetes CPU request for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_CPU_REQUESTS Default: 2000m db2wh_cpu_limits \uf0c1 Define the Kubernetes CPU limit for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_CPU_LIMITS Default: 4000m db2wh_memory_requests \uf0c1 Define the Kubernetes memory request for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_MEMORY_REQUESTS Default: 6Gi db2wh_memory_limits \uf0c1 Define the Kubernetes memory limit for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_MEMORY_LIMITS Default: 12Gi cpd_entitlement_key \uf0c1 Required. This is the entitlement key used to install the norootsquash Daemonset in the kube-system namespace. Holds your IBM Entitlement key. Environment Variable: CPD_ENTITLEMENT_KEY Default: None cpd_api_username \uf0c1 Required for CP4D v3.5 only. These credentials are used to call the REST API to create the database because CP4D v3.5 Kubernetes API is broken. Yes, the default admin account for CP4D v3.5 really is set up as admin/password. Environment Variable: CPD_API_USERNAME Default: admin cpd_api_password \uf0c1 Required for CP4D v3.5 only. These credentials are used to call the REST API to create the database because CP4D v3.5 Kubernetes API is broken. Yes, the default admin account for CP4D v3.5 really is set up as admin/password. Environment Variable: CPD_API_PASSWORD Default: password mas_instance_id \uf0c1 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None mas_config_dir \uf0c1 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_CONFIG_DIR Default: None mas_config_scope \uf0c1 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Environment Variable: MAS_CONFIG_SCOPE Default: system mas_workspace_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None mas_application_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Environment Variable: 'MAS_APP_ID Default: None db2wh_workload \uf0c1 The workload profile of the db2wh instance, possible values are 'PUREDATA_OLAP' or 'ANALYTICS' Environment Variable: 'DB2WH_WORKLOAD Default: 'ANALYTICS' db2wh_node_label \uf0c1 The label used to specify node affinity and tolerations in the db2ucluster CR. Environment Variable: 'DB2WH_NODE_LABEL Default: None db2wh_dedicated_node \uf0c1 The name of the worker node to apply the {{ db2wh_node_label }} taint and label to. Environment Variable: 'DB2WH_DEDICATED_NODE Default: None db2wh_mln_count \uf0c1 The number of logical nodes (i.e. database partitions to create). Environment Variable: 'DB2WH_MLN_COUNT Default: 1 db2wh_num_pods \uf0c1 The number of Db2 pods to create in the instance. Note that db2wh_num_pods must be less than or equal to db2wh_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2wh_mln_count while specifying a small number for db2wh_num_pods . If in doubt, make db2wh_mln_count = db2wh_num_pods . For more information refer to the Db2 documentation . Environment Variable: 'DB2WH_NUM_PODS Default: 1 Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Create a Db2 instance in CPD v4.0 db2wh_instance_name: db2wh-shared # Configure storage suitable for IBM Cloud ROKS db2wh_meta_storage_class: ibmc-file-silver-gid db2wh_user_storage_class: ibmc-file-gold-gid db2wh_backup_storage_class: ibmc-file-gold-gid db2wh_logs_storage_class: ibmc-file-silver-gid db2wh_temp_storage_class: ibmc-file-silver-gid # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.cp4d_db2wh License \uf0c1 EPL-2.0","title":"cp4d_db2wh"},{"location":"roles/cp4d_db2wh/#cp4d_db2wh","text":"This role creates a Db2 Warehouse instance in Cloud Pak for Data. A Db2 Warehouse cluster will be created and a public TLS encrypted route is configured to allow external access to the cluster.","title":"cp4d_db2wh"},{"location":"roles/cp4d_db2wh/#cloudpak-for-data-35","text":"The certificates are available from the internal-tls secret in the cpd-meta-ops namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the cpd-meta-ops namespace: oc -n cpd-meta-ops get cpdservice,db2ucluster NAME MESSAGE REASON STATUS LASTACTION PHASE CODE cpdservice.metaoperator.cpd.ibm.com/cpdservice-db2wh Completed Ready CPDInstall Ready 0 cpdservice.metaoperator.cpd.ibm.com/cpdservice-db2wh-dmc CPD binary is running Installing CPDInstall Installing 1 NAME STATE AGE db2ucluster.db2u.databases.ibm.com/db2u-bludb NotReady 8m44s","title":"CloudPak for Data 3.5"},{"location":"roles/cp4d_db2wh/#debugging-db2-install-problems","text":"The following command may come in handy: oc -n cpd-meta-ops get formations.db2u.databases.ibm.com db2wh-db01 -o go-template='{{range .status.components}}{{printf \"%s,%s,%s\\n\" .kind .name .status.state}}{{end}}' | column -s, -t Tip The role will generate a yaml file containing the definition of a Secret and JdbcCfg resource that can be used to configure the deployed cluster as the MAS system JDBC datasource. This file can be directly applied using oc apply -f /tmp/jdbccfg-cp4ddb2wh-system.yaml or added to the mas_config list variable used by the ibm.mas_devops.suite_install role to deploy and configure MAS.","title":"Debugging Db2 install problems"},{"location":"roles/cp4d_db2wh/#installing-db2wh-on-nfs-based-volumes","text":"DB2 requires you to set no_root_squash when you use NFS with IBM\u00ae Cloud File Storage (ibmc-file-gold-gid storage class) or the restore morph job fails. This role will install a daemonset which applies the no_root_squash NFS mount option on NFS volumes. For more details see the following references: - https://www.ibm.com/docs/en/db2/11.5?topic=requirements-cloud-file-storage - https://www.ibm.com/docs/en/db2/11.5?topic=cds-nfs-storage-requirements-1 - https://cloud.ibm.com/docs/FileStorage?topic=FileStorage-mountingLinux&interface=ui#norootsquash","title":"Installing DB2WH on NFS based volumes"},{"location":"roles/cp4d_db2wh/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_db2wh/#db2wh_namespace","text":"Only supported when using CloudPak for Data v4.0, otherwise unused. For v3.5 support the value is always set to cpd-meta-ops . Environment Variable: DB2WH_NAMESPACE Default Value: cpd-services","title":"db2wh_namespace"},{"location":"roles/cp4d_db2wh/#db2wh_instance_name","text":"Required. Name of the database instance, note that this is the instance name , which is different from the instance ID . Environment Variable: DB2WH_INSTANCE_NAME Default: None","title":"db2wh_instance_name"},{"location":"roles/cp4d_db2wh/#db2wh_version","text":"Version of the DB2 Warehouse instance to be created. Environment Variable: DB2WH_VERSION Default: 11.5.5.1-cn3-x86_64 (CloudPak for Data v3.5), 11.5.6.0-cn3 (CloudPak for Data v4)","title":"db2wh_version"},{"location":"roles/cp4d_db2wh/#db2wh_table_org","text":"The way database tables will be organized. It can be set to either ROW or COLUMN . Environment Variable: DB2WH_TABLE_ORG Default: ROW","title":"db2wh_table_org"},{"location":"roles/cp4d_db2wh/#db2wh_meta_storage_class","text":"Required for both CP4D v3.5 and CP4D v4. Storage class used for metadata. Environment Variable: DB2WH_META_STORAGE_CLASS Default: None","title":"db2wh_meta_storage_class"},{"location":"roles/cp4d_db2wh/#db2wh_meta_storage_size_gb","text":"Size of the metadata persistent volume, in gigabytes Environment Variable: DB2WH_META_STORAGE_SIZE_GB Default: 20","title":"db2wh_meta_storage_size_gb"},{"location":"roles/cp4d_db2wh/#db2wh_user_storage_class","text":"Required for both CP4D v3.5 and CP4D v4. Storage class used for user data. Environment Variable: DB2WH_USER_STORAGE_CLASS Default: None","title":"db2wh_user_storage_class"},{"location":"roles/cp4d_db2wh/#db2wh_user_storage_size_gb","text":"Size of user persistent volume, in gigabytes. Environment Variable: DB2WH_USER_STORAGE_SIZE_GB Default: 100","title":"db2wh_user_storage_size_gb"},{"location":"roles/cp4d_db2wh/#db2wh_backup_storage_class","text":"Required for both CP4D v3.5 and CP4D v4. Storage class used for backup. Environment Variable: DB2WH_BACKUP_STORAGE_CLASS Default: None","title":"db2wh_backup_storage_class"},{"location":"roles/cp4d_db2wh/#db2wh_backup_storage_size_gb","text":"Size of backup persistent volume, in gigabytes. Environment Variable: DB2WH_BACKUP_STORAGE_SIZE_GB Default: 100","title":"db2wh_backup_storage_size_gb"},{"location":"roles/cp4d_db2wh/#db2wh_logs_storage_class","text":"Required for CP4D v4 only. Storage class used for logs, not used with CP4D v3.5 databases. Environment Variable: DB2WH_LOGS_STORAGE_CLASS Default: None","title":"db2wh_logs_storage_class"},{"location":"roles/cp4d_db2wh/#db2wh_logs_storage_size_gb","text":"Size (in gigabytes) of logs persistent volume, not used with CP4D v3.5 databases. Environment Variable: DB2WH_LOGS_STORAGE_SIZE_GB Default: 100","title":"db2wh_logs_storage_size_gb"},{"location":"roles/cp4d_db2wh/#db2wh_temp_storage_class","text":"Required for CP4D v4 only. Storage class used for temporary data, not used with CP4D v3.5 databases. Environment Variable: DB2WH_TEMP_STORAGE_CLASS Default: None","title":"db2wh_temp_storage_class"},{"location":"roles/cp4d_db2wh/#db2wh_temp_storage_size_gb","text":"Size (in gigabytes) of temporary persistent volume, not used with CP4D v3.5 databases. Environment Variable: DB2WH_TEMP_STORAGE_SIZE_GB Default: 100","title":"db2wh_temp_storage_size_gb"},{"location":"roles/cp4d_db2wh/#db2wh_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_CPU_REQUESTS Default: 2000m","title":"db2wh_cpu_requests"},{"location":"roles/cp4d_db2wh/#db2wh_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_CPU_LIMITS Default: 4000m","title":"db2wh_cpu_limits"},{"location":"roles/cp4d_db2wh/#db2wh_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_MEMORY_REQUESTS Default: 6Gi","title":"db2wh_memory_requests"},{"location":"roles/cp4d_db2wh/#db2wh_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Only supported with CloudPak for Data v4. Environment Variable: DB2WH_MEMORY_LIMITS Default: 12Gi","title":"db2wh_memory_limits"},{"location":"roles/cp4d_db2wh/#cpd_entitlement_key","text":"Required. This is the entitlement key used to install the norootsquash Daemonset in the kube-system namespace. Holds your IBM Entitlement key. Environment Variable: CPD_ENTITLEMENT_KEY Default: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d_db2wh/#cpd_api_username","text":"Required for CP4D v3.5 only. These credentials are used to call the REST API to create the database because CP4D v3.5 Kubernetes API is broken. Yes, the default admin account for CP4D v3.5 really is set up as admin/password. Environment Variable: CPD_API_USERNAME Default: admin","title":"cpd_api_username"},{"location":"roles/cp4d_db2wh/#cpd_api_password","text":"Required for CP4D v3.5 only. These credentials are used to call the REST API to create the database because CP4D v3.5 Kubernetes API is broken. Yes, the default admin account for CP4D v3.5 really is set up as admin/password. Environment Variable: CPD_API_PASSWORD Default: password","title":"cpd_api_password"},{"location":"roles/cp4d_db2wh/#mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None","title":"mas_instance_id"},{"location":"roles/cp4d_db2wh/#mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_CONFIG_DIR Default: None","title":"mas_config_dir"},{"location":"roles/cp4d_db2wh/#mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"mas_config_scope"},{"location":"roles/cp4d_db2wh/#mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"mas_workspace_id"},{"location":"roles/cp4d_db2wh/#mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Environment Variable: 'MAS_APP_ID Default: None","title":"mas_application_id"},{"location":"roles/cp4d_db2wh/#db2wh_workload","text":"The workload profile of the db2wh instance, possible values are 'PUREDATA_OLAP' or 'ANALYTICS' Environment Variable: 'DB2WH_WORKLOAD Default: 'ANALYTICS'","title":"db2wh_workload"},{"location":"roles/cp4d_db2wh/#db2wh_node_label","text":"The label used to specify node affinity and tolerations in the db2ucluster CR. Environment Variable: 'DB2WH_NODE_LABEL Default: None","title":"db2wh_node_label"},{"location":"roles/cp4d_db2wh/#db2wh_dedicated_node","text":"The name of the worker node to apply the {{ db2wh_node_label }} taint and label to. Environment Variable: 'DB2WH_DEDICATED_NODE Default: None","title":"db2wh_dedicated_node"},{"location":"roles/cp4d_db2wh/#db2wh_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Environment Variable: 'DB2WH_MLN_COUNT Default: 1","title":"db2wh_mln_count"},{"location":"roles/cp4d_db2wh/#db2wh_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2wh_num_pods must be less than or equal to db2wh_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2wh_mln_count while specifying a small number for db2wh_num_pods . If in doubt, make db2wh_mln_count = db2wh_num_pods . For more information refer to the Db2 documentation . Environment Variable: 'DB2WH_NUM_PODS Default: 1","title":"db2wh_num_pods"},{"location":"roles/cp4d_db2wh/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Create a Db2 instance in CPD v4.0 db2wh_instance_name: db2wh-shared # Configure storage suitable for IBM Cloud ROKS db2wh_meta_storage_class: ibmc-file-silver-gid db2wh_user_storage_class: ibmc-file-gold-gid db2wh_backup_storage_class: ibmc-file-gold-gid db2wh_logs_storage_class: ibmc-file-silver-gid db2wh_temp_storage_class: ibmc-file-silver-gid # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.cp4d_db2wh","title":"Example Playbook"},{"location":"roles/cp4d_db2wh/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_db2wh_backup/","text":"cp4d_db2wh_backup \uf0c1 This role runs a backup procedure from a CloudPak for Data DB2 Warehouse instance and stores the backup files in a targetted local folder. At the end of the backup process, you will find the required files to run a successful restore process in the chosen DB2WH_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Role Variables \uf0c1 db2wh_backup_dir \uf0c1 Required. Local directory that will store the backup files taken from the DB2 Warehouse instance i.e /Users/Documents/db_backup . Environment Variable: DB2WH_BACKUP_DIR Default: None db2wh_instance_name \uf0c1 Required. DB2 Warehouse source instance to take the backup from i.e db2wh-iot . Environment Variable: DB2WH_INSTANCE_NAME_SOURCE Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: db2wh_backup_folder: \"{{ lookup('env', 'DB2WH_BACKUP_DIR') }}\" db2wh_instance_name: \"{{ lookup('env', 'DB2WH_INSTANCE_NAME_SOURCE') }}\" roles: - ibm.mas_devops.cp4d_db2wh_backup License \uf0c1 EPL-2.0 Note: Support for DB2 Warehouse instances running on CP4D v3.5 \uf0c1 Smart detection of CPD namespace is in place for this role, which means it will use default namespaces accordingly to the CPD version identified. If running this role against a DB2 Warehouse instance in CPD v3.5 version, it will expect you to have a config map named \"mas-automation-config-{{ db2wh_instance_name }}\" in the same namespace as your CP4D is installed, in order for db2wh_instance_id property to be correctly defined, such as below: kind: ConfigMap apiVersion: v1 metadata: name: mas-automation-config-db2wh-iot namespace: cpd-meta-ops data: db2wh_instance_id: '1618938039379016' This config-map is automatically generated if you used ibm/mas_devops/roles/cp4d_db2wh role to create your DB2 Warehouse instance. However, if running this role as a standalone playbook and such config map is not found, the backup/restore process will fail. For DB2 Warehouse instance in CPD v4.0 version, no action regarding config map setup is needed, as the required db2wh_instance_name property is enough, therefore the role will be executed properly without further interventions.","title":"cp4d_db2wh_backup"},{"location":"roles/cp4d_db2wh_backup/#cp4d_db2wh_backup","text":"This role runs a backup procedure from a CloudPak for Data DB2 Warehouse instance and stores the backup files in a targetted local folder. At the end of the backup process, you will find the required files to run a successful restore process in the chosen DB2WH_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb)","title":"cp4d_db2wh_backup"},{"location":"roles/cp4d_db2wh_backup/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_db2wh_backup/#db2wh_backup_dir","text":"Required. Local directory that will store the backup files taken from the DB2 Warehouse instance i.e /Users/Documents/db_backup . Environment Variable: DB2WH_BACKUP_DIR Default: None","title":"db2wh_backup_dir"},{"location":"roles/cp4d_db2wh_backup/#db2wh_instance_name","text":"Required. DB2 Warehouse source instance to take the backup from i.e db2wh-iot . Environment Variable: DB2WH_INSTANCE_NAME_SOURCE Default: None","title":"db2wh_instance_name"},{"location":"roles/cp4d_db2wh_backup/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2wh_backup_folder: \"{{ lookup('env', 'DB2WH_BACKUP_DIR') }}\" db2wh_instance_name: \"{{ lookup('env', 'DB2WH_INSTANCE_NAME_SOURCE') }}\" roles: - ibm.mas_devops.cp4d_db2wh_backup","title":"Example Playbook"},{"location":"roles/cp4d_db2wh_backup/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_db2wh_backup/#note-support-for-db2-warehouse-instances-running-on-cp4d-v35","text":"Smart detection of CPD namespace is in place for this role, which means it will use default namespaces accordingly to the CPD version identified. If running this role against a DB2 Warehouse instance in CPD v3.5 version, it will expect you to have a config map named \"mas-automation-config-{{ db2wh_instance_name }}\" in the same namespace as your CP4D is installed, in order for db2wh_instance_id property to be correctly defined, such as below: kind: ConfigMap apiVersion: v1 metadata: name: mas-automation-config-db2wh-iot namespace: cpd-meta-ops data: db2wh_instance_id: '1618938039379016' This config-map is automatically generated if you used ibm/mas_devops/roles/cp4d_db2wh role to create your DB2 Warehouse instance. However, if running this role as a standalone playbook and such config map is not found, the backup/restore process will fail. For DB2 Warehouse instance in CPD v4.0 version, no action regarding config map setup is needed, as the required db2wh_instance_name property is enough, therefore the role will be executed properly without further interventions.","title":"Note: Support for DB2 Warehouse instances running on CP4D v3.5"},{"location":"roles/cp4d_db2wh_manage_hack/","text":"cp4d_db2wh_manage_hack \uf0c1 This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself. Role Variables \uf0c1 cpd_version \uf0c1 Users may optionally pass this parameter to explicitly control the version of CP4D used, if this is not done then the role will attempt to locate the cpd-meta-ops namespace associated with CP4D v3.5, if this namespace if located then we will switch to CP4D v3.5 mode, in all other cases the role will assume CP4D v4 is in use. Environment Variable: CPD_VERSION Default Value: None db2wh_instancename \uf0c1 Required. The name of the db2 instance to execute the hack in. We will use this to derive the instance ID. Note that for CP4D v3.5 instances we are only able to support those created by the cp4d_db2wh role; custom metadata is created by that role which is required by this role to dervice the instance ID from a named database instance. Environment Variable: DB2WH_INSTANCE_NAME Default Value: None db2wh_username \uf0c1 The username that will be used to connect to the database specified by db2wh_dbname . Environment Variable: None Default Value: db2inst1 db2wh_dbname \uf0c1 The name of the database in the instance to connect to when executing the hack script. Environment Variable: None Default Value: BLUDB db2wh_schema \uf0c1 The name of the Manage schema where the hack should be targeted in. Environment Variable: None Default Value: maximo Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: db2wh_instancename: mydb2 roles: - ibm.mas_devops.cp4d_db2wh_manage_hack License \uf0c1 EPL-2.0","title":"cp4d_db2wh_manage_hack"},{"location":"roles/cp4d_db2wh_manage_hack/#cp4d_db2wh_manage_hack","text":"This role shouldn't need to exist, it should be part of the Manage operator, but is not so we have to do it as a seperate step in the install flow for now. The role will perform some initial setup on the Db2 instance that is needed to prepare it for use with the Manage application and supports both CP4D version 3.5 and 4.0. The role will copy a bash script (setupdb.sh) into the Db2 pod and execute it inside the container, this script will perform a number of configuration changes to the database as well as configuring the tablespaces for Maximo Manage because the operator is not yet able to do this itself.","title":"cp4d_db2wh_manage_hack"},{"location":"roles/cp4d_db2wh_manage_hack/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_db2wh_manage_hack/#cpd_version","text":"Users may optionally pass this parameter to explicitly control the version of CP4D used, if this is not done then the role will attempt to locate the cpd-meta-ops namespace associated with CP4D v3.5, if this namespace if located then we will switch to CP4D v3.5 mode, in all other cases the role will assume CP4D v4 is in use. Environment Variable: CPD_VERSION Default Value: None","title":"cpd_version"},{"location":"roles/cp4d_db2wh_manage_hack/#db2wh_instancename","text":"Required. The name of the db2 instance to execute the hack in. We will use this to derive the instance ID. Note that for CP4D v3.5 instances we are only able to support those created by the cp4d_db2wh role; custom metadata is created by that role which is required by this role to dervice the instance ID from a named database instance. Environment Variable: DB2WH_INSTANCE_NAME Default Value: None","title":"db2wh_instancename"},{"location":"roles/cp4d_db2wh_manage_hack/#db2wh_username","text":"The username that will be used to connect to the database specified by db2wh_dbname . Environment Variable: None Default Value: db2inst1","title":"db2wh_username"},{"location":"roles/cp4d_db2wh_manage_hack/#db2wh_dbname","text":"The name of the database in the instance to connect to when executing the hack script. Environment Variable: None Default Value: BLUDB","title":"db2wh_dbname"},{"location":"roles/cp4d_db2wh_manage_hack/#db2wh_schema","text":"The name of the Manage schema where the hack should be targeted in. Environment Variable: None Default Value: maximo","title":"db2wh_schema"},{"location":"roles/cp4d_db2wh_manage_hack/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2wh_instancename: mydb2 roles: - ibm.mas_devops.cp4d_db2wh_manage_hack","title":"Example Playbook"},{"location":"roles/cp4d_db2wh_manage_hack/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_db2wh_restore/","text":"cp4d_db2wh_restore \uf0c1 This role runs a restore procedure onto a CloudPak for Data DB2 Warehouse instance. In order to begin the restore process, you must have the required files to run a successful restore process in the chosen DB2WH_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Note: These files are generated automatically if you run ibm.mas_devops.cp4d_db2wh_backup role. If any of the above files are not found in the DB2WH_BACKUP_DIR , the restore process will fail. Role Variables \uf0c1 db2wh_backup_dir \uf0c1 Required. Local directory that stores the backup files to be used in the DB2 Warehouse restore process, i.e /Users/Documents/db_backup . Environment Variable: DB2WH_BACKUP_DIR Default: None db2wh_instance_name \uf0c1 Required. DB2 Warehouse target instance to restore the backup to, i.e db2wh-iot . Environment Variable: DB2WH_INSTANCE_NAME_TARGET Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: db2wh_backup_folder: \"{{ lookup('env', 'DB2WH_BACKUP_DIR') }}\" db2wh_instance_name: \"{{ lookup('env', 'DB2WH_INSTANCE_NAME_TARGET') }}\" roles: - ibm.mas_devops.cp4d_db2wh_restore License \uf0c1 EPL-2.0 Note: Support for DB2 Warehouse instances running on CP4D v3.5 \uf0c1 Smart detection of CPD namespace is in place for this role, which means it will use default namespaces accordingly to the CPD version identified. If running this role against a DB2 Warehouse instance in CPD v3.5 version, it will expect you to have a config map named \"mas-automation-config-{{ db2wh_instance_name }}\" in the same namespace as your CP4D is installed, in order for db2wh_instance_id property to be correctly defined, such as below: kind: ConfigMap apiVersion: v1 metadata: name: mas-automation-config-db2wh-iot namespace: cpd-meta-ops data: db2wh_instance_id: '1618938039379016' This config-map is automatically generated if you used ibm/mas_devops/roles/cp4d_db2wh role to create your DB2 Warehouse instance. However, if running this role as a standalone playbook and such config map is not found, the backup/restore process will fail. For DB2 Warehouse instance in CPD v4.0 version, no action regarding config map setup is needed, as the required db2wh_instance_name property is enough, therefore the role will be executed properly without further interventions.","title":"cp4d_db2wh_restore"},{"location":"roles/cp4d_db2wh_restore/#cp4d_db2wh_restore","text":"This role runs a restore procedure onto a CloudPak for Data DB2 Warehouse instance. In order to begin the restore process, you must have the required files to run a successful restore process in the chosen DB2WH_BACKUP_DIR : DB2 backup files i.e BLUDB.0.db2inst1.DBPART000.202XXXXXXXXXXX.001 DB2 keystore files (.p12 and .sth) DB2 instance master key label file (.kdb) Note: These files are generated automatically if you run ibm.mas_devops.cp4d_db2wh_backup role. If any of the above files are not found in the DB2WH_BACKUP_DIR , the restore process will fail.","title":"cp4d_db2wh_restore"},{"location":"roles/cp4d_db2wh_restore/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_db2wh_restore/#db2wh_backup_dir","text":"Required. Local directory that stores the backup files to be used in the DB2 Warehouse restore process, i.e /Users/Documents/db_backup . Environment Variable: DB2WH_BACKUP_DIR Default: None","title":"db2wh_backup_dir"},{"location":"roles/cp4d_db2wh_restore/#db2wh_instance_name","text":"Required. DB2 Warehouse target instance to restore the backup to, i.e db2wh-iot . Environment Variable: DB2WH_INSTANCE_NAME_TARGET Default: None","title":"db2wh_instance_name"},{"location":"roles/cp4d_db2wh_restore/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: db2wh_backup_folder: \"{{ lookup('env', 'DB2WH_BACKUP_DIR') }}\" db2wh_instance_name: \"{{ lookup('env', 'DB2WH_INSTANCE_NAME_TARGET') }}\" roles: - ibm.mas_devops.cp4d_db2wh_restore","title":"Example Playbook"},{"location":"roles/cp4d_db2wh_restore/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_db2wh_restore/#note-support-for-db2-warehouse-instances-running-on-cp4d-v35","text":"Smart detection of CPD namespace is in place for this role, which means it will use default namespaces accordingly to the CPD version identified. If running this role against a DB2 Warehouse instance in CPD v3.5 version, it will expect you to have a config map named \"mas-automation-config-{{ db2wh_instance_name }}\" in the same namespace as your CP4D is installed, in order for db2wh_instance_id property to be correctly defined, such as below: kind: ConfigMap apiVersion: v1 metadata: name: mas-automation-config-db2wh-iot namespace: cpd-meta-ops data: db2wh_instance_id: '1618938039379016' This config-map is automatically generated if you used ibm/mas_devops/roles/cp4d_db2wh role to create your DB2 Warehouse instance. However, if running this role as a standalone playbook and such config map is not found, the backup/restore process will fail. For DB2 Warehouse instance in CPD v4.0 version, no action regarding config map setup is needed, as the required db2wh_instance_name property is enough, therefore the role will be executed properly without further interventions.","title":"Note: Support for DB2 Warehouse instances running on CP4D v3.5"},{"location":"roles/cp4d_hack_worker_nodes/","text":"cp4d_hack_worker_nodes \uf0c1 This is a horrible hack, but it's what CP4D tells us to do :( This should be executed as part of cluster prepararion if you want to use CP4D v4. It will reboot all worker nodes, causing disruption to the entire cluster and everything running on it we do not include this as part of the normal flow because, well it shouldn't be necessary to reboot worker nodes to install containerized software. Hopefully this is just an example of poor documentation and there's a simple alternative that we can implement to remove this role. For more information, refer to https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret Role Variables \uf0c1 cluster_type \uf0c1 Required. Note that only supported value at present is roks . Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. The name of the ROKS cluster that we are going to apply the CP4D hack to. Environment Variable: CLUSTER_NAME Default Value: None ibmcloud_apikey \uf0c1 Required. Provide your IBM Cloud API Key, this will be used to query the status of the cluster and issue the node restart commands. Environment Variable: IBMCLOUD_APIKEY Default Value: None cpd_entitlement_key \uf0c1 Required. Provide your IBM Entitlement Key. Environment Variable: CPD_ENTITLEMENT_KEY Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" cpd_entitlement_key: \"{{ lookup('env', 'CPD_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.cp4d_hack_worker_nodes License \uf0c1 EPL-2.0","title":"cp4d_hack_worker_nodes"},{"location":"roles/cp4d_hack_worker_nodes/#cp4d_hack_worker_nodes","text":"This is a horrible hack, but it's what CP4D tells us to do :( This should be executed as part of cluster prepararion if you want to use CP4D v4. It will reboot all worker nodes, causing disruption to the entire cluster and everything running on it we do not include this as part of the normal flow because, well it shouldn't be necessary to reboot worker nodes to install containerized software. Hopefully this is just an example of poor documentation and there's a simple alternative that we can implement to remove this role. For more information, refer to https://cloud.ibm.com/docs/openshift?topic=openshift-registry#cluster_global_pull_secret","title":"cp4d_hack_worker_nodes"},{"location":"roles/cp4d_hack_worker_nodes/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_hack_worker_nodes/#cluster_type","text":"Required. Note that only supported value at present is roks . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/cp4d_hack_worker_nodes/#cluster_name","text":"Required. The name of the ROKS cluster that we are going to apply the CP4D hack to. Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/cp4d_hack_worker_nodes/#ibmcloud_apikey","text":"Required. Provide your IBM Cloud API Key, this will be used to query the status of the cluster and issue the node restart commands. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/cp4d_hack_worker_nodes/#cpd_entitlement_key","text":"Required. Provide your IBM Entitlement Key. Environment Variable: CPD_ENTITLEMENT_KEY Default Value: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d_hack_worker_nodes/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cluster_name: mycluster cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" cpd_entitlement_key: \"{{ lookup('env', 'CPD_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.cp4d_hack_worker_nodes","title":"Example Playbook"},{"location":"roles/cp4d_hack_worker_nodes/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_install/","text":"cp4d_install \uf0c1 This role installs IBM Cloud Pak for Data Operator in the target cluster. Support is available for both CP4D v3.5 (installed into the cpd-meta-ops namespace) and CP4D v4.0 (installed into the ibm-common-services namespace). If you are installing CP4D v4 then the cp4d_hack_worker_nodes role must have been executed during cluster set up to update the cluster's global image pull secret and reload all worker nodes. The role assumes that you have already installed the IBM Operator Catalog in the target cluster. This action is performed by the ocp_setup_mas_deps role if you want to use this collection to install the CatalogSource. Role Variables \uf0c1 cpd_version \uf0c1 Required. CP4D version to be installed. Supported versions are cpd35 (for CP4D 3.5) and cpd40 (for CP4D 4.0) Environment Variable: CPD_VERSION Default: None cpd_entitlement_key \uf0c1 Required only if cpd_version = cpd35 , otherwise unused because in CP4D v4 we have to use the cp4d_hack_worker_nodes role to prepare the cluster ahead of time by setting up a global image pull secret for CP4D. Holds your IBM Entitlement key. Environment Variable: CPD_ENTITLEMENT_KEY Default: None cpd_storage_class \uf0c1 Required only if cpd_version = cpd40 , otherwise unused. Environment Variable: CPD_STORAGE_CLASS Default Value: None Note: As per CloudPak For Data support team's recommendation, the value set for cpd_storage_class will also be used for file and metastore storages while installing CPD v4.0 Control Plane. Source: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=requirements-storage cpd_services_namespace \uf0c1 Only supported if cpd_version = cpd40 , otherwise unused. For v3.5 support this value is always set to cpd-meta-ops . Environment Variable: CPD_SERVICES_NAMESPACE Default Value: cpd-services mas_channel \uf0c1 You can specify this property as an alternative to cpd_version to allow the role to automatically select the appropriate version of CP4D based on the MAS Channel you are subscribing to. If cpd_version is set, then this is ignored. Environment Variable: MAS_CHANNEL Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: cpd_version: cpd40 cpd_storage_class: ibmc-file-gold-gid roles: - ibm.mas_devops.cp4d_install License \uf0c1 EPL-2.0","title":"cp4d_install"},{"location":"roles/cp4d_install/#cp4d_install","text":"This role installs IBM Cloud Pak for Data Operator in the target cluster. Support is available for both CP4D v3.5 (installed into the cpd-meta-ops namespace) and CP4D v4.0 (installed into the ibm-common-services namespace). If you are installing CP4D v4 then the cp4d_hack_worker_nodes role must have been executed during cluster set up to update the cluster's global image pull secret and reload all worker nodes. The role assumes that you have already installed the IBM Operator Catalog in the target cluster. This action is performed by the ocp_setup_mas_deps role if you want to use this collection to install the CatalogSource.","title":"cp4d_install"},{"location":"roles/cp4d_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_install/#cpd_version","text":"Required. CP4D version to be installed. Supported versions are cpd35 (for CP4D 3.5) and cpd40 (for CP4D 4.0) Environment Variable: CPD_VERSION Default: None","title":"cpd_version"},{"location":"roles/cp4d_install/#cpd_entitlement_key","text":"Required only if cpd_version = cpd35 , otherwise unused because in CP4D v4 we have to use the cp4d_hack_worker_nodes role to prepare the cluster ahead of time by setting up a global image pull secret for CP4D. Holds your IBM Entitlement key. Environment Variable: CPD_ENTITLEMENT_KEY Default: None","title":"cpd_entitlement_key"},{"location":"roles/cp4d_install/#cpd_storage_class","text":"Required only if cpd_version = cpd40 , otherwise unused. Environment Variable: CPD_STORAGE_CLASS Default Value: None Note: As per CloudPak For Data support team's recommendation, the value set for cpd_storage_class will also be used for file and metastore storages while installing CPD v4.0 Control Plane. Source: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=requirements-storage","title":"cpd_storage_class"},{"location":"roles/cp4d_install/#cpd_services_namespace","text":"Only supported if cpd_version = cpd40 , otherwise unused. For v3.5 support this value is always set to cpd-meta-ops . Environment Variable: CPD_SERVICES_NAMESPACE Default Value: cpd-services","title":"cpd_services_namespace"},{"location":"roles/cp4d_install/#mas_channel","text":"You can specify this property as an alternative to cpd_version to allow the role to automatically select the appropriate version of CP4D based on the MAS Channel you are subscribing to. If cpd_version is set, then this is ignored. Environment Variable: MAS_CHANNEL Default: None","title":"mas_channel"},{"location":"roles/cp4d_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: cpd_version: cpd40 cpd_storage_class: ibmc-file-gold-gid roles: - ibm.mas_devops.cp4d_install","title":"Example Playbook"},{"location":"roles/cp4d_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/cp4d_install_services/","text":"cp4d_install_services \uf0c1 Install one or more CloudPak for Data services. This role supports both CP4D v3.5 and v4.0. With CP4D v3.5 all services will be installed to the cpd-meta-ops namespace. With CP4D v4.0 all services will be installed to the cpd-services namespace. Role Variables \uf0c1 cpd_version \uf0c1 Users may optionally pass this parameter to explicitly control the version of CP4D used, if this is not done then the role will attempt to locate the cpd-meta-ops namespace associated with CP4D v3.5, if this namespace if located then we will switch to CP4D v3.5 mode, in all other cases the role will assume CP4D v4 is in use. Environment Variable: CPD_VERSION Default Value: None cpd_storage_class \uf0c1 Required. This is used to set spec.storageClass in all CPD v3.5 services, and many - but not all - CP4D v4.0 services. Environment Variable: CPD_STORAGE_CLASS Default Value: None cpd_wd_storage_class \uf0c1 Required only if installing Watson Discovery service ( wd ) on CP4D v4.0. Environment Variable: CPD_WD_STORAGE_CLASS Default Value: None cpd_services \uf0c1 Required. Provide a list of Cloud Pak for Data services to enable. Environment Variable: None Default Value: None cpd_wsl_project_id \uf0c1 Optional - Stores the CP4D Watson Studio Project ID that can be used to configure HP Utilities application in MAS. If this property is not set, and Watson Studio is installed as part of CP4D, this role will automatically create one Watson Studio project that could be used to configure HP Utilities application in MAS instance ( mas_instance_id and mas_config_dir properties must also be set in order for Watson Studio project to be created as part of this role.) Environment Variable: CPD_WSL_PROJECT_ID Default Value: None. cpd_wsl_project_name \uf0c1 Optional - Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl_default_project cpd_wsl_project_description \uf0c1 Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio project to be used by HP Utilities app in MAS mas_instance_id \uf0c1 If Watson Studio is installed as part of CP4D: The instance ID of Maximo Application Suite that the WatsonStudioCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a WatsonStudioCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 If Watson Studio is installed as part of CP4D: Local directory to save the generated WatsonStudioCfg resource definition. This can be used to manually configure a MAS instance to connect to the Watson Studio, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a WatsonStudioCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: cpd_storage_class: ibmc-file-gold-gid cpd_wd_storage_class: ibmc-block-gold # Install the Db2 Warehouse & WSL services cpd_services: - db2wh - wsl roles: - ibm.mas_devops.cp4d_install_services License \uf0c1 EPL-2.0","title":"cp4d_install_services"},{"location":"roles/cp4d_install_services/#cp4d_install_services","text":"Install one or more CloudPak for Data services. This role supports both CP4D v3.5 and v4.0. With CP4D v3.5 all services will be installed to the cpd-meta-ops namespace. With CP4D v4.0 all services will be installed to the cpd-services namespace.","title":"cp4d_install_services"},{"location":"roles/cp4d_install_services/#role-variables","text":"","title":"Role Variables"},{"location":"roles/cp4d_install_services/#cpd_version","text":"Users may optionally pass this parameter to explicitly control the version of CP4D used, if this is not done then the role will attempt to locate the cpd-meta-ops namespace associated with CP4D v3.5, if this namespace if located then we will switch to CP4D v3.5 mode, in all other cases the role will assume CP4D v4 is in use. Environment Variable: CPD_VERSION Default Value: None","title":"cpd_version"},{"location":"roles/cp4d_install_services/#cpd_storage_class","text":"Required. This is used to set spec.storageClass in all CPD v3.5 services, and many - but not all - CP4D v4.0 services. Environment Variable: CPD_STORAGE_CLASS Default Value: None","title":"cpd_storage_class"},{"location":"roles/cp4d_install_services/#cpd_wd_storage_class","text":"Required only if installing Watson Discovery service ( wd ) on CP4D v4.0. Environment Variable: CPD_WD_STORAGE_CLASS Default Value: None","title":"cpd_wd_storage_class"},{"location":"roles/cp4d_install_services/#cpd_services","text":"Required. Provide a list of Cloud Pak for Data services to enable. Environment Variable: None Default Value: None","title":"cpd_services"},{"location":"roles/cp4d_install_services/#cpd_wsl_project_id","text":"Optional - Stores the CP4D Watson Studio Project ID that can be used to configure HP Utilities application in MAS. If this property is not set, and Watson Studio is installed as part of CP4D, this role will automatically create one Watson Studio project that could be used to configure HP Utilities application in MAS instance ( mas_instance_id and mas_config_dir properties must also be set in order for Watson Studio project to be created as part of this role.) Environment Variable: CPD_WSL_PROJECT_ID Default Value: None.","title":"cpd_wsl_project_id"},{"location":"roles/cp4d_install_services/#cpd_wsl_project_name","text":"Optional - Stores the CP4D Watson Studio Project name that can be used to configure HP Utilities application in MAS. Environment Variable: CPD_WSL_PROJECT_NAME Default Value: wsl_default_project","title":"cpd_wsl_project_name"},{"location":"roles/cp4d_install_services/#cpd_wsl_project_description","text":"Optional - Stores the CP4D Watson Studio Project description that can be used to configure HP Utilities application in MAS. Environment Variable: CPD_WSL_PROJECT_DESCRIPTION Default Value: Watson Studio project to be used by HP Utilities app in MAS","title":"cpd_wsl_project_description"},{"location":"roles/cp4d_install_services/#mas_instance_id","text":"If Watson Studio is installed as part of CP4D: The instance ID of Maximo Application Suite that the WatsonStudioCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a WatsonStudioCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/cp4d_install_services/#mas_config_dir","text":"If Watson Studio is installed as part of CP4D: Local directory to save the generated WatsonStudioCfg resource definition. This can be used to manually configure a MAS instance to connect to the Watson Studio, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a WatsonStudioCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/cp4d_install_services/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: cpd_storage_class: ibmc-file-gold-gid cpd_wd_storage_class: ibmc-block-gold # Install the Db2 Warehouse & WSL services cpd_services: - db2wh - wsl roles: - ibm.mas_devops.cp4d_install_services","title":"Example Playbook"},{"location":"roles/cp4d_install_services/#license","text":"EPL-2.0","title":"License"},{"location":"roles/db2u/","text":"db2u \uf0c1 This role creates a Db2 Warehouse instance using the Db2u Operator. A namespace called db2u will be created and the db2u operator will be installed into the ibm-common-services namespace to service the db2ucluster requests in db2u namespace. A self-signed certificate is created and a Db2 Warehouse cluster will be created along with a public TLS encrypted route is configured to allow external access to the cluster (access is via port 443 on the route). The certificates are available from the db2u-ca and db2u-certificate secrets in the db2u namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2u namespace: oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2u namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2u_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the db2u_node_label and db2u_dedicated_node variables are defined then role will taint and drain the dedicated node before labeling it using database={{ db2u_node_label }} . The node is then uncordoned. Role Variables \uf0c1 [required] db2u_instance_name \uf0c1 Required. Name of the database instance, note that this is the instance name , which is different from the instance ID . Environment Variable: DB2U_INSTANCE_NAME Default: None [optional] db2u_dbname \uf0c1 Name of the database within the instance. Environment Variable: DB2U_DBNAME Default: BLUDB [optional] db2u_version \uf0c1 Version of the DB2U operator instance to be created. Environment Variable: DB2U_VERSION Default: 11.5.7.0-cn2 [optional] db2u_4k_device_support \uf0c1 Whether 4K device support is turned on or not. Environment Variable: DB2U_4K_DEVICE_SUPPORT Default: ON [optional] db2u_table_org \uf0c1 The way database tables will be organized. It can be set to either ROW or COLUMN . Environment Variable: DB2U_TABLE_ORG Default: ROW [required] db2u_meta_storage_class \uf0c1 Storage class used for metadata. This must support ReadWriteMany Environment Variable: DB2U_META_STORAGE_CLASS Default: None [optional] db2u_meta_storage_size_gb \uf0c1 Size of the metadata persistent volume, in gigabytes Environment Variable: DB2U_META_STORAGE_SIZE_GB Default: 20 [required] db2u_data_storage_class \uf0c1 Storage class used for user data. This must support ReadWriteOnce Environment Variable: DB2U_USER_STORAGE_CLASS Default: None [optional] db2u_user_storage_size_gb \uf0c1 Size of user persistent volume, in gigabytes. Environment Variable: DB2U_USER_STORAGE_SIZE_GB Default: 100 [optional] db2u_backup_storage_class \uf0c1 Storage class used for backup. This must support ReadWriteMany Environment Variable: DB2U_BACKUP_STORAGE_CLASS Default: None [optional] db2u_backup_storage_size_gb \uf0c1 Size of backup persistent volume, in gigabytes. Environment Variable: DB2U_BACKUP_STORAGE_SIZE_GB Default: 100 [optional] db2u_logs_storage_class \uf0c1 Storage class used for transaction logs. This must support ReadWriteOnce Environment Variable: DB2U_LOGS_STORAGE_CLASS Default: None [optional] db2u_logs_storage_size_gb \uf0c1 Size (in gigabytes) of transaction logs persistent volume. Environment Variable: DB2U_LOGS_STORAGE_SIZE_GB Default: 100 [optional] db2u_temp_storage_class \uf0c1 Storage class used for temporary data. This must support ReadWriteOnce Environment Variable: DB2U_TEMP_STORAGE_CLASS Default: None [optional] db2u_temp_storage_size_gb \uf0c1 Size (in gigabytes) of temporary persistent volume. Environment Variable: DB2U_TEMP_STORAGE_SIZE_GB Default: 100 [optional] db2u_cpu_requests \uf0c1 Define the Kubernetes CPU request for the Db2 pod. Environment Variable: DB2U_CPU_REQUESTS Default: 2000m [optional] db2u_cpu_limits \uf0c1 Define the Kubernetes CPU limit for the Db2 pod. Environment Variable: DB2U_CPU_LIMITS Default: 4000m [optional] db2u_memory_requests \uf0c1 Define the Kubernetes memory request for the Db2 pod. Environment Variable: DB2U_MEMORY_REQUESTS Default: 6Gi [optional] db2u_memory_limits \uf0c1 Define the Kubernetes memory limit for the Db2 pod. Environment Variable: DB2U_MEMORY_LIMITS Default: 12Gi [required] entitlement_key \uf0c1 Required. This is the entitlement key used to install the Db2u Operator and Db2 images. Holds your IBM Entitlement key. Environment Variable: ENTITLEMENT_KEY Default: None [optional] mas_instance_id \uf0c1 Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None [optional] mas_config_dir \uf0c1 Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_CONFIG_DIR Default: None [optional] mas_config_scope \uf0c1 Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Environment Variable: MAS_CONFIG_SCOPE Default: system [optional] mas_workspace_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None [optional] mas_application_id \uf0c1 This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Environment Variable: 'MAS_APP_ID Default: None [optional] db2u_workload \uf0c1 The workload profile of the db2 instance, possible values are 'PUREDATA_OLAP' or 'ANALYTICS' Environment Variable: 'DB2U_WORKLOAD Default: 'ANALYTICS' db2u_node_label \uf0c1 The label used to specify node affinity and tolerations in the db2ucluster CR. Environment Variable: 'DB2U_NODE_LABEL Default: None db2u_dedicated_node \uf0c1 The name of the worker node to apply the {{ db2u_node_label }} taint and label to. Environment Variable: 'DB2U_DEDICATED_NODE Default: None db2u_mln_count \uf0c1 The number of logical nodes (i.e. database partitions to create). Environment Variable: 'DB2U_MLN_COUNT Default: 1 db2u_num_pods \uf0c1 The number of Db2 pods to create in the instance. Note that db2u_num_pods must be less than or equal to db2u_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2u_mln_count while specifying a small number for db2u_num_pods . If in doubt, make db2u_mln_count = db2u_num_pods . For more information refer to the Db2 documentation . Environment Variable: 'DB2U_NUM_PODS Default: 1 Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2u_version: 11.5.7.0-cn2 db2u_instance_name: db2u-db01 db2u_dbname: BLUDB db2u_meta_storage_class: \"ibmc-file-gold\" db2u_data_storage_class: \"ibmc-block-gold\" db2u_backup_storage_class: \"ibmc-file-gold\" db2u_logs_storage_class: \"ibmc-block-gold\" db2u_temp_storage_class: \"ibmc-block-gold\" entitlement_key: \"{{ lookup('env', 'ENTITLEMENT_KEY') }}\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.db2u License \uf0c1 EPL-2.0","title":"db2u"},{"location":"roles/db2u/#db2u","text":"This role creates a Db2 Warehouse instance using the Db2u Operator. A namespace called db2u will be created and the db2u operator will be installed into the ibm-common-services namespace to service the db2ucluster requests in db2u namespace. A self-signed certificate is created and a Db2 Warehouse cluster will be created along with a public TLS encrypted route is configured to allow external access to the cluster (access is via port 443 on the route). The certificates are available from the db2u-ca and db2u-certificate secrets in the db2u namespace. The default user is db2inst1 and the password is available in the instancepassword secret in the same namespace. You can examine the deployed resources in the db2u namespace: oc -n db2u get db2ucluster NAME STATE MAINTENANCESTATE AGE db2u-db01 Ready None 29m It typically takes 20-30 minutes from the db2ucluster being created till it is ready. If the db2ucluster is not ready after that period then check that all the PersistentVolumeClaims in the db2u namespace are ready and that the pods in the namespace are not stuck in init state. If the c-<db2u_instance_name>-db2u-0 pod is running then you can exec into the pod and check the /var/log/db2u.log for any issue. If the db2u_node_label and db2u_dedicated_node variables are defined then role will taint and drain the dedicated node before labeling it using database={{ db2u_node_label }} . The node is then uncordoned.","title":"db2u"},{"location":"roles/db2u/#role-variables","text":"","title":"Role Variables"},{"location":"roles/db2u/#required-db2u_instance_name","text":"Required. Name of the database instance, note that this is the instance name , which is different from the instance ID . Environment Variable: DB2U_INSTANCE_NAME Default: None","title":"[required] db2u_instance_name"},{"location":"roles/db2u/#optional-db2u_dbname","text":"Name of the database within the instance. Environment Variable: DB2U_DBNAME Default: BLUDB","title":"[optional] db2u_dbname"},{"location":"roles/db2u/#optional-db2u_version","text":"Version of the DB2U operator instance to be created. Environment Variable: DB2U_VERSION Default: 11.5.7.0-cn2","title":"[optional] db2u_version"},{"location":"roles/db2u/#optional-db2u_4k_device_support","text":"Whether 4K device support is turned on or not. Environment Variable: DB2U_4K_DEVICE_SUPPORT Default: ON","title":"[optional] db2u_4k_device_support"},{"location":"roles/db2u/#optional-db2u_table_org","text":"The way database tables will be organized. It can be set to either ROW or COLUMN . Environment Variable: DB2U_TABLE_ORG Default: ROW","title":"[optional] db2u_table_org"},{"location":"roles/db2u/#required-db2u_meta_storage_class","text":"Storage class used for metadata. This must support ReadWriteMany Environment Variable: DB2U_META_STORAGE_CLASS Default: None","title":"[required] db2u_meta_storage_class"},{"location":"roles/db2u/#optional-db2u_meta_storage_size_gb","text":"Size of the metadata persistent volume, in gigabytes Environment Variable: DB2U_META_STORAGE_SIZE_GB Default: 20","title":"[optional] db2u_meta_storage_size_gb"},{"location":"roles/db2u/#required-db2u_data_storage_class","text":"Storage class used for user data. This must support ReadWriteOnce Environment Variable: DB2U_USER_STORAGE_CLASS Default: None","title":"[required] db2u_data_storage_class"},{"location":"roles/db2u/#optional-db2u_user_storage_size_gb","text":"Size of user persistent volume, in gigabytes. Environment Variable: DB2U_USER_STORAGE_SIZE_GB Default: 100","title":"[optional] db2u_user_storage_size_gb"},{"location":"roles/db2u/#optional-db2u_backup_storage_class","text":"Storage class used for backup. This must support ReadWriteMany Environment Variable: DB2U_BACKUP_STORAGE_CLASS Default: None","title":"[optional] db2u_backup_storage_class"},{"location":"roles/db2u/#optional-db2u_backup_storage_size_gb","text":"Size of backup persistent volume, in gigabytes. Environment Variable: DB2U_BACKUP_STORAGE_SIZE_GB Default: 100","title":"[optional] db2u_backup_storage_size_gb"},{"location":"roles/db2u/#optional-db2u_logs_storage_class","text":"Storage class used for transaction logs. This must support ReadWriteOnce Environment Variable: DB2U_LOGS_STORAGE_CLASS Default: None","title":"[optional] db2u_logs_storage_class"},{"location":"roles/db2u/#optional-db2u_logs_storage_size_gb","text":"Size (in gigabytes) of transaction logs persistent volume. Environment Variable: DB2U_LOGS_STORAGE_SIZE_GB Default: 100","title":"[optional] db2u_logs_storage_size_gb"},{"location":"roles/db2u/#optional-db2u_temp_storage_class","text":"Storage class used for temporary data. This must support ReadWriteOnce Environment Variable: DB2U_TEMP_STORAGE_CLASS Default: None","title":"[optional] db2u_temp_storage_class"},{"location":"roles/db2u/#optional-db2u_temp_storage_size_gb","text":"Size (in gigabytes) of temporary persistent volume. Environment Variable: DB2U_TEMP_STORAGE_SIZE_GB Default: 100","title":"[optional] db2u_temp_storage_size_gb"},{"location":"roles/db2u/#optional-db2u_cpu_requests","text":"Define the Kubernetes CPU request for the Db2 pod. Environment Variable: DB2U_CPU_REQUESTS Default: 2000m","title":"[optional] db2u_cpu_requests"},{"location":"roles/db2u/#optional-db2u_cpu_limits","text":"Define the Kubernetes CPU limit for the Db2 pod. Environment Variable: DB2U_CPU_LIMITS Default: 4000m","title":"[optional] db2u_cpu_limits"},{"location":"roles/db2u/#optional-db2u_memory_requests","text":"Define the Kubernetes memory request for the Db2 pod. Environment Variable: DB2U_MEMORY_REQUESTS Default: 6Gi","title":"[optional] db2u_memory_requests"},{"location":"roles/db2u/#optional-db2u_memory_limits","text":"Define the Kubernetes memory limit for the Db2 pod. Environment Variable: DB2U_MEMORY_LIMITS Default: 12Gi","title":"[optional] db2u_memory_limits"},{"location":"roles/db2u/#required-entitlement_key","text":"Required. This is the entitlement key used to install the Db2u Operator and Db2 images. Holds your IBM Entitlement key. Environment Variable: ENTITLEMENT_KEY Default: None","title":"[required] entitlement_key"},{"location":"roles/db2u/#optional-mas_instance_id","text":"Providing this and mas_config_dir will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_INSTANCE_ID Default: None","title":"[optional] mas_instance_id"},{"location":"roles/db2u/#optional-mas_config_dir","text":"Providing this and mas_instance_id will instruct the role to generate a JdbcCfg template that can be used to configure MAS to connect to this database. Environment Variable: MAS_CONFIG_DIR Default: None","title":"[optional] mas_config_dir"},{"location":"roles/db2u/#optional-mas_config_scope","text":"Supported values are system , ws , app , or wsapp , this is only used when both mas_config_dir and mas_instance_id are set. Environment Variable: MAS_CONFIG_SCOPE Default: system","title":"[optional] mas_config_scope"},{"location":"roles/db2u/#optional-mas_workspace_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either ws or wsapp Environment Variable: MAS_WORKSPACE_ID Default: None","title":"[optional] mas_workspace_id"},{"location":"roles/db2u/#optional-mas_application_id","text":"This is only used when both mas_config_dir and mas_instance_id are set, and mas_config_scope is set to either app or wsapp Environment Variable: 'MAS_APP_ID Default: None","title":"[optional] mas_application_id"},{"location":"roles/db2u/#optional-db2u_workload","text":"The workload profile of the db2 instance, possible values are 'PUREDATA_OLAP' or 'ANALYTICS' Environment Variable: 'DB2U_WORKLOAD Default: 'ANALYTICS'","title":"[optional] db2u_workload"},{"location":"roles/db2u/#db2u_node_label","text":"The label used to specify node affinity and tolerations in the db2ucluster CR. Environment Variable: 'DB2U_NODE_LABEL Default: None","title":"db2u_node_label"},{"location":"roles/db2u/#db2u_dedicated_node","text":"The name of the worker node to apply the {{ db2u_node_label }} taint and label to. Environment Variable: 'DB2U_DEDICATED_NODE Default: None","title":"db2u_dedicated_node"},{"location":"roles/db2u/#db2u_mln_count","text":"The number of logical nodes (i.e. database partitions to create). Environment Variable: 'DB2U_MLN_COUNT Default: 1","title":"db2u_mln_count"},{"location":"roles/db2u/#db2u_num_pods","text":"The number of Db2 pods to create in the instance. Note that db2u_num_pods must be less than or equal to db2u_mln_count . A single db2u pod can contain multiple logical nodes. So be sure to avoid specifying a large number for db2u_mln_count while specifying a small number for db2u_num_pods . If in doubt, make db2u_mln_count = db2u_num_pods . For more information refer to the Db2 documentation . Environment Variable: 'DB2U_NUM_PODS Default: 1","title":"db2u_num_pods"},{"location":"roles/db2u/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Configuration for the Db2 cluster db2u_version: 11.5.7.0-cn2 db2u_instance_name: db2u-db01 db2u_dbname: BLUDB db2u_meta_storage_class: \"ibmc-file-gold\" db2u_data_storage_class: \"ibmc-block-gold\" db2u_backup_storage_class: \"ibmc-file-gold\" db2u_logs_storage_class: \"ibmc-block-gold\" db2u_temp_storage_class: \"ibmc-block-gold\" entitlement_key: \"{{ lookup('env', 'ENTITLEMENT_KEY') }}\" # Create the MAS JdbcCfg & Secret resource definitions mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.db2u","title":"Example Playbook"},{"location":"roles/db2u/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_sls/","text":"gencfg_sls \uf0c1 This role is used to generate a SLSCfg Custom Resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. If mas_instance_id and mas_config_dir are not both set, then the role will simply print a debug message containing the configuration information. Role Variables \uf0c1 mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the SlsCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a SlsCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None slscfg_tls_crt \uf0c1 Required. TODO: Write me Environment Variable: None Default Value: None slscfg_url \uf0c1 Required. TODO: Write me Environment Variable: SLSCFG_URL Default Value: None slscfg_registration_key \uf0c1 Required. TODO: Write me Environment Variable: SLSCFG_REGISTRATION_KEY Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me slscfg_tls_crt: \"{{ lookup('file', '/home/me/sls.crt') }}\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls_gencfg License \uf0c1 EPL-2.0","title":"gencfg_sls"},{"location":"roles/gencfg_sls/#gencfg_sls","text":"This role is used to generate a SLSCfg Custom Resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. If mas_instance_id and mas_config_dir are not both set, then the role will simply print a debug message containing the configuration information.","title":"gencfg_sls"},{"location":"roles/gencfg_sls/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_sls/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the SlsCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a SlsCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/gencfg_sls/#mas_config_dir","text":"Local directory to save the generated SlsCfg resource definition. This can be used to manually configure a MAS instance to connect to SLS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a SlsCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/gencfg_sls/#slscfg_tls_crt","text":"Required. TODO: Write me Environment Variable: None Default Value: None","title":"slscfg_tls_crt"},{"location":"roles/gencfg_sls/#slscfg_url","text":"Required. TODO: Write me Environment Variable: SLSCFG_URL Default Value: None","title":"slscfg_url"},{"location":"roles/gencfg_sls/#slscfg_registration_key","text":"Required. TODO: Write me Environment Variable: SLSCFG_REGISTRATION_KEY Default Value: None","title":"slscfg_registration_key"},{"location":"roles/gencfg_sls/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: inst1 mas_config_dir: /home/me slscfg_tls_crt: \"{{ lookup('file', '/home/me/sls.crt') }}\" slscfg_url: \"https://xxx\" slscfg_registration_key: \"xxx\" roles: - ibm.mas_devops.sls_gencfg","title":"Example Playbook"},{"location":"roles/gencfg_sls/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gencfg_workspace/","text":"gencfg_workspace \uf0c1 This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable. Role Variables \uf0c1 mas_instance_id \uf0c1 Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None mas_workspace_id \uf0c1 Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None mas_workspace_name \uf0c1 Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None mas_config_dir \uf0c1 Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace License \uf0c1 EPL-2.0","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#gencfg_workspace","text":"This role is used to generate a Workspace custom resource that can be applied to Maximo Application Suite manually, or using the suite_config role. The configuration will be saved to local disk in the directory specified by the mas_config_dir variable.","title":"gencfg_workspace"},{"location":"roles/gencfg_workspace/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gencfg_workspace/#mas_instance_id","text":"Required. The MAS instance ID that the workspace will be used in Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/gencfg_workspace/#mas_workspace_id","text":"Required. The ID of the workspace Environment Variable: MAS_WORKSPACE_ID Default Value: None","title":"mas_workspace_id"},{"location":"roles/gencfg_workspace/#mas_workspace_name","text":"Required. The display name for the workspace Environment Variable: MAS_WORKSPACE_NAME Default Value: None","title":"mas_workspace_name"},{"location":"roles/gencfg_workspace/#mas_config_dir","text":"Required. The directory to save the configuration to. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/gencfg_workspace/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_workspace_id: \"masdev\" mas_workspace_name: \"MAS Development\" mas_config_dir: \"/home/david/masconfig/inst1\" roles: - ibm.mas_devops.gencfg_workspace","title":"Example Playbook"},{"location":"roles/gencfg_workspace/#license","text":"EPL-2.0","title":"License"},{"location":"roles/gpu_install/","text":"gpu_install \uf0c1 This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source. Role Variables \uf0c1 nfd_namespace \uf0c1 The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: gpu-operator-resources nfd_channel \uf0c1 The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable gpu_namespace \uf0c1 The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore namespace is defaulted to openshift-operators . NVIDIA's suggested namespace to use for versions 1.9.0 and above is nvidia-gpu-operator Environment Variable: GPU_NAMESPACE Default Value: openshift-operators gpu_channel \uf0c1 The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v1.8 gpu_driver_version \uf0c1 The gpu driver version image that needs to be pulled from the gpu repository. It is recommended that the right version if GPU driver is used. The MVI installation documentation, the default version below should be used. Environment Variable: GPU_DRIVER_VERSION Default Value: 450.80.02 For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gpu_install License \uf0c1 EPL-2.0","title":"Gpu install"},{"location":"roles/gpu_install/#gpu_install","text":"This role installs the NVIDIA Graphical Processing Unit (GPU) operator and its prerequisite Node Feature Discovery (NFD) operator in an IBM Cloud Openshift cluster console. The role first installs the NFD operator and continues with the final step to install the NVIDIA GPU Operator. The NFD Operator is installed using the Red Hat Operators catalog source and the GPU operator is installed using the Certified Operators catalog source.","title":"gpu_install"},{"location":"roles/gpu_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/gpu_install/#nfd_namespace","text":"The namespace where the node feature discovery operator will be deployed. Environment Variable: NFD_NAMESPACE Default Value: gpu-operator-resources","title":"nfd_namespace"},{"location":"roles/gpu_install/#nfd_channel","text":"The channel to subscribe to for the nfd operator installation and updates. Available channels may be found in the package manifest of nfd operator in openshift. Environment Variable: NFD_CHANNEL Default Value: stable","title":"nfd_channel"},{"location":"roles/gpu_install/#gpu_namespace","text":"The namespace where the NVIDIA GPU operator will be deployed. For version 1.8.x, use of single namespace is not supported, therefore namespace is defaulted to openshift-operators . NVIDIA's suggested namespace to use for versions 1.9.0 and above is nvidia-gpu-operator Environment Variable: GPU_NAMESPACE Default Value: openshift-operators","title":"gpu_namespace"},{"location":"roles/gpu_install/#gpu_channel","text":"The channel to subscribe to for the gpu operator installation and updates. Available channels may be found in the package manifest of gpu-operator-certified operator in openshift. Environment Variable: GPU_CHANNEL Default Value: v1.8","title":"gpu_channel"},{"location":"roles/gpu_install/#gpu_driver_version","text":"The gpu driver version image that needs to be pulled from the gpu repository. It is recommended that the right version if GPU driver is used. The MVI installation documentation, the default version below should be used. Environment Variable: GPU_DRIVER_VERSION Default Value: 450.80.02 For more information on the NVIDIA GPU and NFD operators, visit https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html","title":"gpu_driver_version"},{"location":"roles/gpu_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true roles: - ibm.mas_devops.gpu_install","title":"Example Playbook"},{"location":"roles/gpu_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/install_operator/","text":"install_operator \uf0c1 TODO: Summarize role Role Variables \uf0c1 TODO: Finish documentation Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"install_operator"},{"location":"roles/install_operator/#install_operator","text":"TODO: Summarize role","title":"install_operator"},{"location":"roles/install_operator/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/install_operator/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/install_operator/#license","text":"EPL-2.0","title":"License"},{"location":"roles/mongodb/","text":"mongodb \uf0c1 MongoDb CE operator will be installed into the specified namespace, a 3 node cluster cluster will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role. Role Variables \uf0c1 mongodb_namespace \uf0c1 The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce mongodb_storage_class \uf0c1 Required. The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None mongodb_storage_capacity_data \uf0c1 The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi mongodb_storage_capacity_logs \uf0c1 The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi mongodb_cpu_limits \uf0c1 The CPU limits on the mongod container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1 mongodb_mem_limits \uf0c1 The Memory limits on the mongod container. Environment Variable: MONGODB_MEM_LIMITS_GB Default Value: 1Gi mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb License \uf0c1 EPL-2.0","title":"mongodb"},{"location":"roles/mongodb/#mongodb","text":"MongoDb CE operator will be installed into the specified namespace, a 3 node cluster cluster will be created. The cluster will bind six PVCs, these provide persistence for the data and system logs across the three nodes. Currently there is no support built-in for customizing the cluster beyond this configuration. Tip The role will generate a yaml file containing the definition of a Secret and MongoCfg resource that can be used to configure the deployed instance as the MAS system MongoDb. This file can be directly applied using oc apply -f $MAS_CONFIG_DIR/mongocfg-mongoce-system.yaml or used in conjunction with the suite_config role.","title":"mongodb"},{"location":"roles/mongodb/#role-variables","text":"","title":"Role Variables"},{"location":"roles/mongodb/#mongodb_namespace","text":"The namespace where the operator and MongoDb cluster will be deployed. Environment Variable: MONGODB_NAMESPACE Default Value: mongoce","title":"mongodb_namespace"},{"location":"roles/mongodb/#mongodb_storage_class","text":"Required. The name of the storage class to configure the MongoDb operator to use for persistent storage in the MongoDb cluster. Environment Variable: MONGODB_STORAGE_CLASS Default Value: None","title":"mongodb_storage_class"},{"location":"roles/mongodb/#mongodb_storage_capacity_data","text":"The size of the PVC that will be created for data storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_DATA Default Value: 20Gi","title":"mongodb_storage_capacity_data"},{"location":"roles/mongodb/#mongodb_storage_capacity_logs","text":"The size of the PVC that will be created for log storage in the cluster. Environment Variable: MONGODB_STORAGE_CAPACITY_LOGS Default Value: 20Gi","title":"mongodb_storage_capacity_logs"},{"location":"roles/mongodb/#mongodb_cpu_limits","text":"The CPU limits on the mongod container. Environment Variable: MONGODB_CPU_LIMITS Default Value: 1","title":"mongodb_cpu_limits"},{"location":"roles/mongodb/#mongodb_mem_limits","text":"The Memory limits on the mongod container. Environment Variable: MONGODB_MEM_LIMITS_GB Default Value: 1Gi","title":"mongodb_mem_limits"},{"location":"roles/mongodb/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the MongoCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/mongodb/#mas_config_dir","text":"Local directory to save the generated MongoCfg resource definition. This can be used to manually configure a MAS instance to connect to the Mongo cluster, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a MongoCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/mongodb/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mongodb_storage_class: ibmc-block-gold mas_instance_id: masinst1 mas_config_dir: ~/masconfig roles: - ibm.mas_devops.mongodb","title":"Example Playbook"},{"location":"roles/mongodb/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_deprovision/","text":"ocp_deprovision \uf0c1 Deprovision OCP cluster in Fyre and IBM Cloud Role Variables \uf0c1 cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks ROKS specific facts \uf0c1 ibmcloud_apikey APIKey to be used by ibmcloud login comand Fyre specific facts \uf0c1 username Required when cluster type is quickburn password Required when cluster type is quickburn Example Playbook \uf0c1 - hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_deprovision License \uf0c1 EPL-2.0","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#ocp_deprovision","text":"Deprovision OCP cluster in Fyre and IBM Cloud","title":"ocp_deprovision"},{"location":"roles/ocp_deprovision/#role-variables","text":"cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks","title":"Role Variables"},{"location":"roles/ocp_deprovision/#roks-specific-facts","text":"ibmcloud_apikey APIKey to be used by ibmcloud login comand","title":"ROKS specific facts"},{"location":"roles/ocp_deprovision/#fyre-specific-facts","text":"username Required when cluster type is quickburn password Required when cluster type is quickburn","title":"Fyre specific facts"},{"location":"roles/ocp_deprovision/#example-playbook","text":"- hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_deprovision","title":"Example Playbook"},{"location":"roles/ocp_deprovision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_login/","text":"ocp_login \uf0c1 This role provides support to login to a cluster using the oc cli . If you set ocp_server and ocp_token then a non cluster type specific login is attempted rather than using the cluster_type specific facts (apikey or username/password). Role Variables \uf0c1 cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks ROKS specific facts \uf0c1 ibmcloud_apikey APIKey to be used by ibmcloud login comand Fyre specific facts \uf0c1 username Required when cluster type is quickburn password Required when cluster type is quickburn Non cluster_type specific facts \uf0c1 ocp_server The OCP server address to perform oc login against ocp_token The login token to use for oc login Example Playbook \uf0c1 - hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_login License \uf0c1 EPL-2.0","title":"ocp_login"},{"location":"roles/ocp_login/#ocp_login","text":"This role provides support to login to a cluster using the oc cli . If you set ocp_server and ocp_token then a non cluster type specific login is attempted rather than using the cluster_type specific facts (apikey or username/password).","title":"ocp_login"},{"location":"roles/ocp_login/#role-variables","text":"cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks","title":"Role Variables"},{"location":"roles/ocp_login/#roks-specific-facts","text":"ibmcloud_apikey APIKey to be used by ibmcloud login comand","title":"ROKS specific facts"},{"location":"roles/ocp_login/#fyre-specific-facts","text":"username Required when cluster type is quickburn password Required when cluster type is quickburn","title":"Fyre specific facts"},{"location":"roles/ocp_login/#non-cluster_type-specific-facts","text":"ocp_server The OCP server address to perform oc login against ocp_token The login token to use for oc login","title":"Non cluster_type specific facts"},{"location":"roles/ocp_login/#example-playbook","text":"- hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_login","title":"Example Playbook"},{"location":"roles/ocp_login/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_provision/","text":"ocp_provision \uf0c1 Provision OCP cluster on DevIT Fyre or IBM Cloud ROKS. Role Variables \uf0c1 cluster_type \uf0c1 Required. Specify the cluster type, supported values are roks and quickburn . Environment Variable: CLUSTER_TYPE Default Value: None cluster_name \uf0c1 Required. Specify the name of the cluster Environment Variable: CLUSTER_NAME Default Value: None ocp_version \uf0c1 Required. Specify the version of OCP to install. The exact format of this will vary depending on cluster_type . For ROKS clusters the format is 4.6_openshift , 4.8_openshift , for Fyre it is 4.6.16 . Environment Variable: OCP_VERSION Default Value: None Role Variables - ROKS \uf0c1 The following variables are only used when cluster_type = roks . ibmcloud_apikey \uf0c1 Required if cluster_type = roks . The APIKey to be used by ibmcloud login comand. Environment Variable: IBMCLOUD_APIKEY Default Value: None ibmcloud_resourcegroup \uf0c1 The resource group to create the cluster inside. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default roks_zone \uf0c1 IBM Cloud zone where the cluster should be provisioned. Environment Variable: ROKS_ZONE Default Value: lon02 roks_flavor \uf0c1 Worker node flavor Environment Variable: ROKS_FLAVOR Default Value: b3c.8x32 roks_workers \uf0c1 Number of worker nodes for the roks cluster Environment Variable: ROKS_WORKERS Default Value: 6 roks_flags \uf0c1 Can be used to specify additional parameters for the cluster creation Environment Variable: ROKS_FLAGS Default Value: None Role Variables - Quickburn \uf0c1 The following variables are only used when cluster_type = quickburn . fyre_username \uf0c1 Required if cluster_type = quickburn . IBM Cloud zone where the cluster should be provisioned. Environment Variable: FYRE_USERNAME Default Value: None fyre_password \uf0c1 Required if cluster_type = quickburn . IBM Cloud zone where the cluster should be provisioned. Environment Variable: FYRE_PASSWORD Default Value: None fyre_product_id \uf0c1 Required if cluster_type = quickburn . The Product ID that the cluster will be associated with for accounting purposes. Environment Variable: FYRE_PRODUCT_ID Default Value: None fyre_cluster_size \uf0c1 The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Environment Variable: FYRE_CLUSTER_SIZE Default Value: large Example Playbook - ROKS \uf0c1 - hosts: localhost vars: cluster_name: masinst1 cluster_type: roks ocp_version: \"4.8_openshift\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" roles: - ibm.mas_devops.ocp_provision Example Playbook - Quickburn \uf0c1 - hosts: localhost vars: cluster_name: masinst1 cluster_type: quickburn ocp_version: \"4.6.16\" fyre_username: \"{{ lookup('env', 'FYRE_USERNAME') }}\" fyre_password: \"{{ lookup('env', 'FYRE_PASSWORD') }}\" fyre_product_id: \"{{ lookup('env', 'FYRE_PRODUCT_ID') }}\" roles: - ibm.mas_devops.ocp_provision License \uf0c1 EPL-2.0","title":"ocp_provision"},{"location":"roles/ocp_provision/#ocp_provision","text":"Provision OCP cluster on DevIT Fyre or IBM Cloud ROKS.","title":"ocp_provision"},{"location":"roles/ocp_provision/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_provision/#cluster_type","text":"Required. Specify the cluster type, supported values are roks and quickburn . Environment Variable: CLUSTER_TYPE Default Value: None","title":"cluster_type"},{"location":"roles/ocp_provision/#cluster_name","text":"Required. Specify the name of the cluster Environment Variable: CLUSTER_NAME Default Value: None","title":"cluster_name"},{"location":"roles/ocp_provision/#ocp_version","text":"Required. Specify the version of OCP to install. The exact format of this will vary depending on cluster_type . For ROKS clusters the format is 4.6_openshift , 4.8_openshift , for Fyre it is 4.6.16 . Environment Variable: OCP_VERSION Default Value: None","title":"ocp_version"},{"location":"roles/ocp_provision/#role-variables-roks","text":"The following variables are only used when cluster_type = roks .","title":"Role Variables - ROKS"},{"location":"roles/ocp_provision/#ibmcloud_apikey","text":"Required if cluster_type = roks . The APIKey to be used by ibmcloud login comand. Environment Variable: IBMCLOUD_APIKEY Default Value: None","title":"ibmcloud_apikey"},{"location":"roles/ocp_provision/#ibmcloud_resourcegroup","text":"The resource group to create the cluster inside. Environment Variable: IBMCLOUD_RESOURCEGROUP Default Value: Default","title":"ibmcloud_resourcegroup"},{"location":"roles/ocp_provision/#roks_zone","text":"IBM Cloud zone where the cluster should be provisioned. Environment Variable: ROKS_ZONE Default Value: lon02","title":"roks_zone"},{"location":"roles/ocp_provision/#roks_flavor","text":"Worker node flavor Environment Variable: ROKS_FLAVOR Default Value: b3c.8x32","title":"roks_flavor"},{"location":"roles/ocp_provision/#roks_workers","text":"Number of worker nodes for the roks cluster Environment Variable: ROKS_WORKERS Default Value: 6","title":"roks_workers"},{"location":"roles/ocp_provision/#roks_flags","text":"Can be used to specify additional parameters for the cluster creation Environment Variable: ROKS_FLAGS Default Value: None","title":"roks_flags"},{"location":"roles/ocp_provision/#role-variables-quickburn","text":"The following variables are only used when cluster_type = quickburn .","title":"Role Variables - Quickburn"},{"location":"roles/ocp_provision/#fyre_username","text":"Required if cluster_type = quickburn . IBM Cloud zone where the cluster should be provisioned. Environment Variable: FYRE_USERNAME Default Value: None","title":"fyre_username"},{"location":"roles/ocp_provision/#fyre_password","text":"Required if cluster_type = quickburn . IBM Cloud zone where the cluster should be provisioned. Environment Variable: FYRE_PASSWORD Default Value: None","title":"fyre_password"},{"location":"roles/ocp_provision/#fyre_product_id","text":"Required if cluster_type = quickburn . The Product ID that the cluster will be associated with for accounting purposes. Environment Variable: FYRE_PRODUCT_ID Default Value: None","title":"fyre_product_id"},{"location":"roles/ocp_provision/#fyre_cluster_size","text":"The name of one of Fyre's pre-defined cluster sizes to use for the new cluster. Environment Variable: FYRE_CLUSTER_SIZE Default Value: large","title":"fyre_cluster_size"},{"location":"roles/ocp_provision/#example-playbook-roks","text":"- hosts: localhost vars: cluster_name: masinst1 cluster_type: roks ocp_version: \"4.8_openshift\" ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook - ROKS"},{"location":"roles/ocp_provision/#example-playbook-quickburn","text":"- hosts: localhost vars: cluster_name: masinst1 cluster_type: quickburn ocp_version: \"4.6.16\" fyre_username: \"{{ lookup('env', 'FYRE_USERNAME') }}\" fyre_password: \"{{ lookup('env', 'FYRE_PASSWORD') }}\" fyre_product_id: \"{{ lookup('env', 'FYRE_PRODUCT_ID') }}\" roles: - ibm.mas_devops.ocp_provision","title":"Example Playbook - Quickburn"},{"location":"roles/ocp_provision/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_setup_github_oauth/","text":"ocp_setup_github_oauth \uf0c1 This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action. Role Variables \uf0c1 oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"ocp_setup_github_oauth"},{"location":"roles/ocp_setup_github_oauth/#ocp_setup_github_oauth","text":"This role provides to support to configure cluster oauth using GitHub. Warning Make sure you have configured the oauth app in GitHub organization before use this role. When configuring make sure to use ibmgithub as the oauth id. Requires organization admin permission to perform this action.","title":"ocp_setup_github_oauth"},{"location":"roles/ocp_setup_github_oauth/#role-variables","text":"oauth.github_client_secret_value Secret value provided by the GitHub oauth app configuration. ouath.github_client_id_value Client ID value provided by the GitHub oauth app configuration. oauth.github_hostname can be used to target public GitHub or an enterprise account (e.g. github.ibm.com) oauth.groups List of groups to be created and its cluster role bindings oauth.groups.name Defines the name of the group oauth.groups.users List of users to be added to the group oauth.groups.groups_cluster_rolebindings List of cluster role bindings to be created for the group oauth.organizations List of GitHub organizations where the authentication will be performed","title":"Role Variables"},{"location":"roles/ocp_setup_github_oauth/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/ocp_setup_github_oauth/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_setup_mas_deps/","text":"ocp_setup_mas_deps \uf0c1 This role provides support to install operators that are required by MAS to work. The role will deploy Service Binding Operator in all namespaces and Cert Manager in the cert-manager namespace. The role declares a dependency on ocp_verify to ensure that the RedHat Operator Catalog is installed and ready before we try to install the Service Binding Operator from that catalog. In addition, this role updates cluster's internal image registry settings to increase storage to 400GB (needed only for ROKS Cluster to install full stack of services in CP4D) and to configure custom storage for the Prometheus monitoring service for both the k8s and User Workload (in namespace openshift-user-workload-monitoring ) Prometheus instances. For MAS 8.6 or earlier JetStack cert-manager v1.2 is installed into the cert-manager namespace. When used for MAS 8.7+ this role will result in the following operators being installed in the ibm-common-services namespace: - IBM Cert Manager - IBM Cloud Pak Foundational Services - IBM NamespaceScope Operator - Operand Deployment Lifecycle Manager For MAS 8.6 or earlier (or when running MAS 8.7 on OCP 4.6) Service Binding Operator v0.8 will be installed from the preview channel. It is important not to upgrade to later preview builds as they are incompatible with MAS due to breaking API changes in SBO. For MAS 8.7 or later the stable channel will be used instead, with automatic updates enabled. In both cases, the operator will be installed in the openshift-operators namespace with cluster scope. Role Variables \uf0c1 artifactory_username \uf0c1 Use to enable the install of development catalog sources for pre-release installation. Environment Variable: W3_USERNAME Default Value: None artifactory_apikey \uf0c1 Use to enable the install of development catalog sources for pre-release installation. Environment Variable: ARTIFACTORY_APIKEY Default Value: None prometheus_retention_period \uf0c1 Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d prometheus_storage_class \uf0c1 Declare the storage class for Prometheus' metrics data persistent volume. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: None prometheus_storage_size \uf0c1 Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 300Gi prometheus_alertmgr_storage_class \uf0c1 Declare the storage class for AlertManager's persistent volume. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: None prometheus_alertmgr_storage_size \uf0c1 Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi prometheus_userworkload_retention_period \uf0c1 Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d prometheus_userworkload_storage_class \uf0c1 Declare the storage class for User Workload Prometheus' metrics data persistent volume. Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS prometheus_userworkload_storage_size \uf0c1 Adjust the size of the volume used to store User Workload metrics. Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 300Gi mas_channel \uf0c1 Used to determine whether to install SBO stable channel and the IBM badged cert-manager. Environment Variable: MAS_CHANNEL Default Value: 8.x Example Playbook \uf0c1 - hosts: localhost vars: cluster_type: roks prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_setup_mas_deps License \uf0c1 EPL-2.0","title":"ocp_setup_mas_deps"},{"location":"roles/ocp_setup_mas_deps/#ocp_setup_mas_deps","text":"This role provides support to install operators that are required by MAS to work. The role will deploy Service Binding Operator in all namespaces and Cert Manager in the cert-manager namespace. The role declares a dependency on ocp_verify to ensure that the RedHat Operator Catalog is installed and ready before we try to install the Service Binding Operator from that catalog. In addition, this role updates cluster's internal image registry settings to increase storage to 400GB (needed only for ROKS Cluster to install full stack of services in CP4D) and to configure custom storage for the Prometheus monitoring service for both the k8s and User Workload (in namespace openshift-user-workload-monitoring ) Prometheus instances. For MAS 8.6 or earlier JetStack cert-manager v1.2 is installed into the cert-manager namespace. When used for MAS 8.7+ this role will result in the following operators being installed in the ibm-common-services namespace: - IBM Cert Manager - IBM Cloud Pak Foundational Services - IBM NamespaceScope Operator - Operand Deployment Lifecycle Manager For MAS 8.6 or earlier (or when running MAS 8.7 on OCP 4.6) Service Binding Operator v0.8 will be installed from the preview channel. It is important not to upgrade to later preview builds as they are incompatible with MAS due to breaking API changes in SBO. For MAS 8.7 or later the stable channel will be used instead, with automatic updates enabled. In both cases, the operator will be installed in the openshift-operators namespace with cluster scope.","title":"ocp_setup_mas_deps"},{"location":"roles/ocp_setup_mas_deps/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_setup_mas_deps/#artifactory_username","text":"Use to enable the install of development catalog sources for pre-release installation. Environment Variable: W3_USERNAME Default Value: None","title":"artifactory_username"},{"location":"roles/ocp_setup_mas_deps/#artifactory_apikey","text":"Use to enable the install of development catalog sources for pre-release installation. Environment Variable: ARTIFACTORY_APIKEY Default Value: None","title":"artifactory_apikey"},{"location":"roles/ocp_setup_mas_deps/#prometheus_retention_period","text":"Adjust the retention period for Prometheus metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_RETENTION_PERIOD Default Value: 15d","title":"prometheus_retention_period"},{"location":"roles/ocp_setup_mas_deps/#prometheus_storage_class","text":"Declare the storage class for Prometheus' metrics data persistent volume. Environment Variable: PROMETHEUS_STORAGE_CLASS Default Value: None","title":"prometheus_storage_class"},{"location":"roles/ocp_setup_mas_deps/#prometheus_storage_size","text":"Adjust the size of the volume used to store metrics, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_STORAGE_SIZE Default Value: 300Gi","title":"prometheus_storage_size"},{"location":"roles/ocp_setup_mas_deps/#prometheus_alertmgr_storage_class","text":"Declare the storage class for AlertManager's persistent volume. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_CLASS Default Value: None","title":"prometheus_alertmgr_storage_class"},{"location":"roles/ocp_setup_mas_deps/#prometheus_alertmgr_storage_size","text":"Adjust the size of the volume used by AlertManager, only used when both prometheus_storage_class and prometheus_alertmgr_storage_class are set. Environment Variable: PROMETHEUS_ALERTMGR_STORAGE_SIZE Default Value: 20Gi","title":"prometheus_alertmgr_storage_size"},{"location":"roles/ocp_setup_mas_deps/#prometheus_userworkload_retention_period","text":"Adjust the retention period for User Workload Prometheus metrics, this parameter applies only to the User Workload Prometheus instance. Environment Variable: PROMETHEUS_USERWORKLOAD_RETENTION_PERIOD Default Value: 15d","title":"prometheus_userworkload_retention_period"},{"location":"roles/ocp_setup_mas_deps/#prometheus_userworkload_storage_class","text":"Declare the storage class for User Workload Prometheus' metrics data persistent volume. Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_CLASS Default Value: PROMETHEUS_STORAGE_CLASS","title":"prometheus_userworkload_storage_class"},{"location":"roles/ocp_setup_mas_deps/#prometheus_userworkload_storage_size","text":"Adjust the size of the volume used to store User Workload metrics. Environment Variable: PROMETHEUS_USERWORKLOAD_STORAGE_SIZE Default Value: 300Gi","title":"prometheus_userworkload_storage_size"},{"location":"roles/ocp_setup_mas_deps/#mas_channel","text":"Used to determine whether to install SBO stable channel and the IBM badged cert-manager. Environment Variable: MAS_CHANNEL Default Value: 8.x","title":"mas_channel"},{"location":"roles/ocp_setup_mas_deps/#example-playbook","text":"- hosts: localhost vars: cluster_type: roks prometheus_storage_class: \"ibmc-block-gold\" prometheus_alertmgr_storage_class: \"ibmc-file-gold-gid\" roles: - ibm.mas_devops.ocp_setup_mas_deps","title":"Example Playbook"},{"location":"roles/ocp_setup_mas_deps/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_setup_ocs/","text":"ocp_setup_ocs \uf0c1 This role provides support to install Openshift Container Storage. This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. Unfortunately, starting fromOCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep. Role Variables \uf0c1 ocp_release \uf0c1 Set this to e.g. 4.6 , 4.7 , 4.8 . We need to know what the release level is to know what channel to target the operator subscriptions at. Environment Variable: OCP_RELEASE Default Value: 4.6 Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: ocp_release: \"4.6\" roles: - ibm.mas_devops.ocp_setup_ocs License \uf0c1 EPL-2.0","title":"ocp_setup_ocs"},{"location":"roles/ocp_setup_ocs/#ocp_setup_ocs","text":"This role provides support to install Openshift Container Storage. This role is not used by default when setting up IBM Cloud ROKS clusters because they are automatically provisioned with their own storage plugin already. Unfortunately, starting fromOCP 4.8 IBM/Red Hat have decided to stop supporting OCS on IBMCloud ROKS. So this role is of limited value to users of ROKS going forward. If you attempt to install OpenShift Container Storage on ROKS via a Subscription channel you will be met by the following error as the admission webhook has been coded to prevent use of the OCS operator on IBM Cloud ROKS: Failed to apply object: \"admission webhook \"validate.managed.openshift.io\" denied the request: Installing OpenShift Data Foundation on IBM Cloud by using OperatorHub is not supported. You can install OpenShift Data Foundation by using the IBM Cloud add-on. For more information, see https://cloud.ibm.com/docs/openshift?topic=openshift-ocs-storage-prep.","title":"ocp_setup_ocs"},{"location":"roles/ocp_setup_ocs/#role-variables","text":"","title":"Role Variables"},{"location":"roles/ocp_setup_ocs/#ocp_release","text":"Set this to e.g. 4.6 , 4.7 , 4.8 . We need to know what the release level is to know what channel to target the operator subscriptions at. Environment Variable: OCP_RELEASE Default Value: 4.6","title":"ocp_release"},{"location":"roles/ocp_setup_ocs/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: ocp_release: \"4.6\" roles: - ibm.mas_devops.ocp_setup_ocs","title":"Example Playbook"},{"location":"roles/ocp_setup_ocs/#license","text":"EPL-2.0","title":"License"},{"location":"roles/ocp_verify/","text":"ocp_verify \uf0c1 This role will verify that a provisioned OCP cluster is ready to be setup for MAS. In IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into those roles are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use. Role Variables \uf0c1 The role requires no variables itself, but depends on the ibm.mas_devops.ocp_login role, and as such inherits it's requirements. If you set ocp_server and ocp_token then a non cluster type specific login is attempted rather than using the cluster_type specific facts (apikey or username/password). cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks ROKS specific facts \uf0c1 ibmcloud_apikey APIKey to be used by ibmcloud login comand Fyre specific facts \uf0c1 username Required when cluster type is quickburn password Required when cluster type is quickburn Non cluster_type specific facts \uf0c1 ocp_server The OCP server address to perform oc login against ocp_token The login token to use for oc login Example Playbook \uf0c1 - hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_verify License \uf0c1 EPL-2.0","title":"ocp_verify"},{"location":"roles/ocp_verify/#ocp_verify","text":"This role will verify that a provisioned OCP cluster is ready to be setup for MAS. In IBMCloud ROKS we have seen delays of over an hour before the Red Hat Operator catalog is ready to use. This will cause attempts to install anything from that CatalogSource to fail as the timeouts built into those roles are designed to catch problems with an install, rather than a half-provisioned cluster that is not properly ready to use.","title":"ocp_verify"},{"location":"roles/ocp_verify/#role-variables","text":"The role requires no variables itself, but depends on the ibm.mas_devops.ocp_login role, and as such inherits it's requirements. If you set ocp_server and ocp_token then a non cluster type specific login is attempted rather than using the cluster_type specific facts (apikey or username/password). cluster_name Gives a name for the provisioning cluster cluster_type quickburn | roks","title":"Role Variables"},{"location":"roles/ocp_verify/#roks-specific-facts","text":"ibmcloud_apikey APIKey to be used by ibmcloud login comand","title":"ROKS specific facts"},{"location":"roles/ocp_verify/#fyre-specific-facts","text":"username Required when cluster type is quickburn password Required when cluster type is quickburn","title":"Fyre specific facts"},{"location":"roles/ocp_verify/#non-cluster_type-specific-facts","text":"ocp_server The OCP server address to perform oc login against ocp_token The login token to use for oc login","title":"Non cluster_type specific facts"},{"location":"roles/ocp_verify/#example-playbook","text":"- hosts: localhost vars: cluster_name: \"{{ lookup('env', 'CLUSTER_NAME')}}\" cluster_type: roks ibmcloud_apikey: \"{{ lookup('env', 'IBMCLOUD_APIKEY') }}\" ibmcloud_resourcegroup: \"{{ lookup('env', 'IBMCLOUD_RESOURCEGROUP') | default('Default', true) }}\" roles: - ibm.mas_devops.ocp_verify","title":"Example Playbook"},{"location":"roles/ocp_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/sls_install/","text":"sls_install \uf0c1 Install IBM Suite License Service and generate properties that can be used in the sls_gencfg role to create a configuration that can be directly applied to IBM Maximo Application Suite. Role Variables \uf0c1 sls_catalog_source \uf0c1 TODO: Write me Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog sls_channel \uf0c1 TODO: Write me Environment Variable: SLS_CHANNEL Default: 3.x sls_namespace \uf0c1 Define the namespace where sls must be installed. Environment Variable: SLS_NAMESPACE Default: ibm-sls sls_icr_cp \uf0c1 TODO: Write me Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp sls_icr_cpopen \uf0c1 TODO: Write me Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen sls_instance_name \uf0c1 Defines the instance ID to be used for SLS installation. Environment Variable: SLS_INSTANCE_NAME Default: sls sls_entitlement_username \uf0c1 Username for entitled registry. This username will be used to create the image pull secret. Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp sls_entitlement_key \uf0c1 Required. API Key for entitled registry. This password will be used to create the image pull secret. Environment Variable: SLS_ENTITLEMENT_KEY or IBM_ENTITLEMENT_KEY Default: sls sls_storage_class \uf0c1 TODO: Write me Environment Variable: SLS_STORAGE_CLASS Default: None sls_storage_size \uf0c1 TODO: Write me Environment Variable: None Default: 5g sls_domain \uf0c1 TODO: Write me Environment Variable: SLS_DOMAIN Default: None sls_auth_enforce \uf0c1 TODO: Write me Environment Variable: SLS_AUTH_ENFORCE Default: None sls_compliance_enforce \uf0c1 TODO: Write me Environment Variable: SLS_COMPLIANCE_ENFORCE Default: None sls_registration_open \uf0c1 TODO: Write me Environment Variable: SLS_REGISTRATION_OPEN Default: None bootstrap.license_id \uf0c1 Defines the License Id to be used to bootstrap SLS. Environment Variable: SLS_LICENSE_ID Default: None bootstrap.registration_key \uf0c1 Defines the Registration Key to be used to bootstrap SLS. Environment Variable: SLS_REGISTRATION_KEY Default: None bootstrap.entitlement_file \uf0c1 Defines the License File to be used to bootstrap SLS. Environment Variable: None Default: {{ mas_config_dir }}/entitlement.lic sls_mongodb_cfg_file \uf0c1 Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Location of a MAS MongoCfg defintion (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None sls_mongodb.hosts \uf0c1 Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines list of host and port pair for MongoDb to be used with SLS. Environment Variable: None Default: None sls_mongodb.username \uf0c1 Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines the MongoDB Username. Environment Variable: None Default: None sls_mongodb.password \uf0c1 Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines the MongoDb Password. Environment Variable: None Default: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: sls_entitlement_key: \"{{ lookup('env', 'SLS_ENTITLEMENT_KEY') }}\" sls_storage_class: \"ibmc-block-bronze\" sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" registration_key: \"{{ lookup('env', 'SLS_REGISTRATION_KEY') }}\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls_install License \uf0c1 EPL-2.0","title":"sls_install"},{"location":"roles/sls_install/#sls_install","text":"Install IBM Suite License Service and generate properties that can be used in the sls_gencfg role to create a configuration that can be directly applied to IBM Maximo Application Suite.","title":"sls_install"},{"location":"roles/sls_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/sls_install/#sls_catalog_source","text":"TODO: Write me Environment Variable: SLS_CATALOG_SOURCE Default: ibm-operator-catalog","title":"sls_catalog_source"},{"location":"roles/sls_install/#sls_channel","text":"TODO: Write me Environment Variable: SLS_CHANNEL Default: 3.x","title":"sls_channel"},{"location":"roles/sls_install/#sls_namespace","text":"Define the namespace where sls must be installed. Environment Variable: SLS_NAMESPACE Default: ibm-sls","title":"sls_namespace"},{"location":"roles/sls_install/#sls_icr_cp","text":"TODO: Write me Environment Variable: SLS_ICR_CP Default: cp.icr.io/cp","title":"sls_icr_cp"},{"location":"roles/sls_install/#sls_icr_cpopen","text":"TODO: Write me Environment Variable: SLS_ICR_CPOPEN Default: icr.io/cpopen","title":"sls_icr_cpopen"},{"location":"roles/sls_install/#sls_instance_name","text":"Defines the instance ID to be used for SLS installation. Environment Variable: SLS_INSTANCE_NAME Default: sls","title":"sls_instance_name"},{"location":"roles/sls_install/#sls_entitlement_username","text":"Username for entitled registry. This username will be used to create the image pull secret. Environment Variable: SLS_ENTITLEMENT_USERNAME Default: cp","title":"sls_entitlement_username"},{"location":"roles/sls_install/#sls_entitlement_key","text":"Required. API Key for entitled registry. This password will be used to create the image pull secret. Environment Variable: SLS_ENTITLEMENT_KEY or IBM_ENTITLEMENT_KEY Default: sls","title":"sls_entitlement_key"},{"location":"roles/sls_install/#sls_storage_class","text":"TODO: Write me Environment Variable: SLS_STORAGE_CLASS Default: None","title":"sls_storage_class"},{"location":"roles/sls_install/#sls_storage_size","text":"TODO: Write me Environment Variable: None Default: 5g","title":"sls_storage_size"},{"location":"roles/sls_install/#sls_domain","text":"TODO: Write me Environment Variable: SLS_DOMAIN Default: None","title":"sls_domain"},{"location":"roles/sls_install/#sls_auth_enforce","text":"TODO: Write me Environment Variable: SLS_AUTH_ENFORCE Default: None","title":"sls_auth_enforce"},{"location":"roles/sls_install/#sls_compliance_enforce","text":"TODO: Write me Environment Variable: SLS_COMPLIANCE_ENFORCE Default: None","title":"sls_compliance_enforce"},{"location":"roles/sls_install/#sls_registration_open","text":"TODO: Write me Environment Variable: SLS_REGISTRATION_OPEN Default: None","title":"sls_registration_open"},{"location":"roles/sls_install/#bootstraplicense_id","text":"Defines the License Id to be used to bootstrap SLS. Environment Variable: SLS_LICENSE_ID Default: None","title":"bootstrap.license_id"},{"location":"roles/sls_install/#bootstrapregistration_key","text":"Defines the Registration Key to be used to bootstrap SLS. Environment Variable: SLS_REGISTRATION_KEY Default: None","title":"bootstrap.registration_key"},{"location":"roles/sls_install/#bootstrapentitlement_file","text":"Defines the License File to be used to bootstrap SLS. Environment Variable: None Default: {{ mas_config_dir }}/entitlement.lic","title":"bootstrap.entitlement_file"},{"location":"roles/sls_install/#sls_mongodb_cfg_file","text":"Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Location of a MAS MongoCfg defintion (as generated by the mongodb role). If provided the role will use the information in that config file to configure SLS. Environment Variable: SLS_MONGODB_CFG_FILE Default: None","title":"sls_mongodb_cfg_file"},{"location":"roles/sls_install/#sls_mongodbhosts","text":"Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines list of host and port pair for MongoDb to be used with SLS. Environment Variable: None Default: None","title":"sls_mongodb.hosts"},{"location":"roles/sls_install/#sls_mongodbusername","text":"Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines the MongoDB Username. Environment Variable: None Default: None","title":"sls_mongodb.username"},{"location":"roles/sls_install/#sls_mongodbpassword","text":"Either sls_mongodb_cfg_file or the sls_mongodb object are required to configure MongoDb. Defines the MongoDb Password. Environment Variable: None Default: None","title":"sls_mongodb.password"},{"location":"roles/sls_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: sls_entitlement_key: \"{{ lookup('env', 'SLS_ENTITLEMENT_KEY') }}\" sls_storage_class: \"ibmc-block-bronze\" sls_mongodb_cfg_file: \"/etc/mas/mongodb.yml\" bootstrap: license_id: \"aa78dd65ef10\" registration_key: \"{{ lookup('env', 'SLS_REGISTRATION_KEY') }}\" entitlement_file: \"/etc/mas/entitlement.lic\" roles: - ibm.mas_devops.sls_install","title":"Example Playbook"},{"location":"roles/sls_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_configure/","text":"suite_app_configure \uf0c1 This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite. Role Variables \uf0c1 mas_instance_id \uf0c1 Defines the instance id that was used for the MAS installation mas_app_id \uf0c1 Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_workspace_id \uf0c1 MAS application workspace to use to configure app components mas_appws_components \uf0c1 Defines the app components and versions to configure in the application workspace. mas_app_ws_spec \uf0c1 Optional. The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_app_ws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" components: base: version: 'latest' roles: - ibm.mas_devops.suite_app_config License \uf0c1 EPL-2.0","title":"suite_app_configure"},{"location":"roles/suite_app_configure/#suite_app_configure","text":"This role is used to configure specific components of the application workspace after the application has been installed in the Maximo Application Suite.","title":"suite_app_configure"},{"location":"roles/suite_app_configure/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_app_configure/#mas_instance_id","text":"Defines the instance id that was used for the MAS installation","title":"mas_instance_id"},{"location":"roles/suite_app_configure/#mas_app_id","text":"Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety","title":"mas_app_id"},{"location":"roles/suite_app_configure/#mas_workspace_id","text":"MAS application workspace to use to configure app components","title":"mas_workspace_id"},{"location":"roles/suite_app_configure/#mas_appws_components","text":"Defines the app components and versions to configure in the application workspace.","title":"mas_appws_components"},{"location":"roles/suite_app_configure/#mas_app_ws_spec","text":"Optional. The application workspace deployment spec used to configure various aspects of the application workspace configuration. Note that use of this will override anything set in mas_appws_components Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml","title":"mas_app_ws_spec"},{"location":"roles/suite_app_configure/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS workspace configuration mas_workspace_id: \"{{ lookup('env', 'MAS_WORKSPACE_ID') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" mas_app_ws_spec: bindings: jdbc: \"{{ mas_appws_jdbc_binding | default( 'system' , true) }}\" components: base: version: 'latest' roles: - ibm.mas_devops.suite_app_config","title":"Example Playbook"},{"location":"roles/suite_app_configure/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_app_install/","text":"suite_app_install \uf0c1 This role is used to install a specified application in Maximo Application Suite. Role Variables \uf0c1 mas_app_catalog_source Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. artifactory_username Required when using this role for development versions of the MAS application artifactory_apikey Required when using this role for development versions of the MAS application mas_app_channel Defines which channel of the MAS application to subscribe to. Set to 8.x when installing released version mas_instance_id Defines the instance id that was used for the MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_app_id Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_app_upgrade_strategy Defines the Upgrade strategy for the MAS Application Operator. Default is set to Manual mas_app_spec \uf0c1 Optional. The application deployment spec used to configure various aspects of the application deployment configuration. Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - IBM container registry configuration mas_icr_cp: \"{{ lookup('env', 'MAS_ICR_CP') | default('cp.icr.io/cp', true) }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Determine MAS Operator Upgrade Strategy Manual | Automatic mas_app_upgrade_strategy: \"{{ lookup('env', 'MAS_APP_UPGRADE_STRATEGY') | default('Manual', true) }}\" mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium roles: - ibm.mas_devops.suite_app_install License \uf0c1 EPL-2.0","title":"suite_app_install"},{"location":"roles/suite_app_install/#suite_app_install","text":"This role is used to install a specified application in Maximo Application Suite.","title":"suite_app_install"},{"location":"roles/suite_app_install/#role-variables","text":"mas_app_catalog_source Defines the catalog to be used to install the MAS app. You can set it to ibm-operator-catalog for release install or ibm-mas-{mas_app_id}-operators for development, where {mas_app_id} will be manage for the Manage and Health app installation, for example. artifactory_username Required when using this role for development versions of the MAS application artifactory_apikey Required when using this role for development versions of the MAS application mas_app_channel Defines which channel of the MAS application to subscribe to. Set to 8.x when installing released version mas_instance_id Defines the instance id that was used for the MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_app_id Defines the kind of application that is intended for installation such as assist , health , iot , manage , monitor , mso , predict , or safety mas_app_upgrade_strategy Defines the Upgrade strategy for the MAS Application Operator. Default is set to Manual","title":"Role Variables"},{"location":"roles/suite_app_install/#mas_app_spec","text":"Optional. The application deployment spec used to configure various aspects of the application deployment configuration. Environment Variable: None Default: defaults are specified in vars/defaultspecs/{{mas_app_id}}.yml","title":"mas_app_spec"},{"location":"roles/suite_app_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_app_catalog_source: \"{{ lookup('env', 'MAS_APP_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_app_channel: \"{{ lookup('env', 'MAS_APP_CHANNEL') | default('8.x', true) }}\" # MAS configuration mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - IBM container registry configuration mas_icr_cp: \"{{ lookup('env', 'MAS_ICR_CP') | default('cp.icr.io/cp', true) }}\" # MAS configuration - Entitlement mas_entitlement_username: \"{{ lookup('env', 'MAS_ENTITLEMENT_USERNAME') | default('cp', true) }}\" mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # MAS application configuration mas_app_id: \"{{ lookup('env', 'MAS_APP_ID') }}\" # Determine MAS Operator Upgrade Strategy Manual | Automatic mas_app_upgrade_strategy: \"{{ lookup('env', 'MAS_APP_UPGRADE_STRATEGY') | default('Manual', true) }}\" mas_app_spec: bindings: jdbc: system mongo: system kafka: system settings: messagesight: storage: class: block1000p size: 100Gi deployment: size: medium roles: - ibm.mas_devops.suite_app_install","title":"Example Playbook"},{"location":"roles/suite_app_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_config/","text":"suite_config \uf0c1 TODO: Summarize role Role Variables \uf0c1 TODO: Finish documentation Example Playbook \uf0c1 TODO: Add example License \uf0c1 EPL-2.0","title":"suite_config"},{"location":"roles/suite_config/#suite_config","text":"TODO: Summarize role","title":"suite_config"},{"location":"roles/suite_config/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/suite_config/#example-playbook","text":"TODO: Add example","title":"Example Playbook"},{"location":"roles/suite_config/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_dns/","text":"suite_dns \uf0c1 This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently. Cloud Internet Services (CIS) \uf0c1 This role will create DNS entries automatically in the CIS service instance. Two different modes are available: Top Level DNS entries \uf0c1 This mode will create the entries directly using your DNS zone value. It is usually recommended when you have 1x1 relationship between MAS Instance -> CIS service. e.g: mas.whirlpool.com, where the domain matches exactly the CIS zone name. Subdomain DNS entries \uf0c1 This mode will create entries using a subdomain. It allows you to have multiple MAS instances using same CIS service. e.g: dev.mas.whirlpool.com, where 'dev' is the subdomain. Webhook \uf0c1 The Webhook Task will deploy a cert-manager webhook for CIS integration. The webhook is responsible for managent the certificate challenge requests from letsencrypt and CIS. This task will also create two ClusterIssuers by default, pointing to Staging & Production LetsEncrypt servers. Note There are issues with how cert-manager works with LetsEncrypt staging servers. We end up with a secret for the certificate that doesn't contain the LetsEncrypt CA; but the staging service does not use a well known cert, so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer. Role Variables \uf0c1 TODO: Finish documentation Example Playbook \uf0c1 --- - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_catalog_source: \"{{ lookup('env', 'MAS_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('8.x', true) }}\" # MAS configuration custom_domain: \"{{ lookup('env', 'MAS_DOMAIN') | default(None)}}\" mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # --- DNS settings ---------------------------------------------------------------------------------------- # you can obtain CRN from overview page of your CIS service in IBM Cloud cis_crn: \"{{ lookup('env', 'CIS_CRN') }}\" # Domain prefix is whatever you want to append to your DNS entry to make it unique cis_subdomain: \"{{ lookup('env', 'CIS_SUBDOMAIN') }}\" # generate a Service ID apikey in IBM Cloud for strict access to the 'Internet Services` service with # an 'Access Policy' of Editor/Manager cis_apikey: \"{{ lookup('env', 'CIS_APIKEY') | default(lookup('env', 'IBMCLOUD_APIKEY'), true) }}\" # Email used register letsencrypt certificates and receive cert notifications cis_email: \"{{ lookup('env', 'CIS_EMAIL') }}\" # Skip DNS entries creation cis_skip_dns_entries: \"{{ lookup('env', 'CIS_SKIP_DNS_ENTRIES') | default(false, true) }}\" # Skip CIS cluster issuer creation, in case you just want to create the DNS entries cis_skip_cluster_issuer: \"{{ lookup('env', 'CIS_SKIP_CLUSTER_ISSUER') | default(false, true) }}\" # Do you want to update a DNS entry if it already exists? update_dns: \"{{ lookup('env', 'UPDATE_DNS_ENTRIES') | default(true, true) }}\" # e.g: \"apps.joaopauloksn.cp.fyre.ibm.com\" Default will be always from cluster ingress CR. custom_ocp_ingress: \"{{ lookup('env', 'OCP_INGRESS') | default(None, true)}}\" # If cis, custom_cluster_issuer = cis-letsencrypt-production-{{ mas_instance_id }} # If PKI, custom_cluster_issuer = MAS_CUSTOM_CLUSTER_ISSUER # If not specified, custom_cluster_issuer = autogenerated default_custom_cluster_issuer: \"'cis-letsencrypt-production-{{ mas_instance_id }}\" custom_cluster_issuer: \"{{ (cis_crn != '') | ternary (lookup('env', 'MAS_CUSTOM_CLUSTER_ISSUER') | default(default_custom_cluster_issuer, true), lookup('env', 'MAS_CUSTOM_CLUSTER_ISSUER') | default(None, true)) }}\" certificate_duration: \"{{ lookup('env', 'CERTIFICATE_DURATION') | default('8760h0m0s', true) }}\" certificate_renew_before: \"{{ lookup('env', 'CERTIFICATE_RENEW_BEFORE') | default('720h0m0s', true) }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify License \uf0c1 EPL-2.0","title":"suite_dns"},{"location":"roles/suite_dns/#suite_dns","text":"This role will manage MAS and DNS provider integration. IBM Cloud Internet Services is the only supported DNS provider currently.","title":"suite_dns"},{"location":"roles/suite_dns/#cloud-internet-services-cis","text":"This role will create DNS entries automatically in the CIS service instance. Two different modes are available:","title":"Cloud Internet Services (CIS)"},{"location":"roles/suite_dns/#top-level-dns-entries","text":"This mode will create the entries directly using your DNS zone value. It is usually recommended when you have 1x1 relationship between MAS Instance -> CIS service. e.g: mas.whirlpool.com, where the domain matches exactly the CIS zone name.","title":"Top Level DNS entries"},{"location":"roles/suite_dns/#subdomain-dns-entries","text":"This mode will create entries using a subdomain. It allows you to have multiple MAS instances using same CIS service. e.g: dev.mas.whirlpool.com, where 'dev' is the subdomain.","title":"Subdomain DNS entries"},{"location":"roles/suite_dns/#webhook","text":"The Webhook Task will deploy a cert-manager webhook for CIS integration. The webhook is responsible for managent the certificate challenge requests from letsencrypt and CIS. This task will also create two ClusterIssuers by default, pointing to Staging & Production LetsEncrypt servers. Note There are issues with how cert-manager works with LetsEncrypt staging servers. We end up with a secret for the certificate that doesn't contain the LetsEncrypt CA; but the staging service does not use a well known cert, so we end up with MAS unable to trust the certificates generated by LetsEncrypt staging. At present there is no workaround for this, so do not use the LetsEncrypt staging certificate issuer.","title":"Webhook"},{"location":"roles/suite_dns/#role-variables","text":"TODO: Finish documentation","title":"Role Variables"},{"location":"roles/suite_dns/#example-playbook","text":"--- - hosts: localhost any_errors_fatal: true vars: # Choose which catalog source to use for the MAS install, default to the IBM operator catalog mas_catalog_source: \"{{ lookup('env', 'MAS_CATALOG_SOURCE') | default('ibm-operator-catalog', true) }}\" # Which MAS channel to subscribe to mas_channel: \"{{ lookup('env', 'MAS_CHANNEL') | default('8.x', true) }}\" # MAS configuration custom_domain: \"{{ lookup('env', 'MAS_DOMAIN') | default(None)}}\" mas_instance_id: \"{{ lookup('env', 'MAS_INSTANCE_ID') }}\" # MAS configuration - Entitlement mas_entitlement_key: \"{{ lookup('env', 'MAS_ENTITLEMENT_KEY') }}\" # --- DNS settings ---------------------------------------------------------------------------------------- # you can obtain CRN from overview page of your CIS service in IBM Cloud cis_crn: \"{{ lookup('env', 'CIS_CRN') }}\" # Domain prefix is whatever you want to append to your DNS entry to make it unique cis_subdomain: \"{{ lookup('env', 'CIS_SUBDOMAIN') }}\" # generate a Service ID apikey in IBM Cloud for strict access to the 'Internet Services` service with # an 'Access Policy' of Editor/Manager cis_apikey: \"{{ lookup('env', 'CIS_APIKEY') | default(lookup('env', 'IBMCLOUD_APIKEY'), true) }}\" # Email used register letsencrypt certificates and receive cert notifications cis_email: \"{{ lookup('env', 'CIS_EMAIL') }}\" # Skip DNS entries creation cis_skip_dns_entries: \"{{ lookup('env', 'CIS_SKIP_DNS_ENTRIES') | default(false, true) }}\" # Skip CIS cluster issuer creation, in case you just want to create the DNS entries cis_skip_cluster_issuer: \"{{ lookup('env', 'CIS_SKIP_CLUSTER_ISSUER') | default(false, true) }}\" # Do you want to update a DNS entry if it already exists? update_dns: \"{{ lookup('env', 'UPDATE_DNS_ENTRIES') | default(true, true) }}\" # e.g: \"apps.joaopauloksn.cp.fyre.ibm.com\" Default will be always from cluster ingress CR. custom_ocp_ingress: \"{{ lookup('env', 'OCP_INGRESS') | default(None, true)}}\" # If cis, custom_cluster_issuer = cis-letsencrypt-production-{{ mas_instance_id }} # If PKI, custom_cluster_issuer = MAS_CUSTOM_CLUSTER_ISSUER # If not specified, custom_cluster_issuer = autogenerated default_custom_cluster_issuer: \"'cis-letsencrypt-production-{{ mas_instance_id }}\" custom_cluster_issuer: \"{{ (cis_crn != '') | ternary (lookup('env', 'MAS_CUSTOM_CLUSTER_ISSUER') | default(default_custom_cluster_issuer, true), lookup('env', 'MAS_CUSTOM_CLUSTER_ISSUER') | default(None, true)) }}\" certificate_duration: \"{{ lookup('env', 'CERTIFICATE_DURATION') | default('8760h0m0s', true) }}\" certificate_renew_before: \"{{ lookup('env', 'CERTIFICATE_RENEW_BEFORE') | default('720h0m0s', true) }}\" mas_config_dir: \"{{ lookup('env', 'MAS_CONFIG_DIR') }}\" roles: - ibm.mas_devops.suite_dns - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_dns/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_install/","text":"suite_install \uf0c1 This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . By default this role install MAS Operator using Manual Upgrade Strategy. Set MAS_UPGRADE_STRATEGY environment variable to Automatic to override it. In the Manual upgrade mode, IBM Common Services operators requested by MAS will inherit the upgrade strategy from MAS and their pending install plans approved. Role Variables \uf0c1 mas_catalog_source Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for release install or ibm-mas-operators for development artifactory_username Required when using this role for development versions of MAS artifactory_apikey Required when using this role for development versions of MAS mas_channel Defines which channel of MAS to subscribe to mas_domain Opitional fact, if not provided the role will use the default cluster subdomain mas_instance_id Defines the instance id to be used for MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_icr_cpopen Defines the registry for non entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev. mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_config_dir Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster. certManager.namespace The namespace containing the cert-manager to be used by MAS mas_upgrade_strategy The Upgrade strategy for MAS Operator. Default is set to Automatic Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify License \uf0c1 EPL-2.0","title":"suite_install"},{"location":"roles/suite_install/#suite_install","text":"This role install Maximo Application Suite. It internally resolve the namespace based on the mas_instance_id as mas-{mas_instance_id}-core . By default this role install MAS Operator using Manual Upgrade Strategy. Set MAS_UPGRADE_STRATEGY environment variable to Automatic to override it. In the Manual upgrade mode, IBM Common Services operators requested by MAS will inherit the upgrade strategy from MAS and their pending install plans approved.","title":"suite_install"},{"location":"roles/suite_install/#role-variables","text":"mas_catalog_source Defines the catalog to be used to install MAS. You can set it to ibm-operator-catalog for release install or ibm-mas-operators for development artifactory_username Required when using this role for development versions of MAS artifactory_apikey Required when using this role for development versions of MAS mas_channel Defines which channel of MAS to subscribe to mas_domain Opitional fact, if not provided the role will use the default cluster subdomain mas_instance_id Defines the instance id to be used for MAS installation mas_icr_cp Defines the entitled registry from the images should be pulled from. Set this to cp.icr.io/cp when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_icr_cpopen Defines the registry for non entitled images, such as operators. Set this to icr.io/cpopen when installing release version of MAS or wiotp-docker-local.artifactory.swg-devops.com for dev mas_entitlement_username Username for entitled registry. This username will be used to create the image pull secret. Set to cp when installing release or use your w3Id for dev. mas_entitlement_key API Key for entitled registry. This password will be used to create the image pull secret. Set to with IBM entitlement key when installing release or use your artifactory apikey for dev. mas_config_dir Directory containing configuration files ( *.yaml and *.yml ) to be applied to the MAS installation. Intended for creating the various MAS custom resources to configure the suite post-install, but can be used to apply any kubernetes resource you need to customize any aspect of your cluster. certManager.namespace The namespace containing the cert-manager to be used by MAS mas_upgrade_strategy The Upgrade strategy for MAS Operator. Default is set to Automatic","title":"Role Variables"},{"location":"roles/suite_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: \"inst1\" mas_config_dir: \"/home/david/masconfig\" mas_entitlement_key: \"{{ lookup('env', 'IBM_ENTITLEMENT_KEY') }}\" roles: - ibm.mas_devops.suite_install - ibm.mas_devops.suite_config - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_install/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_mustgather/","text":"suite_mustgather \uf0c1 Run IBM AI Applications' Must Gather tool against a MAS instance Role Variables \uf0c1 mas_instance_id \uf0c1 Required. MAS instance ID to run against. Environment Variable: MAS_INSTANCE_ID Default Value: None base_output_dir \uf0c1 Required. Location on the local disk to save the must-gather file. Environment Variable: BASE_OUTPUT_DIR Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 base_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather License \uf0c1 EPL-2.0","title":"suite_mustgather"},{"location":"roles/suite_mustgather/#suite_mustgather","text":"Run IBM AI Applications' Must Gather tool against a MAS instance","title":"suite_mustgather"},{"location":"roles/suite_mustgather/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_mustgather/#mas_instance_id","text":"Required. MAS instance ID to run against. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_mustgather/#base_output_dir","text":"Required. Location on the local disk to save the must-gather file. Environment Variable: BASE_OUTPUT_DIR Default Value: None","title":"base_output_dir"},{"location":"roles/suite_mustgather/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 base_output_dir: ~/mas/mustgather roles: - ibm.mas_devops.suite_mustgather","title":"Example Playbook"},{"location":"roles/suite_mustgather/#license","text":"EPL-2.0","title":"License"},{"location":"roles/suite_verify/","text":"suite_verify \uf0c1 Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True . Role Variables \uf0c1 mas_instance_id \uf0c1 Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_hide_superuser_credentials \uf0c1 Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify License \uf0c1 EPL-2.0","title":"suite_verify"},{"location":"roles/suite_verify/#suite_verify","text":"Verify a MAS installation is ready to use. This role will also print out the Admin Dashboard URL and the username and password of the superuser. If you want to disable these credentials being written to the output set the mas_hide_superuser_credentials to True .","title":"suite_verify"},{"location":"roles/suite_verify/#role-variables","text":"","title":"Role Variables"},{"location":"roles/suite_verify/#mas_instance_id","text":"Required. The instance ID of the Maximo Application Suite installation to verify. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/suite_verify/#mas_hide_superuser_credentials","text":"Set this to True if you want to disable the display of the superuser credentials as part of the verify. When this is enabled the debug will only identify the name of the secret containing the credentials rather than displaying the actual values. Environment Variable: MAS_HIDE_SUPERUSER_CREDENTIALS Default Value: False","title":"mas_hide_superuser_credentials"},{"location":"roles/suite_verify/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: mas_instance_id: masinst1 mas_hide_superuser_credentials: True roles: - ibm.mas_devops.suite_verify","title":"Example Playbook"},{"location":"roles/suite_verify/#license","text":"EPL-2.0","title":"License"},{"location":"roles/uds_install/","text":"uds_install \uf0c1 Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite. Role Variables \uf0c1 uds_storage_class \uf0c1 Required. Storage class where BAS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. Environment Variable: UDS_STORAGE_CLASS Default Value: None uds_event_scheduler_frequency \uf0c1 Defines the frequency that BAS will collect event data. The value can be set following a cron tab format, however support in UDS is limited to the following subset of valid cron formats: @annually , @yearly , @monthly , @weekly , @daily , @hourly . Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily mas_instance_id \uf0c1 The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None mas_config_dir \uf0c1 Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None mas_segment_key \uf0c1 Override the biult-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Environment Variable: MAS_SEGMENT_KEY Default Value: None uds_contact.email \uf0c1 Required when mas_instance_id and mas_config_dir are set. Sets the Contact e-mail address used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_EMAIL Default Value: None uds_contact.first_name \uf0c1 Required when mas_instance_id and mas_config_dir are set. Sets the Contact first name used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None uds_contact.last_name \uf0c1 Required when mas_instance_id and mas_config_dir are set. Sets the Contact last name used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_LASTNAME Default Value: None Example Playbook \uf0c1 - hosts: localhost any_errors_fatal: true vars: uds_meta_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' firstName: 'john' lastName: 'winter' roles: - ibm.mas_devops.uds_install License \uf0c1 EPL-2.0","title":"uds_install"},{"location":"roles/uds_install/#uds_install","text":"Installs IBM User Data Services as part of IBM Foundational Services in the ibm-common-services namespace. If mas_instance_id and the others associated parameters are provided then the role will also generate a configuration file that can be directly applied to IBM Maximo Application Suite.","title":"uds_install"},{"location":"roles/uds_install/#role-variables","text":"","title":"Role Variables"},{"location":"roles/uds_install/#uds_storage_class","text":"Required. Storage class where BAS will be installed. On IBM Cloud RedHat Openshift Kubernetes Service (ROKS) ibmc-block-bronze is the recommended value. Environment Variable: UDS_STORAGE_CLASS Default Value: None","title":"uds_storage_class"},{"location":"roles/uds_install/#uds_event_scheduler_frequency","text":"Defines the frequency that BAS will collect event data. The value can be set following a cron tab format, however support in UDS is limited to the following subset of valid cron formats: @annually , @yearly , @monthly , @weekly , @daily , @hourly . Environment Variable: UDS_EVENT_SCHEDULER_FREQUENCY Default Value: @daily","title":"uds_event_scheduler_frequency"},{"location":"roles/uds_install/#mas_instance_id","text":"The instance ID of Maximo Application Suite that the BasCfg configuration will target. If this or mas_config_dir are not set then the role will not generate a BasCfg template. Environment Variable: MAS_INSTANCE_ID Default Value: None","title":"mas_instance_id"},{"location":"roles/uds_install/#mas_config_dir","text":"Local directory to save the generated BasCfg resource definition. This can be used to manually configure a MAS instance to connect to BAS instance, or used as an input to the suite_config role. If this or mas_instance_id are not set then the role will not generate a BasCfg template. Environment Variable: MAS_CONFIG_DIR Default Value: None","title":"mas_config_dir"},{"location":"roles/uds_install/#mas_segment_key","text":"Override the biult-in segment key used by MAS when communicating with User Data Services. This variable is only used for the generation of the BASCfg template, and in 99% of use cases you will not need to set this. Environment Variable: MAS_SEGMENT_KEY Default Value: None","title":"mas_segment_key"},{"location":"roles/uds_install/#uds_contactemail","text":"Required when mas_instance_id and mas_config_dir are set. Sets the Contact e-mail address used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_EMAIL Default Value: None","title":"uds_contact.email"},{"location":"roles/uds_install/#uds_contactfirst_name","text":"Required when mas_instance_id and mas_config_dir are set. Sets the Contact first name used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_FIRSTNAME Default Value: None","title":"uds_contact.first_name"},{"location":"roles/uds_install/#uds_contactlast_name","text":"Required when mas_instance_id and mas_config_dir are set. Sets the Contact last name used by the MAS instance's UDS configuration. Environment Variable: UDS_CONTACT_LASTNAME Default Value: None","title":"uds_contact.last_name"},{"location":"roles/uds_install/#example-playbook","text":"- hosts: localhost any_errors_fatal: true vars: uds_meta_storage_class: ibmc-block-bronze mas_instance_id: masinst1 mas_config_dir: ~/masconfig uds_contact: email: 'john@email.com' firstName: 'john' lastName: 'winter' roles: - ibm.mas_devops.uds_install","title":"Example Playbook"},{"location":"roles/uds_install/#license","text":"EPL-2.0","title":"License"}]}